{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93feba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classifier.model import Model #, tfidf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import ijson\n",
    "import pickle\n",
    "import shap\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy\n",
    "load_dotenv()\n",
    "\n",
    "PATH_DRIVE = os.getenv(\"OUTPUTS_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a89874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_data_for_class(model, model_name, _class, k_best_features, Classifier_option):\n",
    "    \n",
    "    # split data\n",
    "    print(\"model_name: {}\".format(model_name))\n",
    "    if Classifier_option == \"binary\":\n",
    "        model.split_corpus_binary(_class, k_best_features)\n",
    "        is_splited = model.balance_data_Undersample_the_biggest_dataset(_class)\n",
    "        return is_splited\n",
    "\n",
    "def get_feature_importances(trained_model, model_name, k_best_features):\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        lst = []\n",
    "        # get importance\n",
    "        importance = trained_model.coef_[0]\n",
    "        \n",
    "        # summarize feature importance\n",
    "        for i,v in enumerate(importance):\n",
    "            lst.append((k_best_features[i],v))\n",
    "        return lst\n",
    "    else:\n",
    "        return list(zip(k_best_features,trained_model.feature_importances_)) \n",
    "    \n",
    "def detach_list(lst):\n",
    "    new_lst = []\n",
    "    for a,b in lst:\n",
    "        new_lst.append(max(a,b))\n",
    "    return new_lst\n",
    "\n",
    "def print_miss(y_predict_test, df_test):\n",
    "    test_jump = 66241+18926\n",
    "    miss_lst = []\n",
    "    for i,val in enumerate (y_predict_test):\n",
    "        if val != model.data.iloc[test_jump+i,:].status:\n",
    "            miss_lst.append(model.data.index[test_jump+i])\n",
    "    \n",
    "        \n",
    "def run_model(model, _class, Classifier_option, model_name, k_best_features, paths_to_save_data, subreddit_name, dec_tree_params={'max_depth':\"--\"}):\n",
    "    \n",
    "    # train model\n",
    "    \n",
    "    k_feature_names.remove('created_date')\n",
    "    k_feature_names.remove('status')\n",
    "    print(\"---------------------- \" + _class + \" -------------------------\")\n",
    "#     print(\"k_feature_names {} : {}\".format(len(k_feature_names), k_feature_names))\n",
    "    \n",
    "\n",
    "    trined_model = model.train_model(model_name=model_name , dec_tree_params=dec_tree_params)\n",
    "    \n",
    "    y_predict_train = trined_model.predict(model.df_train)\n",
    "    y_predict_prob_train = numpy.array(trined_model.predict_proba(model.df_train))[:, 1]\n",
    "    \n",
    "   \n",
    "    y_predict_valid = trined_model.predict(model.df_valid)\n",
    "    y_predict_prob_vaild =  numpy.array(trined_model.predict_proba(model.df_valid))[:, 1]\n",
    "   \n",
    "    y_predict_test = trined_model.predict(model.df_test)\n",
    "    y_predict_prob_test = numpy.array(trined_model.predict_proba(model.df_test))[:, 1]\n",
    "\n",
    "        \n",
    "    # evaluate\n",
    "\n",
    "    evaluation_res_test, evaluation_res_valid, evaluation_res_train = model.evaluation_indices(y_predict_test, y_predict_prob_test, _class, y_predict_train, y_predict_prob_train, y_predict_valid, y_predict_prob_vaild)\n",
    "    \n",
    "    classifier_data = [model_name, Classifier_option, _class]\n",
    "    \n",
    "    classifier_data.append(model.get_class_size(model.train_labels)) # model.train_labels\n",
    "    classifier_data.append(model.get_class_size(model.test_labels)) # model.test_labels\n",
    "    classifier_data.append(model.get_class_size(model.valid_labels)) # model.valid_labels\n",
    "    \n",
    "    if _class != 'all':\n",
    "        classifier_data.append(len(model.data[model.data['status'] == _class]))\n",
    "        tn, fp, fn, tp = confusion_matrix(model.test_labels, y_predict_test, labels=[\"not_\"+_class, _class]).ravel()\n",
    "        print(\"TN: \", tn, \"FP: \", fp, \" FN: \", fn, \" TP: \", tp)\n",
    "    else:\n",
    "        classifier_data.append(len(model.data.index))\n",
    "             \n",
    "    features_data = [sorted(get_feature_importances(trined_model, model_name, k_best_features), key=lambda x:x[1], reverse=True), len(k_best_features)]\n",
    "    model.file_reader.testing_recorder(features_data, evaluation_res_train, evaluation_res_valid, evaluation_res_test, classifier_data, dec_tree_params['max_depth'], paths_to_save_data[\"write_result_path\"])\n",
    "    \n",
    "    # saving trained model to disk\n",
    "    \n",
    "    # Create the directory \n",
    "    try: \n",
    "        os.mkdir(paths_to_save_data[\"save_model_path\"]) \n",
    "    except OSError as error: \n",
    "        print(error)  \n",
    "\n",
    "    model_file_name = 'k_{}_{}_{}_{}_model.pkl'.format(len(k_feature_names),subreddit_name, model_name, _class)\n",
    "    if model_name == 'DecisionTreeClassifier':\n",
    "        model_file_name = 'k_{}_{}_{}_{}_{}_model.pkl'.format(len(k_feature_names),subreddit_name, model_name, dec_tree_params['max_depth'], _class)\n",
    "    model_file_name = paths_to_save_data[\"save_model_path\"] + model_file_name\n",
    "    with open(model_file_name, 'wb') as f:\n",
    "        pickle.dump(trined_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1023a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree hyperparameter optimization using Grid Search\n",
    "from sklearn import decomposition, datasets\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def DecisionTree_optimization(model):\n",
    "    X = model.df_train\n",
    "    y = model.train_labels\n",
    "    std_slc = StandardScaler()\n",
    "    pca = decomposition.PCA()\n",
    "    dec_tree = tree.DecisionTreeClassifier()\n",
    "    pipe = Pipeline(steps=[('std_slc', std_slc),\n",
    "                         ('pca', pca),\n",
    "                         ('dec_tree', dec_tree)])\n",
    "    n_components = list(range(1,X.shape[1]+1,1))\n",
    "    criterion = ['gini', 'entropy']\n",
    "    max_depth = [dep for dep in range(10,12)]\n",
    "    parameters = dict(pca__n_components=n_components,\n",
    "                      dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth)\n",
    "\n",
    "    clf_GS = GridSearchCV(pipe, parameters) \n",
    "    clf_GS.fit(X, y)\n",
    "    \n",
    "    # Now we are using print statements to print the results. It will give the values of hyperparameters as a result.\n",
    "    print(); print(clf_GS.best_estimator_.get_params()['dec_tree'])\n",
    "    return  (clf_GS.best_estimator_.get_params()['dec_tree__criterion'], clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e7ec47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocissing(model, feature_list):\n",
    "    columns=model.data.columns\n",
    "    if 'date' in columns:\n",
    "        model.data.rename(columns = {'date':'created_date'}, inplace = True)\n",
    "    if 'author_fullname' in columns:\n",
    "        model.data.drop(columns=['author_fullname'], inplace = True)\n",
    "    if 'num_comments' in columns:\n",
    "        model.data.drop(columns=['num_comments'], inplace = True)\n",
    "    if 'is_retrieved' in columns:\n",
    "        print('is_retrieved droped')\n",
    "        model.data.drop(columns=['is_retrieved'], inplace = True)\n",
    "    if 'selftext_len' in columns:\n",
    "        model.data.drop(columns=['selftext_len'], inplace = True)\n",
    "    if 'title_selftext' in columns:\n",
    "        model.data.drop(columns=['title_selftext'], inplace = True)\n",
    "    contdition = ((model.data['status'] != 'deleted')&(model.data['status'] != 'poll')&(model.data['status'] != 'automod_filtered'))\n",
    "    model.data = model.data[contdition]\n",
    "    model.data.status.replace(['removed', 'shadow_ban'], 'not_exist', inplace=True)\n",
    "    model.MAX_POST_NUMBER = model.data.shape[0]\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42b35d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def SHAP(model, k_features):\n",
    "        model.split_corpus_basic()\n",
    "        y = model.train_labels \n",
    "        y.replace(['removed', 'shadow_ban'], 'not_exist', inplace=True)\n",
    "        model.df_train.drop(columns=['created_date'], inplace=True)\n",
    "        X = model.df_train*1\n",
    "        \n",
    "        #train XGBoost model\n",
    "        model1 = xgboost.XGBClassifier(max_depth=5, learning_rate=0.5).fit(X, y)\n",
    "\n",
    "        #compute SHAP values\n",
    "        explainer = shap.Explainer(model1, X, link=shap.links.logit)\n",
    "        shap_values = explainer(X)        \n",
    "        shap.plots.bar(shap_values)\n",
    "        \n",
    "        vals = np.abs(shap_values.values).mean(0)\n",
    "        feature_names = X.columns\n",
    "\n",
    "        feature_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name','feature_importance_vals'])\n",
    "        feature_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "        k_featues_name = [f_name for f_name, val in feature_importance.head(k_features).values.tolist()]\n",
    "        return k_featues_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d68ba4ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------2022-----------------------\n",
      "-----------------------------antiwork    DONE-----------------------\n",
      "-----------------------------AskARussian    DONE-----------------------\n",
      "-----------------------------cryptocurrency    DONE-----------------------\n",
      "-----------------------------UkrainianConflict    DONE-----------------------\n",
      "-----------------------------robinhood    DONE-----------------------\n",
      "-------------------------------2021-----------------------\n",
      "-----------------------------antiwork    DONE-----------------------\n",
      "-----------------------------cryptocurrency    DONE-----------------------\n",
      "-----------------------------trueoffmychest    DONE-----------------------\n",
      "-----------------------------robinhood    DONE-----------------------\n",
      "-------------------------------2020-----------------------\n",
      "-----------------------------antiwork    DONE-----------------------\n",
      "-----------------------------conservative    DONE-----------------------\n",
      "-----------------------------cryptocurrency    DONE-----------------------\n",
      "-----------------------------israel    DONE-----------------------\n",
      "-----------------------------mensrights    DONE-----------------------\n",
      "-----------------------------palestine    DONE-----------------------\n",
      "-----------------------------politics    DONE-----------------------\n",
      "-----------------------------stocks-----------------------\n",
      "is_retrieved droped\n",
      "[18:57:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 32830/33204 [00:53<00:00]        "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x468 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: DecisionTreeClassifier\n",
      "class_name exist\n",
      "((29053, 7), (29053,))\n",
      "((8301, 7), (8301,))\n",
      "((4151, 7), (4151,))\n",
      "\n",
      "DecisionTreeClassifier(max_depth=10)\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 84.43%\n",
      "precision: 85.09% \n",
      "recall: 84.43% \n",
      "fscore: 84.35% \n",
      "AUC 90.6025%\n",
      "---VALID---\n",
      "Accuracy: 79.47%\n",
      "precision: 80.29% \n",
      "recall: 79.47% \n",
      "fscore: 79.33% \n",
      "AUC 81.7112%\n",
      "---TEST---\n",
      "Accuracy: 82.82%\n",
      "precision: 83.33% \n",
      "recall: 82.82% \n",
      "fscore: 82.75% \n",
      "AUC 85.4651%\n",
      "TN:  1470 FP:  448  FN:  211  TP:  1707\n",
      "classifier_data:  [76, '06/23/2022', '18:58:33', 10, 'DecisionTreeClassifier', 'binary', 'exist', [('not_exist', 8293), ('exist', 8293)], [('not_exist', 1918), ('exist', 1918)], [('exist', 4052), ('not_exist', 4052)], 27242, [('neutral', 0.3887888399743343), ('surprise', 0.2341930267998869), ('fear', 0.1242426524789408), ('joy', 0.10850458878179615), ('title_len', 0.10238207804712415), ('disgust', 0.04188881391791776)], 6, 0.8442662486434342, 0.8508615303726615, 0.8442662486434342, 0.8435309461073943, 0.9060252974935283, 0.7946692991115498, 0.8029014905637697, 0.7946692991115498, 0.7932646468898179, 0.8171123616090528, 0.828206465067779, 0.8332954227057974, 0.828206465067779, 0.8275481902350729, 0.8546506886626994]\n",
      "[Errno 17] File exists: '/sise/home/shai1/combain_classifier_output/2020/stocks/models_combine/'\n",
      "model_name: RandomForestClassifier\n",
      "class_name exist\n",
      "((29053, 7), (29053,))\n",
      "((8301, 7), (8301,))\n",
      "((4151, 7), (4151,))\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 79.09%\n",
      "precision: 79.28% \n",
      "recall: 79.09% \n",
      "fscore: 79.06% \n",
      "AUC 86.0890%\n",
      "---VALID---\n",
      "Accuracy: 79.12%\n",
      "precision: 79.31% \n",
      "recall: 79.12% \n",
      "fscore: 79.09% \n",
      "AUC 85.8005%\n",
      "---TEST---\n",
      "Accuracy: 82.90%\n",
      "precision: 83.05% \n",
      "recall: 82.90% \n",
      "fscore: 82.88% \n",
      "AUC 89.9486%\n",
      "TN:  1526 FP:  392  FN:  264  TP:  1654\n",
      "classifier_data:  [77, '06/23/2022', '18:58:35', '--', 'RandomForestClassifier', 'binary', 'exist', [('not_exist', 8293), ('exist', 8293)], [('not_exist', 1918), ('exist', 1918)], [('exist', 4052), ('not_exist', 4052)], 27242, [('neutral', 0.34489508262877044), ('surprise', 0.3003653144196516), ('fear', 0.13279404168750766), ('disgust', 0.10042998661484843), ('joy', 0.0817273254321684), ('title_len', 0.039788249217053644)], 6, 0.7909079946943205, 0.7928307851035091, 0.7909079946943205, 0.7905641944701816, 0.8608896893352589, 0.7912142152023692, 0.7931349901171717, 0.7912142152023692, 0.7908716353925083, 0.8580045964651047, 0.8289885297184567, 0.8304603067983858, 0.8289885297184567, 0.8287979082508488, 0.8994863436343689]\n",
      "[Errno 17] File exists: '/sise/home/shai1/combain_classifier_output/2020/stocks/models_combine/'\n",
      "model_name: XGBClassifier\n",
      "class_name exist\n",
      "((29053, 7), (29053,))\n",
      "((8301, 7), (8301,))\n",
      "((4151, 7), (4151,))\n",
      "---------------------- exist -------------------------\n",
      "[18:58:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.59503\tvalidation_1-logloss:0.57680\n",
      "[1]\tvalidation_0-logloss:0.53684\tvalidation_1-logloss:0.50794\n",
      "[2]\tvalidation_0-logloss:0.50125\tvalidation_1-logloss:0.46600\n",
      "[3]\tvalidation_0-logloss:0.47811\tvalidation_1-logloss:0.43868\n",
      "[4]\tvalidation_0-logloss:0.46318\tvalidation_1-logloss:0.42212\n",
      "[5]\tvalidation_0-logloss:0.45301\tvalidation_1-logloss:0.41088\n",
      "[6]\tvalidation_0-logloss:0.44598\tvalidation_1-logloss:0.40223\n",
      "[7]\tvalidation_0-logloss:0.44015\tvalidation_1-logloss:0.39548\n",
      "[8]\tvalidation_0-logloss:0.43638\tvalidation_1-logloss:0.39282\n",
      "[9]\tvalidation_0-logloss:0.43225\tvalidation_1-logloss:0.38872\n",
      "[10]\tvalidation_0-logloss:0.42830\tvalidation_1-logloss:0.38698\n",
      "[11]\tvalidation_0-logloss:0.42628\tvalidation_1-logloss:0.38647\n",
      "[12]\tvalidation_0-logloss:0.42400\tvalidation_1-logloss:0.38432\n",
      "[13]\tvalidation_0-logloss:0.42150\tvalidation_1-logloss:0.38251\n",
      "[14]\tvalidation_0-logloss:0.42007\tvalidation_1-logloss:0.38223\n",
      "[15]\tvalidation_0-logloss:0.41904\tvalidation_1-logloss:0.38103\n",
      "[16]\tvalidation_0-logloss:0.41765\tvalidation_1-logloss:0.38073\n",
      "[17]\tvalidation_0-logloss:0.41569\tvalidation_1-logloss:0.37950\n",
      "[18]\tvalidation_0-logloss:0.41463\tvalidation_1-logloss:0.37896\n",
      "[19]\tvalidation_0-logloss:0.41137\tvalidation_1-logloss:0.37851\n",
      "[20]\tvalidation_0-logloss:0.41050\tvalidation_1-logloss:0.37847\n",
      "[21]\tvalidation_0-logloss:0.40831\tvalidation_1-logloss:0.37735\n",
      "[22]\tvalidation_0-logloss:0.40662\tvalidation_1-logloss:0.37613\n",
      "[23]\tvalidation_0-logloss:0.40553\tvalidation_1-logloss:0.37634\n",
      "[24]\tvalidation_0-logloss:0.40358\tvalidation_1-logloss:0.37562\n",
      "[25]\tvalidation_0-logloss:0.40300\tvalidation_1-logloss:0.37557\n",
      "[26]\tvalidation_0-logloss:0.40242\tvalidation_1-logloss:0.37559\n",
      "[27]\tvalidation_0-logloss:0.40100\tvalidation_1-logloss:0.37525\n",
      "[28]\tvalidation_0-logloss:0.40005\tvalidation_1-logloss:0.37508\n",
      "[29]\tvalidation_0-logloss:0.39825\tvalidation_1-logloss:0.37472\n",
      "[30]\tvalidation_0-logloss:0.39644\tvalidation_1-logloss:0.37411\n",
      "[31]\tvalidation_0-logloss:0.39517\tvalidation_1-logloss:0.37366\n",
      "[32]\tvalidation_0-logloss:0.39384\tvalidation_1-logloss:0.37412\n",
      "[33]\tvalidation_0-logloss:0.39178\tvalidation_1-logloss:0.37498\n",
      "[34]\tvalidation_0-logloss:0.38935\tvalidation_1-logloss:0.37457\n",
      "[35]\tvalidation_0-logloss:0.38838\tvalidation_1-logloss:0.37465\n",
      "[36]\tvalidation_0-logloss:0.38682\tvalidation_1-logloss:0.37459\n",
      "[37]\tvalidation_0-logloss:0.38548\tvalidation_1-logloss:0.37483\n",
      "[38]\tvalidation_0-logloss:0.38508\tvalidation_1-logloss:0.37482\n",
      "[39]\tvalidation_0-logloss:0.38486\tvalidation_1-logloss:0.37472\n",
      "[40]\tvalidation_0-logloss:0.38439\tvalidation_1-logloss:0.37473\n",
      "[41]\tvalidation_0-logloss:0.38247\tvalidation_1-logloss:0.37480\n",
      "---TRAIN---\n",
      "Accuracy: 82.75%\n",
      "precision: 83.01% \n",
      "recall: 82.75% \n",
      "fscore: 82.72% \n",
      "AUC 89.6789%\n",
      "---VALID---\n",
      "Accuracy: 81.32%\n",
      "precision: 81.66% \n",
      "recall: 81.32% \n",
      "fscore: 81.27% \n",
      "AUC 87.0344%\n",
      "---TEST---\n",
      "Accuracy: 84.93%\n",
      "precision: 85.14% \n",
      "recall: 84.93% \n",
      "fscore: 84.91% \n",
      "AUC 90.7154%\n",
      "TN:  1555 FP:  363  FN:  215  TP:  1703\n",
      "classifier_data:  [78, '06/23/2022', '18:58:36', '--', 'XGBClassifier', 'binary', 'exist', [('not_exist', 8293), ('exist', 8293)], [('not_exist', 1918), ('exist', 1918)], [('exist', 4052), ('not_exist', 4052)], 27242, [('neutral', 0.29395646), ('surprise', 0.26849705), ('fear', 0.17751059), ('joy', 0.1250734), ('title_len', 0.09374381), ('disgust', 0.04121869)], 6, 0.8275051248040516, 0.8300697097844579, 0.8275051248040516, 0.8271694087983024, 0.8967888651978749, 0.8131786771964462, 0.816612789413212, 0.8131786771964462, 0.8126707146243571, 0.8703435118874181, 0.8493222106360793, 0.8514146170716633, 0.8493222106360793, 0.8490975833921368, 0.9071544916117653]\n",
      "[Errno 17] File exists: '/sise/home/shai1/combain_classifier_output/2020/stocks/models_combine/'\n",
      "is_retrieved droped\n",
      "-----------------------------technology    DONE-----------------------\n",
      "-----------------------------wallstreetbets    DONE-----------------------\n",
      "-----------------------------robinhood    DONE-----------------------\n"
     ]
    }
   ],
   "source": [
    "models_name = ['DecisionTreeClassifier', \"RandomForestClassifier\", \"XGBClassifier\"] #, 'LogisticRegression']\n",
    "\n",
    "# without deleted class\n",
    "\n",
    "classes = ['exist'] \n",
    "\n",
    "# Insert the directory path in here\n",
    "path_to_classifier_data_folder = r'/sise/home/shai1/combain_classifier_output/' \n",
    "\n",
    "\n",
    "l_year_folders = os.listdir(path_to_classifier_data_folder)\n",
    "file_set = set()\n",
    "\n",
    "feature_list = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise', 'bertweet_neg', \n",
    "                'bertweet_neu', 'bertweet_pos', 'hate', 'offensive', 'title_len', 'title_hashtag',\n",
    "                'title_exclamation_mark', 'title_dot', 'title_asterisk', 'selftext_hashtag', \n",
    "                'selftext_exclamation_mark', 'selftext_dot', 'selftext_asterisk', 'is_video', 'contain_URL', \n",
    "                'is_profane', 'contain_question', 'detect_english', 'author_cumcount'] \n",
    "\n",
    "already_done=[\"antiwork\",\"conservative\", 'israel', 'politics', 'palestine', 'cryptocurrency', 'mensrights',\n",
    "              'stocks', 'technology']\n",
    "\n",
    "for year in l_year_folders:\n",
    "    print(\"-------------------------------{}-----------------------\".format(year))\n",
    "    for file_name in os.listdir(path_to_classifier_data_folder + year):\n",
    "        subreddit = file_name.split(\"_\")[0]\n",
    "        file_set.add(file_name)\n",
    "        if year != \"2020\":\n",
    "            print(\"-----------------------------{}    DONE-----------------------\".format(file_name))\n",
    "            continue\n",
    "        if year ==\"2020\" and subreddit != 'stocks': \n",
    "            print(\"-----------------------------{}    DONE-----------------------\".format(file_name))\n",
    "            continue            \n",
    "        print(\"-----------------------------{}-----------------------\".format(file_name))\n",
    "        subreddiit_path_to_write_analysis = path_to_classifier_data_folder + '{}/{}/combine_classifier_{}_post_{}.csv'.format(year, subreddit, subreddit, year) # folder, file\n",
    "        model = Model(year=year, subreddit=subreddit, sub_kind='post')\n",
    "#         model_2022 = Model(year='2022', subreddit=subreddit, sub_kind='post')\n",
    "#         data = [model.data ,model_2022.data]\n",
    "#         model.data = pd.concat(data)\n",
    "        feature_list = data_preprocissing(model, feature_list)\n",
    "        k_features = 6\n",
    "        k_feature_names = SHAP(model, k_features)\n",
    "        \n",
    "        for _class in classes:\n",
    "            for model_name in models_name:\n",
    "                is_splited = split_data_for_class(model, model_name, _class, k_feature_names, \"binary\")\n",
    "                if is_splited == False: \n",
    "                    print(\"subreddit\", file_name, \"  status\", model.data.status.value_counts())\n",
    "                    break\n",
    "                paths={'write_result_path': subreddiit_path_to_write_analysis, 'save_model_path': path_to_classifier_data_folder + '{}/{}/models_combine/'.format(year, subreddit, file_name)}\n",
    "                if model_name == 'DecisionTreeClassifier':\n",
    "                    criterion ,max_depth = DecisionTree_optimization(model)\n",
    "                    dec_tree_params = {\"criterion\": criterion, \"max_depth\":max_depth}\n",
    "                    run_model(model, _class, \"binary\", model_name, k_feature_names, paths, subreddit, dec_tree_params)\n",
    "                else:\n",
    "                    run_model(model, _class, \"binary\", model_name, k_feature_names, paths, subreddit)\n",
    "            model.data = model.read_dataset(year=year, subreddit=subreddit, sub_kind='post')\n",
    "            feature_list = data_preprocissing(model, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c577e7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>date</th>\n",
       "      <th>status</th>\n",
       "      <th>retrieved</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i2vudh</th>\n",
       "      <td>thoughts about investing in madison square gar...</td>\n",
       "      <td>['2020-08-03', '15:06:08']</td>\n",
       "      <td>deleted</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_6c4xiskb</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kkw7xz</th>\n",
       "      <td>does anyone use motley fool here i signed up f...</td>\n",
       "      <td>['2020-12-27', '05:26:14']</td>\n",
       "      <td>deleted</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_38omgh87</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i2vpdd</th>\n",
       "      <td>faang stocks with most room to grow i m in the...</td>\n",
       "      <td>['2020-08-03', '14:55:57']</td>\n",
       "      <td>exist</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_1538dr</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kkw7h3</th>\n",
       "      <td>help this baby my first stocks portfolio brand...</td>\n",
       "      <td>['2020-12-27', '05:25:20']</td>\n",
       "      <td>deleted</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_6c6og5lw</td>\n",
       "      <td>None</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i2vong</th>\n",
       "      <td>top stock for august</td>\n",
       "      <td>['2020-08-03', '14:54:26']</td>\n",
       "      <td>removed</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_3m7c4pqd</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i2w52k</th>\n",
       "      <td>trump threatens a shutdown of tiktok unless ms...</td>\n",
       "      <td>['2020-08-03', '15:27:30']</td>\n",
       "      <td>exist</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_dyc5p</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kkwlty</th>\n",
       "      <td>gig economy stocks or etfs i have been really ...</td>\n",
       "      <td>['2020-12-27', '05:52:36']</td>\n",
       "      <td>exist</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_uh5b9ly</td>\n",
       "      <td>Advice Request</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i2w22s</th>\n",
       "      <td>question opinions on investing in madison squa...</td>\n",
       "      <td>['2020-08-03', '15:21:30']</td>\n",
       "      <td>deleted</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_6c4xiskb</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kkwezq</th>\n",
       "      <td>arkg vs idna for biotech exposure</td>\n",
       "      <td>['2020-12-27', '05:39:31']</td>\n",
       "      <td>deleted</td>\n",
       "      <td>False</td>\n",
       "      <td>t2_1033tw</td>\n",
       "      <td>Advice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i2w099</th>\n",
       "      <td>where to find earnings date where do you go to...</td>\n",
       "      <td>['2020-08-03', '15:17:57']</td>\n",
       "      <td>exist</td>\n",
       "      <td>True</td>\n",
       "      <td>t2_2uq6djtk</td>\n",
       "      <td>Question</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60146 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title_selftext  \\\n",
       "post_id                                                      \n",
       "i2vudh   thoughts about investing in madison square gar...   \n",
       "kkw7xz   does anyone use motley fool here i signed up f...   \n",
       "i2vpdd   faang stocks with most room to grow i m in the...   \n",
       "kkw7h3   help this baby my first stocks portfolio brand...   \n",
       "i2vong                                top stock for august   \n",
       "...                                                    ...   \n",
       "i2w52k   trump threatens a shutdown of tiktok unless ms...   \n",
       "kkwlty   gig economy stocks or etfs i have been really ...   \n",
       "i2w22s   question opinions on investing in madison squa...   \n",
       "kkwezq                   arkg vs idna for biotech exposure   \n",
       "i2w099   where to find earnings date where do you go to...   \n",
       "\n",
       "                               date   status retrieved author_fullname  \\\n",
       "post_id                                                                  \n",
       "i2vudh   ['2020-08-03', '15:06:08']  deleted     False     t2_6c4xiskb   \n",
       "kkw7xz   ['2020-12-27', '05:26:14']  deleted      True     t2_38omgh87   \n",
       "i2vpdd   ['2020-08-03', '14:55:57']    exist      True       t2_1538dr   \n",
       "kkw7h3   ['2020-12-27', '05:25:20']  deleted      True     t2_6c6og5lw   \n",
       "i2vong   ['2020-08-03', '14:54:26']  removed     False     t2_3m7c4pqd   \n",
       "...                             ...      ...       ...             ...   \n",
       "i2w52k   ['2020-08-03', '15:27:30']    exist      True        t2_dyc5p   \n",
       "kkwlty   ['2020-12-27', '05:52:36']    exist      True      t2_uh5b9ly   \n",
       "i2w22s   ['2020-08-03', '15:21:30']  deleted      True     t2_6c4xiskb   \n",
       "kkwezq   ['2020-12-27', '05:39:31']  deleted     False       t2_1033tw   \n",
       "i2w099   ['2020-08-03', '15:17:57']    exist      True     t2_2uq6djtk   \n",
       "\n",
       "        link_flair_text  num_comments  \n",
       "post_id                                \n",
       "i2vudh             None             1  \n",
       "kkw7xz             None            17  \n",
       "i2vpdd       Discussion            41  \n",
       "kkw7h3             None            16  \n",
       "i2vong             None             0  \n",
       "...                 ...           ...  \n",
       "i2w52k       Discussion            10  \n",
       "kkwlty   Advice Request             4  \n",
       "i2w22s             None             3  \n",
       "kkwezq           Advice             1  \n",
       "i2w099         Question            17  \n",
       "\n",
       "[60146 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year=\"2020\"\n",
    "sub_kind=\"post\"\n",
    "subreddit = \"stocks\"\n",
    "pd.read_pickle(f\"/dt/puzis/dt-reddit/cleaned_data/{subreddit}_{sub_kind}_{year}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d57f51",
   "metadata": {},
   "source": [
    "# Word Embedding - Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5210931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_df(df):\n",
    "    basic_data_df = df[['post_id', 'created_date', 'status', 'Topic', 'Name', 'topic_words', 'title_selftext']]\n",
    "    basic_data_df = basic_data_df[basic_data_df['title_selftext'].apply(lambda x: type(x) == str)]\n",
    "    documents = [TaggedDocument(row['title_selftext'], [row['post_id']]) for index, row in basic_data_df.iterrows()]\n",
    "    model = Doc2Vec(documents, vector_size=300, window=2, min_count=1, workers=4)\n",
    "    vec_lst = []\n",
    "    post_id_lst = []\n",
    "    for index, row in basic_data_df.iterrows():\n",
    "        post_id_lst.append(row['post_id'])\n",
    "        vec_lst.append(model.docvecs[row['post_id']].tolist())\n",
    "    emdedding_df = pd.DataFrame(data=vec_lst)\n",
    "    emdedding_df['post_id']=post_id_lst\n",
    "    ans_df = pd.merge(basic_data_df, emdedding_df, on=\"post_id\")\n",
    "    ans_df.drop(['post_id','Topic', 'Name', 'topic_words', 'title_selftext'], axis=1, inplace=True)\n",
    "    return ans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520ac55",
   "metadata": {},
   "source": [
    "# Exec Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7eb5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------2020-----------------------\n",
      "-----------------------------antiwork    DONE-----------------------\n",
      "-----------------------------conservative    DONE-----------------------\n",
      "-----------------------------cryptocurrency    DONE-----------------------\n",
      "-----------------------------israel    DONE-----------------------\n",
      "-----------------------------mensrights    DONE-----------------------\n",
      "-----------------------------palestine    DONE-----------------------\n",
      "-----------------------------politics    DONE-----------------------\n",
      "-----------------------------stocks    DONE-----------------------\n",
      "-----------------------------technology    DONE-----------------------\n",
      "-----------------------------wallstreetbets    DONE-----------------------\n",
      "-----------------------------robinhood    DONE-----------------------\n",
      "-------------------------------2021-----------------------\n",
      "-----------------------------antiwork    DONE-----------------------\n",
      "-----------------------------cryptocurrency    DONE-----------------------\n",
      "-----------------------------trueoffmychest    DONE-----------------------\n",
      "-----------------------------robinhood    DONE-----------------------\n",
      "-------------------------------2022-----------------------\n",
      "-----------------------------antiwork-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.4.0 of praw is outdated. Version 7.6.0 was released Tuesday May 10, 2022.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: DecisionTreeClassifier\n",
      "class_name exist\n",
      "((66661, 385), (66661,))\n",
      "((19046, 385), (19046,))\n",
      "((9523, 385), (9523,))\n",
      "is_splited None\n",
      "here\n",
      "dec_tree_params {'criterion': 'gini', 'max_depth': 5}\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 65.84%\n",
      "precision: 65.86% \n",
      "recall: 65.84% \n",
      "fscore: 65.83% \n",
      "AUC 70.9780%\n",
      "---VALID---\n",
      "Accuracy: 64.72%\n",
      "precision: 64.78% \n",
      "recall: 64.72% \n",
      "fscore: 64.68% \n",
      "AUC 70.4404%\n",
      "---TEST---\n",
      "Accuracy: 63.85%\n",
      "precision: 63.87% \n",
      "recall: 63.85% \n",
      "fscore: 63.84% \n",
      "AUC 68.6348%\n",
      "TN:  1810 FP:  1114  FN:  1000  TP:  1924\n",
      "classifier_data:  [19, '06/09/2022', '08:16:36', 5, 'DecisionTreeClassifier', 'binary', 'exist', [('not_exist', 28638), ('exist', 28638)], [('not_exist', 2924), ('exist', 2924)], [('exist', 5229), ('not_exist', 5229)], 58439, [(99, 0.38649112413596803), (227, 0.1755777321371583), (251, 0.12670325210694638), (304, 0.07074037290531851), (221, 0.04261056261393632), (189, 0.03934038493757999), (92, 0.023606368360740423), (106, 0.019222085523550463), (301, 0.0168198091034181), (364, 0.01580395009476917), (167, 0.015120098877032355), (127, 0.00833445827797636), (173, 0.008096973343792176), (187, 0.007364048127335048), (339, 0.007058105520018524), (238, 0.006029722940481449), (179, 0.005274951978357129), (259, 0.004417793993877129), (2, 0.003426983333657837), (299, 0.003378442842005637), (140, 0.003337014616597319), (202, 0.002692627583166174), (15, 0.002368719061281925), (293, 0.0022303218741184495), (208, 0.0020148308366402193), (326, 0.0019392648742767085), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (60, 0.0), (61, 0.0), (62, 0.0), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.0), (76, 0.0), (77, 0.0), (78, 0.0), (79, 0.0), (80, 0.0), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (93, 0.0), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (100, 0.0), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.0), (122, 0.0), (123, 0.0), (124, 0.0), (125, 0.0), (126, 0.0), (128, 0.0), (129, 0.0), (130, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (134, 0.0), (135, 0.0), (136, 0.0), (137, 0.0), (138, 0.0), (139, 0.0), (141, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (145, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (156, 0.0), (157, 0.0), (158, 0.0), (159, 0.0), (160, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (166, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (172, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (178, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (183, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (188, 0.0), (190, 0.0), (191, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0), (200, 0.0), (201, 0.0), (203, 0.0), (204, 0.0), (205, 0.0), (206, 0.0), (207, 0.0), (209, 0.0), (210, 0.0), (211, 0.0), (212, 0.0), (213, 0.0), (214, 0.0), (215, 0.0), (216, 0.0), (217, 0.0), (218, 0.0), (219, 0.0), (220, 0.0), (222, 0.0), (223, 0.0), (224, 0.0), (225, 0.0), (226, 0.0), (228, 0.0), (229, 0.0), (230, 0.0), (231, 0.0), (232, 0.0), (233, 0.0), (234, 0.0), (235, 0.0), (236, 0.0), (237, 0.0), (239, 0.0), (240, 0.0), (241, 0.0), (242, 0.0), (243, 0.0), (244, 0.0), (245, 0.0), (246, 0.0), (247, 0.0), (248, 0.0), (249, 0.0), (250, 0.0), (252, 0.0), (253, 0.0), (254, 0.0), (255, 0.0), (256, 0.0), (257, 0.0), (258, 0.0), (260, 0.0), (261, 0.0), (262, 0.0), (263, 0.0), (264, 0.0), (265, 0.0), (266, 0.0), (267, 0.0), (268, 0.0), (269, 0.0), (270, 0.0), (271, 0.0), (272, 0.0), (273, 0.0), (274, 0.0), (275, 0.0), (276, 0.0), (277, 0.0), (278, 0.0), (279, 0.0), (280, 0.0), (281, 0.0), (282, 0.0), (283, 0.0), (284, 0.0), (285, 0.0), (286, 0.0), (287, 0.0), (288, 0.0), (289, 0.0), (290, 0.0), (291, 0.0), (292, 0.0), (294, 0.0), (295, 0.0), (296, 0.0), (297, 0.0), (298, 0.0), (300, 0.0), (302, 0.0), (303, 0.0), (305, 0.0), (306, 0.0), (307, 0.0), (308, 0.0), (309, 0.0), (310, 0.0), (311, 0.0), (312, 0.0), (313, 0.0), (314, 0.0), (315, 0.0), (316, 0.0), (317, 0.0), (318, 0.0), (319, 0.0), (320, 0.0), (321, 0.0), (322, 0.0), (323, 0.0), (324, 0.0), (325, 0.0), (327, 0.0), (328, 0.0), (329, 0.0), (330, 0.0), (331, 0.0), (332, 0.0), (333, 0.0), (334, 0.0), (335, 0.0), (336, 0.0), (337, 0.0), (338, 0.0), (340, 0.0), (341, 0.0), (342, 0.0), (343, 0.0), (344, 0.0), (345, 0.0), (346, 0.0), (347, 0.0), (348, 0.0), (349, 0.0), (350, 0.0), (351, 0.0), (352, 0.0), (353, 0.0), (354, 0.0), (355, 0.0), (356, 0.0), (357, 0.0), (358, 0.0), (359, 0.0), (360, 0.0), (361, 0.0), (362, 0.0), (363, 0.0), (365, 0.0), (366, 0.0), (367, 0.0), (368, 0.0), (369, 0.0), (370, 0.0), (371, 0.0), (372, 0.0), (373, 0.0), (374, 0.0), (375, 0.0), (376, 0.0), (377, 0.0), (378, 0.0), (379, 0.0), (380, 0.0), (381, 0.0), (382, 0.0), (383, 0.0)], 384, 0.6584084084084084, 0.6585577841066744, 0.6584084084084084, 0.6583279369523976, 0.7097802986943208, 0.6471600688468159, 0.6477848825723116, 0.6471600688468159, 0.6467867348287064, 0.7044036960708813, 0.6385088919288646, 0.6387197514753807, 0.6385088919288646, 0.6383714694869624, 0.6863480399954338]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/antiwork/models_combine/'\n",
      "model_name: RandomForestClassifier\n",
      "class_name exist\n",
      "((66661, 385), (66661,))\n",
      "((19046, 385), (19046,))\n",
      "((9523, 385), (9523,))\n",
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 69.31%\n",
      "precision: 69.31% \n",
      "recall: 69.31% \n",
      "fscore: 69.31% \n",
      "AUC 75.8130%\n",
      "---VALID---\n",
      "Accuracy: 69.02%\n",
      "precision: 69.02% \n",
      "recall: 69.02% \n",
      "fscore: 69.02% \n",
      "AUC 76.0889%\n",
      "---TEST---\n",
      "Accuracy: 67.58%\n",
      "precision: 67.58% \n",
      "recall: 67.58% \n",
      "fscore: 67.58% \n",
      "AUC 74.2886%\n",
      "TN:  1952 FP:  972  FN:  924  TP:  2000\n",
      "classifier_data:  [20, '06/09/2022', '08:17:25', '--', 'RandomForestClassifier', 'binary', 'exist', [('not_exist', 28638), ('exist', 28638)], [('not_exist', 2924), ('exist', 2924)], [('exist', 5229), ('not_exist', 5229)], 58439, [(99, 0.07140881477485898), (267, 0.04352228701828705), (251, 0.04202943961882704), (157, 0.03891803317546082), (227, 0.03450162487858715), (199, 0.031994487981302375), (256, 0.030348448588551298), (304, 0.02870634664422918), (48, 0.02632280696367045), (221, 0.02607647915170887), (364, 0.02286999815453923), (183, 0.02192179075688226), (62, 0.021846918656671422), (11, 0.02163767793696954), (140, 0.01989409177973672), (218, 0.018432368833353193), (266, 0.017945648823852897), (189, 0.016361519158298386), (111, 0.014389584685757831), (187, 0.01412804542843559), (238, 0.01398042536087994), (291, 0.013970247766516801), (6, 0.013766033241709741), (170, 0.01144542125784767), (94, 0.01140848437725485), (167, 0.010619295758366922), (213, 0.010264804138453161), (363, 0.010202259363142962), (39, 0.009830543955668446), (126, 0.00981713662131253), (245, 0.009173499346846347), (92, 0.009152655997620576), (34, 0.008674523766353649), (347, 0.008350926421778034), (97, 0.008256982308831486), (325, 0.008004824310986085), (151, 0.00753359820061937), (180, 0.006890404720535562), (54, 0.00665047109162619), (379, 0.006517212405062688), (203, 0.005821879560763196), (366, 0.005221778450755286), (119, 0.005200336076124861), (339, 0.0049327838630222704), (30, 0.004837506280044168), (359, 0.004605962566974528), (198, 0.004570660731498539), (74, 0.004399516372723196), (102, 0.00438699553156552), (301, 0.004301205458517), (205, 0.004285692102244348), (358, 0.004052183078998696), (338, 0.003901371134637136), (125, 0.003850326441774932), (137, 0.003775510055037479), (223, 0.003613700411369689), (12, 0.003597324843424614), (166, 0.003546546271340985), (106, 0.0032177711410435995), (9, 0.0029627478172228098), (90, 0.0028718453158847394), (29, 0.0028433678416762884), (3, 0.002825287934790353), (214, 0.0028090011626860233), (195, 0.002736154100434893), (134, 0.002667571803024956), (243, 0.002515661380725162), (367, 0.002473076727665366), (28, 0.0024138918859515834), (302, 0.0022990605304228314), (329, 0.0022253668111758343), (215, 0.002209976918828354), (159, 0.0021170676445273653), (127, 0.002076069772330816), (78, 0.0018742504265586598), (344, 0.0018462692690374835), (149, 0.0018084261282351208), (330, 0.0018050740281383177), (154, 0.0017898407098350265), (321, 0.0017434383035947944), (322, 0.0017278841064005631), (270, 0.0017041329741568276), (340, 0.0016921928177675287), (355, 0.001673222055382357), (365, 0.0016119383051051736), (104, 0.001598735654470127), (254, 0.0015929749513983796), (248, 0.0015811117141221434), (244, 0.0015736649442797878), (345, 0.001560151543676371), (271, 0.0015402489012250242), (179, 0.001532713939746209), (274, 0.0015013636536146814), (44, 0.001424587851547453), (173, 0.001407164358879388), (36, 0.001385923788054851), (98, 0.001349158743279239), (182, 0.0013389436660048942), (114, 0.0013086443620864484), (327, 0.0012886190476232137), (378, 0.0012581507302222214), (37, 0.0012497552977830052), (324, 0.0012487653908679211), (136, 0.0012462715989041704), (219, 0.0011766213294328632), (259, 0.0011264402679374706), (118, 0.0011173608022694778), (164, 0.0010897368110286277), (268, 0.0010476370120552907), (20, 0.001039282712412212), (52, 0.0010338150191167659), (158, 0.0010220304532600106), (18, 0.001020911594207897), (0, 0.001020752474171494), (296, 0.0010104728841937266), (332, 0.0010047495124765138), (293, 0.000990466445766733), (83, 0.0009643460986473601), (297, 0.0009401426895487603), (201, 0.0009198516168733136), (144, 0.000893500653876329), (382, 0.0008712325846067316), (261, 0.0008236164928967709), (209, 0.0008138885744951368), (208, 0.0007828594451498511), (326, 0.0007711441332558098), (200, 0.000769473820147085), (53, 0.0007287268535350512), (314, 0.0007206319034417961), (88, 0.0007174520458611265), (349, 0.000711748354443815), (139, 0.0006960167764111751), (171, 0.0006937170829967499), (123, 0.0006791759751879236), (228, 0.0006680742684274794), (96, 0.0006503444657023917), (290, 0.000640188159978906), (55, 0.0006385764436849812), (285, 0.0006355043779857769), (43, 0.0006326930197263802), (265, 0.0006247594822315616), (288, 0.0006072862075888574), (57, 0.0006050843905490929), (380, 0.0006050764835364968), (51, 0.0005818681117077992), (122, 0.0005590590338729723), (41, 0.0005380798318701418), (319, 0.0005355148398153171), (71, 0.0005194080364959733), (315, 0.0005173051698449557), (85, 0.0005016336941831162), (226, 0.0004967135576784554), (263, 0.0004784422323752824), (276, 0.0004776315808153135), (280, 0.00047168585524611493), (61, 0.000470224191617985), (33, 0.00046165769499119277), (147, 0.0004492057049610076), (356, 0.00043775043682391383), (229, 0.00043683182852320517), (86, 0.00043670166549594373), (233, 0.00043392522663022944), (234, 0.0004299873529471152), (210, 0.00042595238268402423), (246, 0.00042260462537149497), (65, 0.00041689252679141396), (383, 0.00040856662773717015), (260, 0.0003998228042734473), (59, 0.0003982016776395377), (115, 0.00038279958645112775), (1, 0.0003755710026572345), (281, 0.00037179306431027123), (312, 0.00037142888033734353), (35, 0.0003650487011950421), (84, 0.000362233552380793), (202, 0.00036098890843387464), (4, 0.00036060873232489116), (105, 0.0003590355294871705), (13, 0.00035600774712170655), (239, 0.0003516221641510153), (103, 0.00034654915700309887), (335, 0.0003428909139144947), (279, 0.00033097707054187284), (153, 0.00033095849733030537), (250, 0.00032572040503903865), (334, 0.0003150282325214505), (255, 0.0003131963520533391), (32, 0.00031194334484436055), (80, 0.00031183612853505994), (206, 0.0003114142617059431), (313, 0.00030955271538629517), (343, 0.00030893731366560327), (360, 0.00030344299329618897), (232, 0.00029421269221286555), (292, 0.0002886048937675791), (77, 0.0002871285276972063), (130, 0.0002848184791166891), (282, 0.0002820564330058616), (185, 0.00027648569985454815), (242, 0.00027517047896659965), (17, 0.000274567803617792), (307, 0.00027074199322538027), (204, 0.0002630588916204975), (262, 0.00026065489952880537), (16, 0.0002578116643028502), (146, 0.00025474096092107393), (45, 0.0002518988466419158), (186, 0.000249407757235242), (353, 0.0002468455562004516), (107, 0.0002421755352976891), (50, 0.0002363543087972819), (178, 0.00023627354537886092), (152, 0.00022599229474370026), (369, 0.00022401488708209826), (93, 0.00021633786492009956), (26, 0.00021426661327383434), (128, 0.00021156236600326875), (169, 0.00021132840943841857), (73, 0.00021071692458154547), (252, 0.000210234626451265), (69, 0.00020967853857594424), (368, 0.00020961292155017937), (72, 0.00020690356101069017), (76, 0.0002065673839355716), (117, 0.00019603826667095113), (25, 0.00019602235308597017), (220, 0.0001951972705571707), (42, 0.00019473436089944117), (21, 0.00019281233543564044), (160, 0.00019235799554558546), (172, 0.00018585304885013527), (371, 0.00018177905679667811), (240, 0.00018092404911781445), (278, 0.0001787669903143626), (294, 0.00017844838076772892), (197, 0.00017725652643008137), (2, 0.00017659724925955935), (196, 0.00017657950179835564), (212, 0.0001760176430141885), (190, 0.0001753238154552819), (100, 0.00017475728138215235), (121, 0.00017451541082342054), (181, 0.0001737830089694799), (168, 0.00016928429795711966), (351, 0.0001689719910603611), (162, 0.00016800404949577217), (161, 0.00016768574848272764), (374, 0.0001674609059566066), (275, 0.00016404670493877336), (217, 0.0001623966541734858), (308, 0.00015951959483667452), (277, 0.00015916102432785825), (138, 0.00015912127938156524), (101, 0.00015733353837617112), (231, 0.00015621040502395895), (148, 0.00015101417583650778), (31, 0.00015010190352851744), (156, 0.00014973603812593026), (273, 0.00014863393167335957), (191, 0.00014794589425457628), (124, 0.0001476884440706983), (372, 0.00014699976030821144), (207, 0.0001461082305526046), (38, 0.0001460684719442544), (310, 0.00014568313285632161), (346, 0.00014390676986146775), (46, 0.00014172930514804997), (19, 0.0001411715044086953), (272, 0.00014057958079664615), (336, 0.00014040231564975525), (184, 0.00013935477960255144), (357, 0.00013621667770266556), (87, 0.00013478779869274723), (109, 0.00013232737743012663), (222, 0.0001311550350706938), (89, 0.00013066795433373508), (155, 0.0001268283680609822), (300, 0.00012387524466937484), (150, 0.00012110230409537686), (131, 0.00012059986331222714), (284, 0.00012057646005811188), (192, 0.00011913771148935796), (112, 0.00011717584244596321), (354, 0.00011702939645891612), (236, 0.00011627029128363584), (323, 0.0001155198525628679), (237, 0.00011509472934168372), (24, 0.00011420785733911647), (75, 0.00011307015817464977), (68, 0.00011278744377909561), (286, 0.00011275086057922288), (309, 0.00011109105141274485), (350, 0.00010834797005166952), (328, 0.0001079661809937998), (235, 0.00010793008724869471), (27, 0.0001070999112886045), (58, 0.00010643134957209856), (320, 0.00010440135180885853), (60, 0.00010325584852140307), (230, 0.00010286568872641875), (129, 9.879227712256659e-05), (317, 9.847732893004494e-05), (81, 9.84403767994797e-05), (15, 9.793862336927078e-05), (318, 9.603451288494905e-05), (64, 9.475664560603109e-05), (47, 9.387093160999488e-05), (40, 9.155973151288424e-05), (298, 9.134334570063585e-05), (264, 8.938519567439253e-05), (70, 8.889186657290466e-05), (299, 8.854640648604854e-05), (66, 8.373711403822454e-05), (14, 7.913459374479204e-05), (225, 7.773310222832319e-05), (56, 7.652138449579761e-05), (381, 7.644997477024067e-05), (306, 7.521867700231986e-05), (224, 7.496025306342076e-05), (113, 7.400496877915724e-05), (63, 7.151606768933382e-05), (305, 6.859718632262333e-05), (188, 6.820224822814432e-05), (133, 6.799580682426266e-05), (108, 6.50958546516575e-05), (174, 6.452661733492576e-05), (194, 6.368239213896312e-05), (331, 6.256642008557792e-05), (283, 6.137119810146692e-05), (110, 6.116150637405295e-05), (241, 5.8967726299711535e-05), (116, 5.6351075669413237e-05), (269, 5.558441450964238e-05), (145, 5.332832470306308e-05), (142, 5.308759275049786e-05), (287, 5.250497650417954e-05), (7, 4.934779341459842e-05), (303, 4.88510558527891e-05), (337, 4.770180573857293e-05), (22, 4.324716354281648e-05), (82, 4.31710371060199e-05), (333, 4.029610657724271e-05), (247, 4.0180554990443134e-05), (8, 3.996131910596444e-05), (193, 3.730401024081057e-05), (342, 3.698904080399037e-05), (377, 3.668574947177947e-05), (316, 3.6652365364842555e-05), (95, 3.63108615197813e-05), (176, 3.5619622047878456e-05), (135, 3.559273731773158e-05), (5, 3.303449208493964e-05), (10, 3.160061004618678e-05), (373, 3.093015262762421e-05), (79, 3.057997780925003e-05), (163, 3.017816572315205e-05), (216, 2.9010796366368437e-05), (165, 2.8440740038315406e-05), (177, 2.547215498915487e-05), (132, 2.4466441275619136e-05), (375, 2.379185543429124e-05), (361, 2.0959831005422905e-05), (257, 1.6883890233561585e-05), (362, 5.899580633554832e-06), (23, 0.0), (49, 0.0), (67, 0.0), (91, 0.0), (120, 0.0), (141, 0.0), (143, 0.0), (175, 0.0), (211, 0.0), (249, 0.0), (253, 0.0), (258, 0.0), (289, 0.0), (295, 0.0), (311, 0.0), (341, 0.0), (348, 0.0), (352, 0.0), (370, 0.0), (376, 0.0)], 384, 0.6931349954605769, 0.6931355841916206, 0.6931349954605769, 0.6931347616079018, 0.7581299001289842, 0.6901893287435457, 0.690209614102659, 0.6901893287435457, 0.6901810683983505, 0.7608891247127496, 0.6757865937072504, 0.6758339775783998, 0.6757865937072504, 0.675764749918121, 0.7428860709333204]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/antiwork/models_combine/'\n",
      "model_name: XGBClassifier\n",
      "class_name exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((66661, 385), (66661,))\n",
      "((19046, 385), (19046,))\n",
      "((9523, 385), (9523,))\n",
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "[08:17:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.65820\tvalidation_1-logloss:0.66360\n",
      "[1]\tvalidation_0-logloss:0.63550\tvalidation_1-logloss:0.64460\n",
      "[2]\tvalidation_0-logloss:0.61977\tvalidation_1-logloss:0.63341\n",
      "[3]\tvalidation_0-logloss:0.60806\tvalidation_1-logloss:0.62560\n",
      "[4]\tvalidation_0-logloss:0.59879\tvalidation_1-logloss:0.61984\n",
      "[5]\tvalidation_0-logloss:0.59097\tvalidation_1-logloss:0.61630\n",
      "[6]\tvalidation_0-logloss:0.58488\tvalidation_1-logloss:0.61314\n",
      "[7]\tvalidation_0-logloss:0.57918\tvalidation_1-logloss:0.61053\n",
      "[8]\tvalidation_0-logloss:0.57461\tvalidation_1-logloss:0.60857\n",
      "[9]\tvalidation_0-logloss:0.57034\tvalidation_1-logloss:0.60557\n",
      "[10]\tvalidation_0-logloss:0.56662\tvalidation_1-logloss:0.60408\n",
      "[11]\tvalidation_0-logloss:0.56302\tvalidation_1-logloss:0.60306\n",
      "[12]\tvalidation_0-logloss:0.55978\tvalidation_1-logloss:0.60245\n",
      "[13]\tvalidation_0-logloss:0.55667\tvalidation_1-logloss:0.60203\n",
      "[14]\tvalidation_0-logloss:0.55351\tvalidation_1-logloss:0.60157\n",
      "[15]\tvalidation_0-logloss:0.55089\tvalidation_1-logloss:0.60128\n",
      "[16]\tvalidation_0-logloss:0.54858\tvalidation_1-logloss:0.60115\n",
      "[17]\tvalidation_0-logloss:0.54608\tvalidation_1-logloss:0.60151\n",
      "[18]\tvalidation_0-logloss:0.54345\tvalidation_1-logloss:0.60118\n",
      "[19]\tvalidation_0-logloss:0.54083\tvalidation_1-logloss:0.60063\n",
      "[20]\tvalidation_0-logloss:0.53880\tvalidation_1-logloss:0.60018\n",
      "[21]\tvalidation_0-logloss:0.53661\tvalidation_1-logloss:0.60073\n",
      "[22]\tvalidation_0-logloss:0.53495\tvalidation_1-logloss:0.60059\n",
      "[23]\tvalidation_0-logloss:0.53283\tvalidation_1-logloss:0.60105\n",
      "[24]\tvalidation_0-logloss:0.53153\tvalidation_1-logloss:0.60139\n",
      "[25]\tvalidation_0-logloss:0.52927\tvalidation_1-logloss:0.60085\n",
      "[26]\tvalidation_0-logloss:0.52823\tvalidation_1-logloss:0.60092\n",
      "[27]\tvalidation_0-logloss:0.52692\tvalidation_1-logloss:0.60109\n",
      "[28]\tvalidation_0-logloss:0.52519\tvalidation_1-logloss:0.60256\n",
      "[29]\tvalidation_0-logloss:0.52342\tvalidation_1-logloss:0.60251\n",
      "---TRAIN---\n",
      "Accuracy: 73.19%\n",
      "precision: 73.20% \n",
      "recall: 73.19% \n",
      "fscore: 73.19% \n",
      "AUC 80.6946%\n",
      "---VALID---\n",
      "Accuracy: 69.63%\n",
      "precision: 69.63% \n",
      "recall: 69.63% \n",
      "fscore: 69.63% \n",
      "AUC 76.4149%\n",
      "---TEST---\n",
      "Accuracy: 67.25%\n",
      "precision: 67.25% \n",
      "recall: 67.25% \n",
      "fscore: 67.25% \n",
      "AUC 74.0989%\n",
      "TN:  1963 FP:  961  FN:  954  TP:  1970\n",
      "classifier_data:  [21, '06/09/2022', '08:18:10', '--', 'XGBClassifier', 'binary', 'exist', [('not_exist', 28638), ('exist', 28638)], [('not_exist', 2924), ('exist', 2924)], [('exist', 5229), ('not_exist', 5229)], 58439, [(227, 0.053272687), (99, 0.049310144), (157, 0.03448918), (304, 0.02108739), (251, 0.019311216), (189, 0.01799717), (221, 0.016351566), (256, 0.015504464), (359, 0.013258017), (6, 0.012751865), (218, 0.012174968), (267, 0.010688042), (37, 0.010111578), (199, 0.010102237), (102, 0.0097152935), (34, 0.009032126), (154, 0.008468849), (238, 0.008324976), (48, 0.008226625), (322, 0.008090231), (245, 0.007939048), (198, 0.0074780025), (347, 0.007284124), (92, 0.0072084866), (187, 0.006923761), (11, 0.0064928727), (302, 0.00640612), (291, 0.0059862696), (214, 0.005946001), (143, 0.005928337), (167, 0.005734372), (62, 0.005604254), (254, 0.0055558453), (213, 0.005136003), (274, 0.0049969186), (364, 0.0048916885), (41, 0.0047799153), (183, 0.004675969), (97, 0.004466806), (159, 0.0044018095), (119, 0.0043719714), (9, 0.004315113), (106, 0.00424357), (124, 0.004174569), (179, 0.004089478), (250, 0.003940649), (319, 0.003932977), (209, 0.003863225), (142, 0.0037655316), (127, 0.0037606573), (29, 0.003734781), (184, 0.003727638), (52, 0.003645748), (239, 0.0036019725), (173, 0.0035970542), (248, 0.0035620064), (334, 0.0035451523), (367, 0.003525879), (46, 0.0034118933), (236, 0.0033992939), (58, 0.003364321), (132, 0.0033581115), (293, 0.0033505082), (180, 0.0033349413), (166, 0.0033346843), (39, 0.0033294288), (325, 0.0032989928), (252, 0.0032778906), (44, 0.0032675345), (87, 0.0032588681), (355, 0.003249563), (63, 0.0031742237), (30, 0.003125459), (131, 0.0030355358), (2, 0.0030188793), (301, 0.002999503), (104, 0.0029471659), (20, 0.002941364), (346, 0.002895191), (259, 0.0028082726), (36, 0.0027884475), (80, 0.0027129273), (138, 0.0026997186), (280, 0.0026858402), (339, 0.002624771), (276, 0.0026115337), (257, 0.0025947827), (246, 0.002577568), (182, 0.0025441917), (244, 0.0025438527), (324, 0.0025338586), (114, 0.0025317655), (158, 0.0025177854), (201, 0.0025087767), (111, 0.0024755155), (13, 0.002453411), (61, 0.0024172682), (331, 0.0023365212), (309, 0.0023273632), (306, 0.0022823433), (266, 0.002281457), (224, 0.0022533825), (372, 0.0022354785), (178, 0.0022258214), (14, 0.0022246463), (223, 0.002224571), (118, 0.0022183484), (86, 0.0022028766), (219, 0.002167109), (265, 0.0021554166), (327, 0.002154807), (130, 0.0021171325), (278, 0.0021098747), (204, 0.0021053914), (313, 0.0021038463), (33, 0.0020878878), (352, 0.0020801611), (294, 0.0020708244), (195, 0.0020464244), (351, 0.002029903), (89, 0.0020281414), (90, 0.002025295), (78, 0.0020208298), (17, 0.0020155332), (27, 0.0020140447), (145, 0.0020120381), (340, 0.0020081091), (307, 0.0019975724), (3, 0.0019722984), (202, 0.0019687794), (210, 0.0019566028), (262, 0.0019543977), (152, 0.0019538933), (285, 0.0019483857), (77, 0.0019335451), (149, 0.0019317641), (353, 0.0019210929), (68, 0.0019202104), (365, 0.0019189152), (303, 0.0019160152), (249, 0.0019152139), (172, 0.0019061998), (310, 0.0019029672), (295, 0.0018831698), (357, 0.0018802108), (4, 0.0018529037), (297, 0.001839871), (112, 0.0018295229), (383, 0.0018204764), (150, 0.0018121275), (378, 0.0018083584), (374, 0.0018081084), (190, 0.0018054931), (100, 0.0017961502), (317, 0.0017767173), (326, 0.0017748941), (275, 0.0017718482), (1, 0.0017706127), (337, 0.0017657815), (350, 0.0017596321), (67, 0.0017519764), (147, 0.0017461507), (98, 0.0017450848), (146, 0.0017423313), (299, 0.0017363342), (136, 0.0017206082), (40, 0.0017203637), (292, 0.0017160513), (160, 0.0017137255), (229, 0.001711784), (356, 0.0017100599), (175, 0.0017084451), (288, 0.001703719), (314, 0.001703439), (222, 0.0016948234), (263, 0.0016929378), (220, 0.001688183), (234, 0.0016798648), (84, 0.0016790364), (73, 0.0016764229), (71, 0.0016660689), (76, 0.0016634986), (18, 0.0016589613), (342, 0.0016572289), (186, 0.0016566544), (369, 0.0016522002), (81, 0.0016500249), (7, 0.0016283897), (358, 0.0016279481), (289, 0.0016256743), (176, 0.0016189269), (24, 0.0016114403), (47, 0.0016114092), (260, 0.0016028291), (0, 0.0016018993), (137, 0.0015989854), (282, 0.0015821111), (272, 0.0015744191), (237, 0.0015706682), (75, 0.0015697179), (113, 0.0015683629), (35, 0.0015623682), (101, 0.0015620681), (151, 0.0015523506), (25, 0.0015359366), (83, 0.0015189195), (15, 0.0015104108), (65, 0.0015057465), (225, 0.0015037329), (161, 0.0015019948), (381, 0.0015010749), (140, 0.0015008731), (217, 0.0014989994), (330, 0.0014910748), (268, 0.0014893416), (155, 0.0014827126), (212, 0.0014754742), (141, 0.0014713374), (5, 0.0014674314), (206, 0.0014608628), (95, 0.0014579466), (121, 0.001451531), (88, 0.001445865), (232, 0.0014441398), (23, 0.0014438428), (110, 0.0014395104), (22, 0.0014345957), (129, 0.0014319544), (281, 0.0014280708), (120, 0.0014256255), (264, 0.001416778), (70, 0.0014162352), (60, 0.0014136196), (185, 0.0014132935), (54, 0.0014109191), (298, 0.0014007727), (109, 0.0013969736), (59, 0.0013922971), (380, 0.0013904683), (208, 0.0013877058), (363, 0.0013873148), (329, 0.001386455), (144, 0.0013846448), (21, 0.0013829584), (242, 0.0013821331), (116, 0.0013709996), (328, 0.0013699003), (53, 0.0013693328), (26, 0.0013652995), (56, 0.001364521), (64, 0.0013546156), (366, 0.0013494691), (375, 0.0013480209), (290, 0.0013437457), (371, 0.0013361502), (270, 0.001328174), (19, 0.0013248369), (72, 0.0013245887), (51, 0.0013108271), (345, 0.0013100057), (373, 0.0013005879), (164, 0.001299172), (382, 0.0012979513), (28, 0.001296659), (296, 0.0012938356), (174, 0.0012872007), (320, 0.0012843027), (286, 0.0012723602), (194, 0.0012678708), (107, 0.001267293), (379, 0.0012655009), (94, 0.0012611304), (338, 0.0012530426), (333, 0.0012378676), (134, 0.0012317117), (108, 0.0012252233), (205, 0.0012206116), (316, 0.0012056915), (181, 0.0012050206), (91, 0.0011925095), (377, 0.001188469), (215, 0.0011656052), (343, 0.0011622165), (191, 0.0011590518), (284, 0.0011500006), (255, 0.001146943), (287, 0.001141415), (196, 0.0011409519), (197, 0.001130386), (321, 0.0011279308), (96, 0.0011147417), (230, 0.0011064002), (125, 0.0011042862), (38, 0.0010960342), (193, 0.0010908246), (82, 0.0010888436), (156, 0.0010829708), (207, 0.0010717561), (148, 0.0010708733), (45, 0.0010652852), (162, 0.001063167), (163, 0.0010591581), (66, 0.0010504067), (203, 0.00102782), (133, 0.0010181201), (192, 0.0010156635), (12, 0.0010135476), (105, 0.0010041056), (235, 0.001002276), (43, 0.0009966894), (348, 0.0009890458), (122, 0.0009874671), (139, 0.0009846449), (243, 0.0009565636), (261, 0.0009453801), (55, 0.0008937073), (165, 0.0008923481), (368, 0.0008703321), (50, 0.000853167), (32, 0.0007765876), (344, 0.0007567725), (200, 0.00073173107), (31, 0.00073038373), (79, 0.0007278635), (370, 0.0007189213), (323, 0.00052448636), (115, 0.00047111948), (336, 0.00041668135), (228, 0.00013692169), (135, 6.1314734e-07), (8, 0.0), (10, 0.0), (16, 0.0), (42, 0.0), (49, 0.0), (57, 0.0), (69, 0.0), (74, 0.0), (85, 0.0), (93, 0.0), (103, 0.0), (117, 0.0), (123, 0.0), (126, 0.0), (128, 0.0), (153, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (177, 0.0), (188, 0.0), (211, 0.0), (216, 0.0), (226, 0.0), (231, 0.0), (233, 0.0), (240, 0.0), (241, 0.0), (247, 0.0), (253, 0.0), (258, 0.0), (269, 0.0), (271, 0.0), (273, 0.0), (277, 0.0), (279, 0.0), (283, 0.0), (300, 0.0), (305, 0.0), (308, 0.0), (311, 0.0), (312, 0.0), (315, 0.0), (318, 0.0), (332, 0.0), (335, 0.0), (341, 0.0), (349, 0.0), (354, 0.0), (360, 0.0), (361, 0.0), (362, 0.0), (376, 0.0)], 384, 0.7319470633424122, 0.7319616374681058, 0.7319470633424122, 0.7319428528417035, 0.8069460387550517, 0.6963090457066361, 0.6963100795810974, 0.6963090457066361, 0.6963086458561344, 0.7641488739063201, 0.6725376196990424, 0.6725386085427055, 0.6725376196990424, 0.6725371505148314, 0.7409894715370322]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/antiwork/models_combine/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------AskARussian-----------------------\n",
      "model_name: DecisionTreeClassifier\n",
      "class_name exist\n",
      "((4370, 385), (4370,))\n",
      "((1248, 385), (1248,))\n",
      "((626, 385), (626,))\n",
      "is_splited None\n",
      "here\n",
      "dec_tree_params {'criterion': 'gini', 'max_depth': 5}\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 78.48%\n",
      "precision: 78.65% \n",
      "recall: 78.48% \n",
      "fscore: 78.45% \n",
      "AUC 86.9659%\n",
      "---VALID---\n",
      "Accuracy: 56.47%\n",
      "precision: 56.52% \n",
      "recall: 56.47% \n",
      "fscore: 56.39% \n",
      "AUC 58.8185%\n",
      "---TEST---\n",
      "Accuracy: 52.72%\n",
      "precision: 52.74% \n",
      "recall: 52.72% \n",
      "fscore: 52.65% \n",
      "AUC 55.8945%\n",
      "TN:  135 FP:  104  FN:  122  TP:  117\n",
      "classifier_data:  [1, '06/09/2022', '08:20:44', 5, 'DecisionTreeClassifier', 'binary', 'exist', [('not_exist', 474), ('exist', 474)], [('exist', 239), ('not_exist', 239)], [('exist', 363), ('not_exist', 363)], 5168, [(382, 0.10023683403827367), (268, 0.08334338801551908), (98, 0.06173654260699455), (367, 0.04686913484881941), (258, 0.046215031279681754), (14, 0.04599847082211289), (282, 0.04546589904036048), (247, 0.04518147640933102), (296, 0.04320679860507916), (40, 0.04159849612647244), (285, 0.0381482535266923), (322, 0.03680951329290602), (156, 0.03641782826955831), (291, 0.035542110086349235), (249, 0.03333219760377514), (123, 0.03178583589740816), (9, 0.03060374777559831), (234, 0.029092586763734607), (223, 0.02803301374502403), (362, 0.027995755619366083), (326, 0.027137781160048634), (138, 0.024163136426687385), (257, 0.01817892200077895), (141, 0.017139526334406217), (207, 0.0159968912454458), (77, 0.009770828459576272), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (41, 0.0), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (60, 0.0), (61, 0.0), (62, 0.0), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.0), (76, 0.0), (78, 0.0), (79, 0.0), (80, 0.0), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.0), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (99, 0.0), (100, 0.0), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.0), (122, 0.0), (124, 0.0), (125, 0.0), (126, 0.0), (127, 0.0), (128, 0.0), (129, 0.0), (130, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (134, 0.0), (135, 0.0), (136, 0.0), (137, 0.0), (139, 0.0), (140, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (145, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (157, 0.0), (158, 0.0), (159, 0.0), (160, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (166, 0.0), (167, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (172, 0.0), (173, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (178, 0.0), (179, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (183, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (191, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0), (200, 0.0), (201, 0.0), (202, 0.0), (203, 0.0), (204, 0.0), (205, 0.0), (206, 0.0), (208, 0.0), (209, 0.0), (210, 0.0), (211, 0.0), (212, 0.0), (213, 0.0), (214, 0.0), (215, 0.0), (216, 0.0), (217, 0.0), (218, 0.0), (219, 0.0), (220, 0.0), (221, 0.0), (222, 0.0), (224, 0.0), (225, 0.0), (226, 0.0), (227, 0.0), (228, 0.0), (229, 0.0), (230, 0.0), (231, 0.0), (232, 0.0), (233, 0.0), (235, 0.0), (236, 0.0), (237, 0.0), (238, 0.0), (239, 0.0), (240, 0.0), (241, 0.0), (242, 0.0), (243, 0.0), (244, 0.0), (245, 0.0), (246, 0.0), (248, 0.0), (250, 0.0), (251, 0.0), (252, 0.0), (253, 0.0), (254, 0.0), (255, 0.0), (256, 0.0), (259, 0.0), (260, 0.0), (261, 0.0), (262, 0.0), (263, 0.0), (264, 0.0), (265, 0.0), (266, 0.0), (267, 0.0), (269, 0.0), (270, 0.0), (271, 0.0), (272, 0.0), (273, 0.0), (274, 0.0), (275, 0.0), (276, 0.0), (277, 0.0), (278, 0.0), (279, 0.0), (280, 0.0), (281, 0.0), (283, 0.0), (284, 0.0), (286, 0.0), (287, 0.0), (288, 0.0), (289, 0.0), (290, 0.0), (292, 0.0), (293, 0.0), (294, 0.0), (295, 0.0), (297, 0.0), (298, 0.0), (299, 0.0), (300, 0.0), (301, 0.0), (302, 0.0), (303, 0.0), (304, 0.0), (305, 0.0), (306, 0.0), (307, 0.0), (308, 0.0), (309, 0.0), (310, 0.0), (311, 0.0), (312, 0.0), (313, 0.0), (314, 0.0), (315, 0.0), (316, 0.0), (317, 0.0), (318, 0.0), (319, 0.0), (320, 0.0), (321, 0.0), (323, 0.0), (324, 0.0), (325, 0.0), (327, 0.0), (328, 0.0), (329, 0.0), (330, 0.0), (331, 0.0), (332, 0.0), (333, 0.0), (334, 0.0), (335, 0.0), (336, 0.0), (337, 0.0), (338, 0.0), (339, 0.0), (340, 0.0), (341, 0.0), (342, 0.0), (343, 0.0), (344, 0.0), (345, 0.0), (346, 0.0), (347, 0.0), (348, 0.0), (349, 0.0), (350, 0.0), (351, 0.0), (352, 0.0), (353, 0.0), (354, 0.0), (355, 0.0), (356, 0.0), (357, 0.0), (358, 0.0), (359, 0.0), (360, 0.0), (361, 0.0), (363, 0.0), (364, 0.0), (365, 0.0), (366, 0.0), (368, 0.0), (369, 0.0), (370, 0.0), (371, 0.0), (372, 0.0), (373, 0.0), (374, 0.0), (375, 0.0), (376, 0.0), (377, 0.0), (378, 0.0), (379, 0.0), (380, 0.0), (381, 0.0), (383, 0.0)], 384, 0.7848101265822784, 0.7864625302175665, 0.7848101265822784, 0.7844993581514762, 0.8696589755915185, 0.5647382920110193, 0.5652453248690199, 0.5647382920110193, 0.5638910221803168, 0.5881846261260236, 0.5271966527196653, 0.5273517967498283, 0.5271966527196653, 0.5265252454417952, 0.5589450464802788]\n",
      "model_name: RandomForestClassifier\n",
      "class_name exist\n",
      "((4370, 385), (4370,))\n",
      "((1248, 385), (1248,))\n",
      "((626, 385), (626,))\n",
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 93.04%\n",
      "precision: 93.29% \n",
      "recall: 93.04% \n",
      "fscore: 93.03% \n",
      "AUC 97.8970%\n",
      "---VALID---\n",
      "Accuracy: 66.25%\n",
      "precision: 66.37% \n",
      "recall: 66.25% \n",
      "fscore: 66.19% \n",
      "AUC 71.0129%\n",
      "---TEST---\n",
      "Accuracy: 64.85%\n",
      "precision: 65.33% \n",
      "recall: 64.85% \n",
      "fscore: 64.58% \n",
      "AUC 69.7904%\n",
      "TN:  134 FP:  105  FN:  63  TP:  176\n",
      "classifier_data:  [2, '06/09/2022', '08:20:45', '--', 'RandomForestClassifier', 'binary', 'exist', [('not_exist', 474), ('exist', 474)], [('exist', 239), ('not_exist', 239)], [('exist', 363), ('not_exist', 363)], 5168, [(251, 0.015894345155194497), (289, 0.013091563629582841), (268, 0.012239647369280399), (98, 0.010555557542352004), (233, 0.010176688377559806), (367, 0.010075027173736997), (382, 0.009604383841236151), (189, 0.00947597043009899), (296, 0.009378349605596017), (99, 0.008995751767659267), (63, 0.008769089562791871), (266, 0.008287860399105706), (291, 0.00811524429741568), (290, 0.00741881978457031), (335, 0.0073813874849992436), (166, 0.007372787756083581), (157, 0.007307344057882082), (280, 0.007052259703077665), (339, 0.006983688269294658), (369, 0.006769742368682862), (18, 0.006717195591651109), (345, 0.006634697048190462), (45, 0.006457067055326265), (364, 0.006448382115599187), (276, 0.006336404853925229), (117, 0.006186092363086168), (61, 0.006159491704137198), (172, 0.006147943927301959), (160, 0.00603084299755531), (125, 0.0059507935459677495), (213, 0.005795737212570593), (363, 0.005667188285818481), (212, 0.005536075548450825), (356, 0.005440082654348532), (196, 0.00532223767180383), (355, 0.0052757975336023555), (66, 0.005248594483469153), (138, 0.005248059055981541), (120, 0.005223971340329605), (28, 0.005126235757181914), (236, 0.005068088692888816), (21, 0.005050159058637548), (246, 0.004966801117163681), (147, 0.004896199908212132), (195, 0.0048924858829787745), (106, 0.004848211187497363), (110, 0.004845960770661371), (349, 0.004845322672078083), (379, 0.004678957771665044), (12, 0.00465853545160164), (90, 0.004584017604244466), (321, 0.004555679305909615), (325, 0.004518776272015925), (131, 0.004415424676638347), (170, 0.004397090505330278), (88, 0.004361679064535561), (92, 0.004359142525882634), (41, 0.004253923363530328), (153, 0.004187182832209695), (185, 0.004180939097232081), (279, 0.004138678131110581), (111, 0.004099088491126412), (178, 0.004090749715685741), (258, 0.004076281950761163), (17, 0.0040753992706707735), (53, 0.003939199024938876), (186, 0.0039053226242919347), (308, 0.0038813160460061053), (311, 0.0038293664660970783), (73, 0.0037472032870037504), (205, 0.0037394412661553287), (315, 0.0037289252350944523), (48, 0.0037268749785586077), (70, 0.003720547840030871), (29, 0.003709178648073783), (8, 0.0036985860659700166), (331, 0.003665099242014772), (26, 0.0036270362897772323), (214, 0.00362032837093617), (14, 0.003618919163230213), (118, 0.003614460468527546), (132, 0.003586777393416589), (65, 0.0035833014355974506), (215, 0.003538005291057925), (122, 0.0035265878712961883), (176, 0.0035187082040709296), (270, 0.0035043713443269555), (152, 0.0035028286366110095), (81, 0.003493352479656871), (223, 0.0034931172219761133), (263, 0.003456005934769498), (247, 0.0034427058027929093), (171, 0.0034341471733713913), (89, 0.003388330461664495), (383, 0.003380488988686959), (4, 0.003364019116214775), (184, 0.0033309824371169894), (151, 0.003323583342695117), (199, 0.0033225970828587455), (375, 0.003272450713567562), (286, 0.003261727036312869), (0, 0.0032529033348495124), (287, 0.0032083750713371682), (346, 0.0032023021814731045), (304, 0.0031635277712618337), (2, 0.0031007341169654854), (9, 0.0030860935023872913), (372, 0.003066449701060728), (144, 0.0030382698075925234), (43, 0.0030045671121830806), (123, 0.003001421214054048), (83, 0.002973657667541051), (230, 0.0029701670866762913), (149, 0.0029618781000961647), (254, 0.0029289367479030824), (57, 0.0029260085242428114), (358, 0.002925978529360143), (211, 0.0029221678581917727), (239, 0.0029165662024145957), (181, 0.0029156379217294554), (336, 0.002908998338064347), (310, 0.002907671182182767), (82, 0.002904547319698467), (175, 0.0028860685728722434), (179, 0.002876646926024275), (94, 0.0028308557896702304), (347, 0.002825199940211654), (177, 0.002817305232726154), (24, 0.0028112283864672703), (159, 0.0028062525398010268), (228, 0.002794947280138668), (257, 0.002778931039611632), (288, 0.0027658004594657585), (295, 0.0027643165810697125), (249, 0.0027605153389381977), (38, 0.0027570415181287188), (284, 0.002755558569710514), (55, 0.002715558160249194), (135, 0.002672206707805091), (130, 0.002657795639817363), (154, 0.002627613422007452), (252, 0.0026110083737883845), (173, 0.0026034421721140844), (209, 0.002598589986942264), (174, 0.0025954025251789707), (11, 0.0025887787894411434), (313, 0.002588467486288082), (241, 0.0025793220288810115), (281, 0.0025514411847064825), (79, 0.00252946284682396), (378, 0.0025142745095693155), (380, 0.0025134795381198103), (77, 0.0025087999097644013), (192, 0.0024881451853125715), (206, 0.0024859524263309194), (93, 0.0024464274786687615), (202, 0.0023856997656017593), (85, 0.0023692155657532234), (256, 0.002367000486769943), (191, 0.0023639564275328285), (95, 0.0023630235001099857), (5, 0.002334207341176405), (102, 0.002332059741623788), (243, 0.00231886247051656), (374, 0.00231555308375021), (274, 0.002315149120839121), (273, 0.0023104325165542953), (116, 0.002309898495221781), (370, 0.002307915740185684), (352, 0.002273260764832697), (277, 0.0022711908466375723), (282, 0.0022697764805719458), (221, 0.002250766278447084), (322, 0.0022386005606194414), (238, 0.0022379057508810436), (333, 0.0022361861967822986), (91, 0.0022324646526203223), (217, 0.002231665638232359), (193, 0.002204044525391123), (167, 0.002194106948710408), (22, 0.002178695528424585), (139, 0.002173289305863699), (71, 0.002159710801523822), (275, 0.002152515251304522), (64, 0.0021291356200763565), (340, 0.0021103012836713514), (260, 0.0021077085231835106), (366, 0.0020929713471455657), (13, 0.002045122161481641), (104, 0.002035853991021343), (162, 0.002026857223542027), (232, 0.0020266441967927273), (293, 0.002009720820065621), (115, 0.0020095001289214347), (320, 0.00199356950927859), (305, 0.001990750598880581), (326, 0.00198974024116398), (108, 0.001939494916613551), (1, 0.0019377498783925559), (49, 0.0019242914837472528), (337, 0.0019143055412200825), (76, 0.0019113531879895187), (27, 0.0018938692171905711), (33, 0.00188413378245678), (109, 0.0018751855608917733), (112, 0.0018720978968132856), (377, 0.0018437333354000632), (259, 0.0018380565197785772), (127, 0.0018127869177111725), (354, 0.0018126606714265476), (105, 0.001809995804938473), (84, 0.001793795426822566), (136, 0.0017916652425827373), (54, 0.0017894244339276939), (25, 0.00177743079737442), (224, 0.0017691610571398146), (301, 0.001768943829906017), (318, 0.0017576319541042298), (19, 0.0017555199091764564), (227, 0.0017326727346233767), (300, 0.0017265944689094687), (244, 0.0017208197809610631), (316, 0.001719593839710751), (113, 0.0017114880594812306), (72, 0.0017100168350819565), (126, 0.0017089200689037632), (200, 0.0017087566630710966), (344, 0.001699663062028027), (37, 0.00169747390088273), (248, 0.001687535676046732), (198, 0.0016808608596408877), (103, 0.001671647492875386), (225, 0.0016629611258301414), (59, 0.001655469003590082), (140, 0.0016535611417839693), (121, 0.0016520975590335377), (20, 0.001651586900311815), (332, 0.0016487669824095125), (267, 0.0016287216020703082), (216, 0.001607886429871792), (208, 0.0015852855840240738), (3, 0.0015773014749068206), (324, 0.0015729065617429028), (298, 0.001567770908496679), (23, 0.0015661013034662026), (231, 0.0015534385080934964), (334, 0.0015124010706157173), (10, 0.0014692191333094539), (35, 0.0014663906002479366), (323, 0.0014624673194424162), (361, 0.0014610258092702282), (250, 0.001457807382904644), (68, 0.0014519071718936715), (188, 0.001448011186200131), (190, 0.0014456320782571936), (357, 0.0014409553612664606), (87, 0.0014319196159203159), (134, 0.0014246769057514779), (114, 0.0014243948414708984), (255, 0.0014232265527217875), (161, 0.0014088718149139654), (78, 0.0014069339374910125), (218, 0.001396048334511003), (201, 0.0013955361741619314), (16, 0.0013867630412105502), (74, 0.0013776689101802177), (42, 0.0013764953420751702), (197, 0.0013754756807520944), (169, 0.0013688915127286008), (342, 0.0013593672865938817), (52, 0.001354555568036531), (360, 0.0013533983329303315), (158, 0.001347650262352332), (330, 0.0013422977196445175), (376, 0.0013412773500493754), (261, 0.0013222609191353906), (328, 0.0013196457803086432), (302, 0.001314446566050383), (203, 0.0013039398360616145), (234, 0.0012859784804477986), (222, 0.001283339604249489), (210, 0.001268885913747917), (62, 0.0012663652080782347), (145, 0.0012590077050912488), (240, 0.0012565974554226832), (180, 0.0012408659787429101), (371, 0.0012278983097696397), (46, 0.001222567946774257), (142, 0.0012166573198882063), (237, 0.0012161086695152536), (283, 0.001198525056620384), (165, 0.0011950780667450895), (183, 0.0011888648818988055), (31, 0.0011784758847468745), (235, 0.0011775485525078412), (306, 0.0011728992624183622), (353, 0.0011670343890433804), (44, 0.0011512461603694617), (60, 0.0011401330187064825), (133, 0.0011257200755642293), (229, 0.001121562701159324), (150, 0.0011188749450720401), (80, 0.001118718724199761), (285, 0.0011151069776574344), (309, 0.0011041525915195716), (381, 0.0010846304609225682), (272, 0.0010641668247123263), (242, 0.001059286617101287), (307, 0.001058373230775652), (155, 0.0010436970515500606), (141, 0.0010398402707503037), (164, 0.0010265659341738822), (341, 0.0010128755320292424), (253, 0.0010066557632931795), (50, 0.0010041187037555445), (317, 0.000985064679884141), (30, 0.0009678775424649963), (204, 0.0009639741702095847), (51, 0.0009533368120331035), (39, 0.0009475217572814825), (146, 0.0009419144033891643), (100, 0.0009282832168374775), (373, 0.0009129623824068957), (314, 0.0008986039509029374), (124, 0.0008889460882616448), (119, 0.000887576187080682), (362, 0.0008763492771929386), (168, 0.0008663974120468873), (343, 0.0008491776320653809), (129, 0.0007908518631051352), (220, 0.0007894309258701587), (278, 0.0007557335814878225), (194, 0.0007543885610433604), (128, 0.0007377565483497198), (338, 0.0007377326099995532), (219, 0.0007342871910037265), (143, 0.0007085454700364655), (265, 0.0006946238186835392), (245, 0.0006892218077504857), (292, 0.0006869682076224095), (163, 0.0006865779541731193), (148, 0.0006834517863233534), (40, 0.0006722991452121698), (264, 0.0006662704177809747), (226, 0.0006444866999352854), (67, 0.0006097391242660205), (368, 0.0006044832910862469), (329, 0.000571623880132903), (359, 0.0005605409556032041), (269, 0.0005569178182896401), (137, 0.000549245119623387), (365, 0.0005422416112507072), (207, 0.0005394699958006584), (58, 0.0005387916610934491), (7, 0.0005370484866104079), (6, 0.0005263077015751255), (303, 0.0005249693651184672), (34, 0.000523288276582344), (156, 0.0005079688084955281), (299, 0.00049880296900837), (297, 0.0004877633246721394), (97, 0.0004786319481185784), (86, 0.0004676277005401852), (312, 0.00046645635128890613), (15, 0.00043536301283412653), (327, 0.0004291988962933403), (96, 0.00040619871408997365), (69, 0.0003570695302140912), (319, 0.00033551358266923416), (36, 0.0003312444053305694), (262, 0.00032732992827583186), (182, 0.00032438074784454484), (348, 0.00030983782672764043), (75, 0.00030861958712709146), (350, 0.000270305320801396), (32, 0.00021515585383879627), (351, 0.00020652113158189898), (101, 0.00019559983238051477), (56, 0.00012100497111921186), (187, 0.0001131637111998325), (47, 0.0), (107, 0.0), (271, 0.0), (294, 0.0)], 384, 0.930379746835443, 0.9328767123287671, 0.930379746835443, 0.9302792041078305, 0.9789697163916039, 0.662534435261708, 0.6637285181334475, 0.662534435261708, 0.6619180217252882, 0.7101290895430641, 0.6485355648535565, 0.6532687826291165, 0.6485355648535565, 0.6458009880028229, 0.6979044484515327]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/AskARussian/models_combine/'\n",
      "model_name: XGBClassifier\n",
      "class_name exist\n",
      "((4370, 385), (4370,))\n",
      "((1248, 385), (1248,))\n",
      "((626, 385), (626,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "[08:20:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.59553\tvalidation_1-logloss:0.69345\n",
      "[1]\tvalidation_0-logloss:0.52257\tvalidation_1-logloss:0.68312\n",
      "[2]\tvalidation_0-logloss:0.46172\tvalidation_1-logloss:0.68920\n",
      "[3]\tvalidation_0-logloss:0.41078\tvalidation_1-logloss:0.68792\n",
      "[4]\tvalidation_0-logloss:0.37009\tvalidation_1-logloss:0.68525\n",
      "[5]\tvalidation_0-logloss:0.34777\tvalidation_1-logloss:0.69307\n",
      "[6]\tvalidation_0-logloss:0.32647\tvalidation_1-logloss:0.69709\n",
      "[7]\tvalidation_0-logloss:0.29381\tvalidation_1-logloss:0.69643\n",
      "[8]\tvalidation_0-logloss:0.27306\tvalidation_1-logloss:0.69120\n",
      "[9]\tvalidation_0-logloss:0.25128\tvalidation_1-logloss:0.69307\n",
      "[10]\tvalidation_0-logloss:0.22229\tvalidation_1-logloss:0.70220\n",
      "---TRAIN---\n",
      "Accuracy: 85.76%\n",
      "precision: 85.76% \n",
      "recall: 85.76% \n",
      "fscore: 85.76% \n",
      "AUC 92.9367%\n",
      "---VALID---\n",
      "Accuracy: 57.58%\n",
      "precision: 57.64% \n",
      "recall: 57.58% \n",
      "fscore: 57.48% \n",
      "AUC 61.9129%\n",
      "---TEST---\n",
      "Accuracy: 56.49%\n",
      "precision: 56.54% \n",
      "recall: 56.49% \n",
      "fscore: 56.39% \n",
      "AUC 59.4291%\n",
      "TN:  124 FP:  115  FN:  93  TP:  146\n",
      "classifier_data:  [3, '06/09/2022', '08:20:46', '--', 'XGBClassifier', 'binary', 'exist', [('not_exist', 474), ('exist', 474)], [('exist', 239), ('not_exist', 239)], [('exist', 363), ('not_exist', 363)], 5168, [(382, 0.02401851), (14, 0.015706353), (247, 0.015691115), (369, 0.015385486), (268, 0.01447023), (266, 0.012907719), (285, 0.012865997), (25, 0.0122504), (156, 0.0120833665), (98, 0.011506399), (181, 0.01132156), (154, 0.011053626), (262, 0.01102086), (84, 0.010711103), (335, 0.010674262), (248, 0.010629916), (258, 0.010559644), (18, 0.010419827), (58, 0.010321004), (291, 0.010079183), (296, 0.010072716), (256, 0.009624959), (70, 0.009522183), (27, 0.009515302), (125, 0.0093310615), (82, 0.008895057), (155, 0.008705976), (279, 0.008598905), (289, 0.008566235), (249, 0.008561734), (321, 0.0084269075), (165, 0.0083595775), (135, 0.008264519), (117, 0.00817923), (57, 0.008069039), (233, 0.008057302), (236, 0.008048213), (362, 0.008041238), (234, 0.007979841), (220, 0.007933395), (322, 0.007909185), (122, 0.007875014), (123, 0.0077359607), (276, 0.007632747), (270, 0.0075745042), (372, 0.007528676), (239, 0.007477102), (196, 0.0073654405), (177, 0.0073096086), (7, 0.0072891614), (118, 0.0071291625), (367, 0.0071284124), (40, 0.0070964107), (337, 0.0070134983), (8, 0.006977116), (29, 0.006947239), (246, 0.0068018404), (223, 0.006779245), (218, 0.0066870507), (310, 0.0066120136), (333, 0.0064419275), (138, 0.0063500013), (228, 0.0062761963), (147, 0.0062614987), (126, 0.0061684647), (110, 0.0060588694), (257, 0.0060486123), (95, 0.006047642), (339, 0.0059118546), (162, 0.005878583), (366, 0.0057941484), (52, 0.0057383506), (22, 0.0056769243), (271, 0.005592939), (90, 0.0055834064), (13, 0.005468808), (64, 0.0053949403), (240, 0.005384769), (139, 0.005371531), (358, 0.0053663617), (65, 0.00525823), (316, 0.0052456725), (144, 0.005222571), (356, 0.005169509), (250, 0.005141083), (166, 0.005139898), (178, 0.0051339925), (63, 0.004990677), (101, 0.0049557276), (340, 0.00481578), (345, 0.0047199023), (41, 0.0045513944), (171, 0.004532316), (274, 0.004530236), (202, 0.0044293376), (359, 0.00441537), (23, 0.0043992363), (134, 0.004392237), (9, 0.004382477), (212, 0.004373928), (160, 0.004346812), (251, 0.004288253), (172, 0.0042866035), (265, 0.0042336565), (31, 0.004050716), (51, 0.0039646705), (102, 0.0038883123), (37, 0.0038867963), (111, 0.0038504396), (308, 0.0038469785), (175, 0.0038285041), (151, 0.003788987), (137, 0.0037620363), (99, 0.0037337588), (78, 0.003722735), (252, 0.0036797395), (272, 0.0035880983), (349, 0.0035600374), (334, 0.0035456796), (157, 0.0034933465), (205, 0.0034866496), (260, 0.003477505), (191, 0.0034662173), (21, 0.0034605695), (179, 0.003445714), (375, 0.00341897), (69, 0.0033886384), (87, 0.003376957), (304, 0.003371718), (238, 0.0033403551), (197, 0.0033371833), (277, 0.00329003), (97, 0.0032490746), (88, 0.0032133479), (148, 0.003210299), (342, 0.0031851411), (127, 0.0031099631), (338, 0.0031032627), (351, 0.0030631428), (61, 0.0029995353), (341, 0.0029094748), (183, 0.0028975233), (192, 0.002847405), (10, 0.0028305042), (317, 0.002820427), (120, 0.0027983733), (28, 0.0027133438), (264, 0.0027038932), (219, 0.0026291765), (180, 0.0025862507), (227, 0.0025827568), (199, 0.0025255745), (2, 0.0025019175), (261, 0.0024889638), (0, 0.0024838627), (152, 0.0024215993), (141, 0.0023191387), (47, 0.0022505736), (214, 0.0021658333), (185, 0.0020087552), (326, 0.0019730253), (263, 0.0017164022), (357, 0.0016843079), (16, 0.0015811886), (208, 0.0015773657), (331, 0.0015487052), (55, 0.001496574), (26, 0.0013638476), (93, 0.0011594491), (130, 0.0010691747), (329, 0.0010103948), (5, 0.0009380384), (66, 0.00088374387), (59, 0.0008602442), (336, 0.0008285361), (354, 0.00065165054), (360, 0.0003334559), (184, 0.00022199517), (15, 0.00016936846), (176, 9.857779e-05), (363, 9.342005e-05), (1, 0.0), (3, 0.0), (4, 0.0), (6, 0.0), (11, 0.0), (12, 0.0), (17, 0.0), (19, 0.0), (20, 0.0), (24, 0.0), (30, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (38, 0.0), (39, 0.0), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (53, 0.0), (54, 0.0), (56, 0.0), (60, 0.0), (62, 0.0), (67, 0.0), (68, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.0), (76, 0.0), (77, 0.0), (79, 0.0), (80, 0.0), (81, 0.0), (83, 0.0), (85, 0.0), (86, 0.0), (89, 0.0), (91, 0.0), (92, 0.0), (94, 0.0), (96, 0.0), (100, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (119, 0.0), (121, 0.0), (124, 0.0), (128, 0.0), (129, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (136, 0.0), (140, 0.0), (142, 0.0), (143, 0.0), (145, 0.0), (146, 0.0), (149, 0.0), (150, 0.0), (153, 0.0), (158, 0.0), (159, 0.0), (161, 0.0), (163, 0.0), (164, 0.0), (167, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (173, 0.0), (174, 0.0), (182, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (198, 0.0), (200, 0.0), (201, 0.0), (203, 0.0), (204, 0.0), (206, 0.0), (207, 0.0), (209, 0.0), (210, 0.0), (211, 0.0), (213, 0.0), (215, 0.0), (216, 0.0), (217, 0.0), (221, 0.0), (222, 0.0), (224, 0.0), (225, 0.0), (226, 0.0), (229, 0.0), (230, 0.0), (231, 0.0), (232, 0.0), (235, 0.0), (237, 0.0), (241, 0.0), (242, 0.0), (243, 0.0), (244, 0.0), (245, 0.0), (253, 0.0), (254, 0.0), (255, 0.0), (259, 0.0), (267, 0.0), (269, 0.0), (273, 0.0), (275, 0.0), (278, 0.0), (280, 0.0), (281, 0.0), (282, 0.0), (283, 0.0), (284, 0.0), (286, 0.0), (287, 0.0), (288, 0.0), (290, 0.0), (292, 0.0), (293, 0.0), (294, 0.0), (295, 0.0), (297, 0.0), (298, 0.0), (299, 0.0), (300, 0.0), (301, 0.0), (302, 0.0), (303, 0.0), (305, 0.0), (306, 0.0), (307, 0.0), (309, 0.0), (311, 0.0), (312, 0.0), (313, 0.0), (314, 0.0), (315, 0.0), (318, 0.0), (319, 0.0), (320, 0.0), (323, 0.0), (324, 0.0), (325, 0.0), (327, 0.0), (328, 0.0), (330, 0.0), (332, 0.0), (343, 0.0), (344, 0.0), (346, 0.0), (347, 0.0), (348, 0.0), (350, 0.0), (352, 0.0), (353, 0.0), (355, 0.0), (361, 0.0), (364, 0.0), (365, 0.0), (368, 0.0), (370, 0.0), (371, 0.0), (373, 0.0), (374, 0.0), (376, 0.0), (377, 0.0), (378, 0.0), (379, 0.0), (380, 0.0), (381, 0.0), (383, 0.0)], 384, 0.8575949367088608, 0.8576092617073269, 0.8575949367088608, 0.857593510590356, 0.9293671776246685, 0.5757575757575758, 0.5764280737751984, 0.5757575757575758, 0.5748250684514755, 0.6191289301732579, 0.5648535564853556, 0.565407772304324, 0.5648535564853556, 0.5639298245614035, 0.5942910663328724]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/AskARussian/models_combine/'\n",
      "-----------------------------cryptocurrency-----------------------\n",
      "model_name: DecisionTreeClassifier\n",
      "class_name exist\n",
      "((34183, 385), (34183,))\n",
      "((9766, 385), (9766,))\n",
      "((4884, 385), (4884,))\n",
      "is_splited None\n",
      "here\n",
      "dec_tree_params {'criterion': 'gini', 'max_depth': 5}\n",
      "---------------------- exist -------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TRAIN---\n",
      "Accuracy: 66.42%\n",
      "precision: 68.08% \n",
      "recall: 66.42% \n",
      "fscore: 65.62% \n",
      "AUC 71.5373%\n",
      "---VALID---\n",
      "Accuracy: 61.81%\n",
      "precision: 62.61% \n",
      "recall: 61.81% \n",
      "fscore: 61.20% \n",
      "AUC 66.0739%\n",
      "---TEST---\n",
      "Accuracy: 65.34%\n",
      "precision: 66.04% \n",
      "recall: 65.34% \n",
      "fscore: 64.96% \n",
      "AUC 69.2893%\n",
      "TN:  766 FP:  629  FN:  338  TP:  1057\n",
      "classifier_data:  [1, '06/09/2022', '08:24:28', 5, 'DecisionTreeClassifier', 'binary', 'exist', [('exist', 13780), ('not_exist', 13780)], [('exist', 1395), ('not_exist', 1395)], [('not_exist', 4250), ('exist', 4250)], 19425, [(157, 0.4870414861249479), (294, 0.12010611904287083), (99, 0.1026613387465569), (278, 0.08963209460420161), (233, 0.039885974157529085), (6, 0.02556656834645002), (367, 0.0245958444054303), (127, 0.021146765843123173), (259, 0.014315557655266318), (191, 0.01191264510554013), (224, 0.009173099408101692), (54, 0.008829776859456939), (126, 0.007672415716559255), (62, 0.007448495251016379), (314, 0.0057233656695995), (377, 0.004110480708808235), (82, 0.00407008480325464), (263, 0.003891958680188542), (3, 0.0036160607413372975), (260, 0.0030916463162564364), (212, 0.0030604791014796563), (207, 0.0024477427120253104), (0, 0.0), (1, 0.0), (2, 0.0), (4, 0.0), (5, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (60, 0.0), (61, 0.0), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.0), (76, 0.0), (77, 0.0), (78, 0.0), (79, 0.0), (80, 0.0), (81, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.0), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (100, 0.0), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.0), (122, 0.0), (123, 0.0), (124, 0.0), (125, 0.0), (128, 0.0), (129, 0.0), (130, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (134, 0.0), (135, 0.0), (136, 0.0), (137, 0.0), (138, 0.0), (139, 0.0), (140, 0.0), (141, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (145, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (156, 0.0), (158, 0.0), (159, 0.0), (160, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (166, 0.0), (167, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (172, 0.0), (173, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (178, 0.0), (179, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (183, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0), (200, 0.0), (201, 0.0), (202, 0.0), (203, 0.0), (204, 0.0), (205, 0.0), (206, 0.0), (208, 0.0), (209, 0.0), (210, 0.0), (211, 0.0), (213, 0.0), (214, 0.0), (215, 0.0), (216, 0.0), (217, 0.0), (218, 0.0), (219, 0.0), (220, 0.0), (221, 0.0), (222, 0.0), (223, 0.0), (225, 0.0), (226, 0.0), (227, 0.0), (228, 0.0), (229, 0.0), (230, 0.0), (231, 0.0), (232, 0.0), (234, 0.0), (235, 0.0), (236, 0.0), (237, 0.0), (238, 0.0), (239, 0.0), (240, 0.0), (241, 0.0), (242, 0.0), (243, 0.0), (244, 0.0), (245, 0.0), (246, 0.0), (247, 0.0), (248, 0.0), (249, 0.0), (250, 0.0), (251, 0.0), (252, 0.0), (253, 0.0), (254, 0.0), (255, 0.0), (256, 0.0), (257, 0.0), (258, 0.0), (261, 0.0), (262, 0.0), (264, 0.0), (265, 0.0), (266, 0.0), (267, 0.0), (268, 0.0), (269, 0.0), (270, 0.0), (271, 0.0), (272, 0.0), (273, 0.0), (274, 0.0), (275, 0.0), (276, 0.0), (277, 0.0), (279, 0.0), (280, 0.0), (281, 0.0), (282, 0.0), (283, 0.0), (284, 0.0), (285, 0.0), (286, 0.0), (287, 0.0), (288, 0.0), (289, 0.0), (290, 0.0), (291, 0.0), (292, 0.0), (293, 0.0), (295, 0.0), (296, 0.0), (297, 0.0), (298, 0.0), (299, 0.0), (300, 0.0), (301, 0.0), (302, 0.0), (303, 0.0), (304, 0.0), (305, 0.0), (306, 0.0), (307, 0.0), (308, 0.0), (309, 0.0), (310, 0.0), (311, 0.0), (312, 0.0), (313, 0.0), (315, 0.0), (316, 0.0), (317, 0.0), (318, 0.0), (319, 0.0), (320, 0.0), (321, 0.0), (322, 0.0), (323, 0.0), (324, 0.0), (325, 0.0), (326, 0.0), (327, 0.0), (328, 0.0), (329, 0.0), (330, 0.0), (331, 0.0), (332, 0.0), (333, 0.0), (334, 0.0), (335, 0.0), (336, 0.0), (337, 0.0), (338, 0.0), (339, 0.0), (340, 0.0), (341, 0.0), (342, 0.0), (343, 0.0), (344, 0.0), (345, 0.0), (346, 0.0), (347, 0.0), (348, 0.0), (349, 0.0), (350, 0.0), (351, 0.0), (352, 0.0), (353, 0.0), (354, 0.0), (355, 0.0), (356, 0.0), (357, 0.0), (358, 0.0), (359, 0.0), (360, 0.0), (361, 0.0), (362, 0.0), (363, 0.0), (364, 0.0), (365, 0.0), (366, 0.0), (368, 0.0), (369, 0.0), (370, 0.0), (371, 0.0), (372, 0.0), (373, 0.0), (374, 0.0), (375, 0.0), (376, 0.0), (378, 0.0), (379, 0.0), (380, 0.0), (381, 0.0), (382, 0.0), (383, 0.0)], 384, 0.6641509433962264, 0.6808208608407589, 0.6641509433962264, 0.6562278343103487, 0.7153725109064062, 0.6181176470588235, 0.6260794015982625, 0.6181176470588235, 0.6119920951071207, 0.6607388788927335, 0.6534050179211469, 0.6603841095466502, 0.6534050179211469, 0.6495930407216441, 0.692893205380198]\n",
      "model_name: RandomForestClassifier\n",
      "class_name exist\n",
      "((34183, 385), (34183,))\n",
      "((9766, 385), (9766,))\n",
      "((4884, 385), (4884,))\n",
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 71.02%\n",
      "precision: 72.07% \n",
      "recall: 71.02% \n",
      "fscore: 70.67% \n",
      "AUC 76.8678%\n",
      "---VALID---\n",
      "Accuracy: 66.12%\n",
      "precision: 66.26% \n",
      "recall: 66.12% \n",
      "fscore: 66.04% \n",
      "AUC 71.5014%\n",
      "---TEST---\n",
      "Accuracy: 69.75%\n",
      "precision: 69.79% \n",
      "recall: 69.75% \n",
      "fscore: 69.73% \n",
      "AUC 75.0266%\n",
      "TN:  941 FP:  454  FN:  390  TP:  1005\n",
      "classifier_data:  [2, '06/09/2022', '08:24:50', '--', 'RandomForestClassifier', 'binary', 'exist', [('exist', 13780), ('not_exist', 13780)], [('exist', 1395), ('not_exist', 1395)], [('not_exist', 4250), ('exist', 4250)], 19425, [(157, 0.07723298441185837), (99, 0.07131291748489052), (294, 0.03750543472945722), (338, 0.03251780009857314), (164, 0.027835752673631798), (126, 0.025607643920892313), (11, 0.025541140809999033), (6, 0.0246482153245207), (50, 0.02393187813017981), (383, 0.02331740485760896), (49, 0.022692911960821474), (233, 0.022621074676087045), (278, 0.02223990660565307), (54, 0.020871457472670565), (36, 0.020151356361159854), (185, 0.020091695587913357), (62, 0.018105443765579314), (380, 0.016216747778092892), (10, 0.016046889635902912), (256, 0.01339561634698372), (137, 0.013101434089415486), (1, 0.012993563423770806), (351, 0.012024896525116182), (339, 0.011717010062297703), (368, 0.010228730147210029), (78, 0.010196232939822913), (260, 0.008711670326975736), (334, 0.008435397997008922), (158, 0.008192325528632222), (127, 0.008074505604366261), (218, 0.008043173925254828), (17, 0.00795409304266081), (97, 0.007889388289959312), (91, 0.0074215719136356455), (272, 0.0072083467553888236), (104, 0.007113467787394953), (251, 0.00706575537176123), (315, 0.006831635319805652), (367, 0.006827433485772496), (230, 0.006669773255380854), (301, 0.006242234824673358), (271, 0.005575044909221084), (201, 0.005443927272103535), (308, 0.005354388834397643), (274, 0.005325250134957205), (184, 0.005227082231019073), (74, 0.005157628485223728), (197, 0.005014249082722915), (108, 0.004944642624341932), (223, 0.004902138102521872), (175, 0.0048113733577407215), (211, 0.004498548236702025), (179, 0.00437428038434571), (347, 0.004348436256047742), (0, 0.004218696275053773), (46, 0.0038543011562092388), (199, 0.003840943668744918), (28, 0.0037306736919440076), (312, 0.0035521435827364617), (66, 0.0032508933202998074), (209, 0.0032344320923283613), (369, 0.002981893011135424), (252, 0.002939104969757208), (111, 0.002915590812503852), (19, 0.002871937664707509), (59, 0.002621490610962164), (228, 0.0025956391947646994), (177, 0.002554925398073875), (183, 0.002552604195664873), (290, 0.002499215063370956), (247, 0.0023812270469798745), (293, 0.002299551958350039), (57, 0.0022936526442601766), (224, 0.002213852546392807), (326, 0.0022058680752184067), (154, 0.002196845156110509), (325, 0.0021656837265208242), (288, 0.00208179575976295), (153, 0.0019693593688729747), (35, 0.001829686807603542), (273, 0.0018192666135825315), (219, 0.0018101041355748698), (71, 0.0017817421503999505), (82, 0.001778369659303466), (200, 0.0017570481800294596), (249, 0.001706231765237925), (292, 0.0016311534052031863), (365, 0.0015867307879401397), (136, 0.0015713466555527028), (266, 0.001545025330667728), (70, 0.0015342383004150343), (267, 0.0015113401509986302), (29, 0.001457127244173718), (240, 0.0013570771937627128), (331, 0.0013548565015948728), (212, 0.0012437262288212438), (112, 0.0012419137173822364), (171, 0.0012277599466852984), (79, 0.0012178230257132501), (48, 0.001204209503941259), (69, 0.0011963575210570748), (259, 0.001194779827417136), (194, 0.0011535089381775492), (134, 0.0011478119901132489), (202, 0.00107984686369185), (147, 0.0010733345398886826), (8, 0.00106871822953953), (371, 0.00106051665613462), (360, 0.0010545898930030377), (26, 0.0010284893315897182), (244, 0.001025077839815509), (191, 0.0010231561594439002), (214, 0.0010119700913032328), (302, 0.001002066203087118), (376, 0.000997950672274928), (65, 0.0009692046626748073), (284, 0.0009658728819522673), (221, 0.0009362323088623958), (255, 0.0009351774933452645), (95, 0.0009109105025126145), (160, 0.0009030795929314971), (248, 0.0008958160866800399), (94, 0.000889564137867362), (254, 0.0008764074323354283), (314, 0.0008405569193109605), (67, 0.0008001664654738255), (182, 0.0007788480104998161), (263, 0.0007772687397541643), (63, 0.0007715280450269525), (270, 0.0007532473236432176), (139, 0.0007501035500790875), (242, 0.0007441143559982142), (361, 0.0007385769066361468), (291, 0.0007155606620409653), (165, 0.0007136504539856387), (363, 0.0006997708548018137), (204, 0.0006975591811637406), (216, 0.0006883899100052628), (239, 0.0006694481162225217), (275, 0.0006575165720748612), (189, 0.0006527580027347034), (176, 0.000644828445126999), (217, 0.0006095576820250066), (140, 0.0006094046176503225), (64, 0.0005891093287819289), (342, 0.0005883896027519851), (180, 0.0005759595010977155), (317, 0.0005693677654359681), (377, 0.0005677140795066969), (374, 0.000553040661214193), (311, 0.000549922405674055), (282, 0.0005354055571871278), (234, 0.0005181145919522021), (190, 0.0005171110094595665), (105, 0.0005158157150927542), (379, 0.0005145873578837459), (213, 0.0005143721566035685), (359, 0.0005128863560399315), (186, 0.0004960265192222112), (168, 0.0004927506324445935), (120, 0.00048776966629202425), (344, 0.00048757984312556673), (210, 0.0004844431293294242), (279, 0.0004733650943873603), (14, 0.00046896263085644446), (43, 0.0004562474736684963), (332, 0.00045427768010419995), (237, 0.0004468129812053275), (141, 0.00044474751263295746), (196, 0.0004433678982250813), (299, 0.0004308478388697786), (9, 0.0004273491623860696), (163, 0.00042456519962511685), (22, 0.0004235901571022478), (353, 0.0004217543983001505), (155, 0.0004171700491570923), (281, 0.0004151103860312137), (124, 0.0004074030779446551), (101, 0.0003927177082798582), (357, 0.00038681484987330793), (269, 0.00038297784671205876), (85, 0.0003822091733672877), (107, 0.0003814590106874969), (349, 0.0003773831173457841), (16, 0.0003690794821309154), (125, 0.00036550137999088897), (27, 0.00036339191039208514), (51, 0.00036333533200604625), (310, 0.00036116791543287346), (55, 0.00036087123322778736), (346, 0.00036006450341704687), (109, 0.00035755885566378533), (122, 0.00035507329369349527), (145, 0.00035131762935032286), (3, 0.0003504056016628057), (356, 0.0003404274089069308), (205, 0.00033396183945703033), (320, 0.00033003836644921407), (235, 0.0003228845218103931), (130, 0.000319502258154537), (18, 0.00031693101817023886), (343, 0.0003161158421261516), (61, 0.000309779011803708), (146, 0.00030810009788538297), (309, 0.000303002495598099), (341, 0.00030135152823470727), (131, 0.00029944947421659793), (276, 0.0002922483154731282), (265, 0.0002884116213365465), (159, 0.00028574297122290534), (13, 0.00028478272900913065), (333, 0.0002842725899977921), (226, 0.0002775533319816071), (21, 0.0002744025489544586), (243, 0.00027110435742595546), (90, 0.0002702027130622752), (327, 0.00026960291375726996), (220, 0.00026901138466190844), (150, 0.0002669689108319029), (304, 0.00026286101767199796), (81, 0.00026197270996569037), (143, 0.0002610906290059278), (152, 0.00025841345210510756), (258, 0.0002573151243891489), (83, 0.0002568803362415786), (98, 0.0002544746120217799), (323, 0.0002527055947875345), (350, 0.00024113471400332586), (41, 0.00024059045646760587), (207, 0.00024046489994896146), (47, 0.00023840394188409507), (33, 0.00023438443175136147), (337, 0.00023393497020563302), (106, 0.00023202424862153728), (287, 0.0002317893614569651), (133, 0.0002276339040845733), (113, 0.00022676704716334564), (329, 0.00022578904907070326), (366, 0.00022191307968474528), (321, 0.00022147850578157493), (52, 0.00021929299096852092), (132, 0.00020921545720103043), (34, 0.0002050490884778774), (236, 0.00020479963284397221), (296, 0.00020464023042563367), (192, 0.00020250001989378575), (44, 0.0002020313127437116), (261, 0.0002005237822982395), (298, 0.00019915008611678722), (114, 0.00019609895593125163), (31, 0.00019529575460095112), (12, 0.00019489136655537212), (103, 0.00019322979321604716), (151, 0.0001911539039435232), (156, 0.00018959859315505702), (277, 0.00018738878766028726), (181, 0.00018733993028926927), (232, 0.00018712446361177133), (92, 0.0001827093061163195), (88, 0.00018115161578427852), (76, 0.00018069653586638967), (37, 0.00017764399974286565), (225, 0.000177108168823801), (45, 0.0001756022370203442), (345, 0.00017442512190812799), (245, 0.00017249584005817945), (300, 0.00016855758673944734), (264, 0.00016474092000134558), (215, 0.00016372658349184113), (313, 0.00016333192717365988), (23, 0.00016268639539951553), (73, 0.00016020393233207017), (56, 0.0001575807677467204), (358, 0.00015619955821910423), (354, 0.00015503213034405477), (285, 0.0001530803236739945), (289, 0.00015236055887288976), (373, 0.00015156563939403546), (306, 0.0001505440760528321), (283, 0.00014858896323655533), (231, 0.00014831523741721994), (229, 0.00014795180620043018), (174, 0.00014598660418374058), (167, 0.0001454801606931297), (355, 0.00014202701792985755), (58, 0.00014001278771855815), (193, 0.00013855268785241704), (129, 0.00013803081905189017), (87, 0.0001349634099932591), (382, 0.00013327731878647704), (303, 0.00013291747790473445), (60, 0.00012912381120490046), (378, 0.00012829029639272184), (370, 0.0001281197783684594), (246, 0.00012662407898934925), (372, 0.00012252955392991687), (149, 0.00012200099789783587), (2, 0.00012076747182624591), (375, 0.0001188329079445454), (118, 0.00011686029577303131), (4, 0.0001140421184352434), (172, 0.00011316456222639869), (128, 0.00011272167156882192), (340, 0.00011184051777775676), (222, 0.00010834014427791307), (336, 0.00010824122951859163), (188, 0.00010793046097490857), (5, 0.00010784642641550888), (144, 0.00010725876256407689), (286, 0.00010513606246193796), (305, 0.00010296484781038917), (173, 9.911561967093264e-05), (86, 9.765088218879217e-05), (24, 9.142387854328816e-05), (77, 8.978755595160038e-05), (324, 8.545393510771551e-05), (322, 8.431999508522212e-05), (15, 8.39978889134817e-05), (178, 8.366571938539779e-05), (75, 8.160908343119037e-05), (7, 8.158191587701998e-05), (238, 7.910635146759633e-05), (206, 7.740163475285156e-05), (20, 7.721427105642161e-05), (38, 7.627326260689785e-05), (123, 7.625221512787001e-05), (162, 7.452306654520366e-05), (187, 7.378527932275085e-05), (135, 7.153918976560115e-05), (262, 7.153215260804527e-05), (250, 7.115827217383203e-05), (121, 6.980342315273636e-05), (119, 6.977091268551132e-05), (42, 6.689722847831483e-05), (110, 6.164010821820493e-05), (335, 6.062369433559058e-05), (30, 5.913888883480973e-05), (198, 5.846334487829679e-05), (40, 5.811558994611739e-05), (115, 5.804251792830657e-05), (241, 5.688057043512527e-05), (319, 5.430983762384653e-05), (96, 5.3739759834138434e-05), (295, 5.357477075890111e-05), (362, 5.2887132678171825e-05), (364, 4.9695957143515566e-05), (161, 4.869417894540038e-05), (116, 4.403214167374528e-05), (93, 4.134992168584182e-05), (307, 4.063980356218829e-05), (166, 3.988595733090596e-05), (203, 3.972458844209413e-05), (142, 3.813499902405067e-05), (330, 3.628845638676731e-05), (268, 3.3773679561052995e-05), (208, 3.213481014033569e-05), (80, 3.1767257889575223e-05), (381, 3.1105477278048065e-05), (68, 3.09465576105312e-05), (195, 2.8600683133180544e-05), (170, 2.0992954533723966e-05), (328, 1.9430766744696155e-05), (39, 1.6172827727519518e-05), (227, 1.2842988443681868e-05), (89, 1.0603899826103838e-05), (316, 1.049505626516571e-05), (348, 1.0466700206209366e-05), (25, 0.0), (32, 0.0), (53, 0.0), (72, 0.0), (84, 0.0), (100, 0.0), (102, 0.0), (117, 0.0), (138, 0.0), (148, 0.0), (169, 0.0), (253, 0.0), (257, 0.0), (280, 0.0), (297, 0.0), (318, 0.0), (352, 0.0)], 384, 0.7101959361393324, 0.7206760862257438, 0.7101959361393324, 0.706713815254632, 0.7686777838983319, 0.6611764705882353, 0.6625736907659373, 0.6611764705882353, 0.6604469084606477, 0.7150143944636678, 0.6974910394265234, 0.6979075960037674, 0.6974910394265234, 0.6973317751507583, 0.7502660551637312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/cryptocurrency/models_combine/'\n",
      "model_name: XGBClassifier\n",
      "class_name exist\n",
      "((34183, 385), (34183,))\n",
      "((9766, 385), (9766,))\n",
      "((4884, 385), (4884,))\n",
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "[08:24:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.65235\tvalidation_1-logloss:0.66114\n",
      "[1]\tvalidation_0-logloss:0.62499\tvalidation_1-logloss:0.64222\n",
      "[2]\tvalidation_0-logloss:0.60555\tvalidation_1-logloss:0.62836\n",
      "[3]\tvalidation_0-logloss:0.59029\tvalidation_1-logloss:0.61787\n",
      "[4]\tvalidation_0-logloss:0.57873\tvalidation_1-logloss:0.60895\n",
      "[5]\tvalidation_0-logloss:0.56813\tvalidation_1-logloss:0.60150\n",
      "[6]\tvalidation_0-logloss:0.55809\tvalidation_1-logloss:0.59579\n",
      "[7]\tvalidation_0-logloss:0.55015\tvalidation_1-logloss:0.59145\n",
      "[8]\tvalidation_0-logloss:0.54316\tvalidation_1-logloss:0.58580\n",
      "[9]\tvalidation_0-logloss:0.53602\tvalidation_1-logloss:0.58281\n",
      "[10]\tvalidation_0-logloss:0.52903\tvalidation_1-logloss:0.58063\n",
      "[11]\tvalidation_0-logloss:0.52266\tvalidation_1-logloss:0.57717\n",
      "[12]\tvalidation_0-logloss:0.51829\tvalidation_1-logloss:0.57392\n",
      "[13]\tvalidation_0-logloss:0.51257\tvalidation_1-logloss:0.57347\n",
      "[14]\tvalidation_0-logloss:0.50763\tvalidation_1-logloss:0.57232\n",
      "[15]\tvalidation_0-logloss:0.50259\tvalidation_1-logloss:0.57129\n",
      "[16]\tvalidation_0-logloss:0.49868\tvalidation_1-logloss:0.56859\n",
      "[17]\tvalidation_0-logloss:0.49446\tvalidation_1-logloss:0.56794\n",
      "[18]\tvalidation_0-logloss:0.49068\tvalidation_1-logloss:0.56688\n",
      "[19]\tvalidation_0-logloss:0.48607\tvalidation_1-logloss:0.56551\n",
      "[20]\tvalidation_0-logloss:0.48293\tvalidation_1-logloss:0.56442\n",
      "[21]\tvalidation_0-logloss:0.47818\tvalidation_1-logloss:0.56429\n",
      "[22]\tvalidation_0-logloss:0.47445\tvalidation_1-logloss:0.56272\n",
      "[23]\tvalidation_0-logloss:0.47058\tvalidation_1-logloss:0.56082\n",
      "[24]\tvalidation_0-logloss:0.46801\tvalidation_1-logloss:0.56079\n",
      "[25]\tvalidation_0-logloss:0.46452\tvalidation_1-logloss:0.55958\n",
      "[26]\tvalidation_0-logloss:0.46105\tvalidation_1-logloss:0.55918\n",
      "[27]\tvalidation_0-logloss:0.45822\tvalidation_1-logloss:0.55997\n",
      "[28]\tvalidation_0-logloss:0.45614\tvalidation_1-logloss:0.55936\n",
      "[29]\tvalidation_0-logloss:0.45258\tvalidation_1-logloss:0.55879\n",
      "[30]\tvalidation_0-logloss:0.44912\tvalidation_1-logloss:0.55724\n",
      "[31]\tvalidation_0-logloss:0.44664\tvalidation_1-logloss:0.55685\n",
      "[32]\tvalidation_0-logloss:0.44325\tvalidation_1-logloss:0.55758\n",
      "[33]\tvalidation_0-logloss:0.44091\tvalidation_1-logloss:0.55757\n",
      "[34]\tvalidation_0-logloss:0.43743\tvalidation_1-logloss:0.55766\n",
      "[35]\tvalidation_0-logloss:0.43410\tvalidation_1-logloss:0.55824\n",
      "[36]\tvalidation_0-logloss:0.43235\tvalidation_1-logloss:0.55755\n",
      "[37]\tvalidation_0-logloss:0.42914\tvalidation_1-logloss:0.55725\n",
      "[38]\tvalidation_0-logloss:0.42534\tvalidation_1-logloss:0.55700\n",
      "[39]\tvalidation_0-logloss:0.42291\tvalidation_1-logloss:0.55708\n",
      "[40]\tvalidation_0-logloss:0.41956\tvalidation_1-logloss:0.55823\n",
      "[41]\tvalidation_0-logloss:0.41726\tvalidation_1-logloss:0.55731\n",
      "---TRAIN---\n",
      "Accuracy: 80.13%\n",
      "precision: 80.63% \n",
      "recall: 80.13% \n",
      "fscore: 80.05% \n",
      "AUC 88.3454%\n",
      "---VALID---\n",
      "Accuracy: 68.12%\n",
      "precision: 68.12% \n",
      "recall: 68.12% \n",
      "fscore: 68.12% \n",
      "AUC 75.5321%\n",
      "---TEST---\n",
      "Accuracy: 71.83%\n",
      "precision: 72.05% \n",
      "recall: 71.83% \n",
      "fscore: 71.76% \n",
      "AUC 79.1846%\n",
      "TN:  1072 FP:  323  FN:  463  TP:  932\n",
      "classifier_data:  [3, '06/09/2022', '08:25:15', '--', 'XGBClassifier', 'binary', 'exist', [('exist', 13780), ('not_exist', 13780)], [('exist', 1395), ('not_exist', 1395)], [('not_exist', 4250), ('exist', 4250)], 19425, [(157, 0.05012824), (294, 0.018701758), (278, 0.015075853), (369, 0.012053741), (11, 0.010959007), (19, 0.01036278), (185, 0.009670427), (256, 0.009656818), (10, 0.009367967), (99, 0.009323116), (220, 0.008954315), (233, 0.00890805), (108, 0.00806957), (49, 0.007501879), (164, 0.0074748565), (91, 0.0074090674), (273, 0.0071301437), (367, 0.0070486544), (50, 0.006353411), (0, 0.0063285064), (339, 0.0060843523), (126, 0.006010026), (308, 0.0057642455), (59, 0.005698587), (383, 0.0056533464), (351, 0.0055505335), (315, 0.0052233767), (136, 0.0052046524), (79, 0.0051007313), (127, 0.005095198), (368, 0.005026011), (6, 0.0049872464), (244, 0.004750004), (54, 0.004721808), (144, 0.00468543), (259, 0.0045681736), (296, 0.0045576133), (265, 0.0044135964), (36, 0.0043885955), (1, 0.004257565), (212, 0.004210047), (85, 0.004207934), (189, 0.004179016), (230, 0.0041665463), (187, 0.0041347486), (62, 0.0040836697), (194, 0.003971754), (331, 0.0039213877), (137, 0.0038657172), (279, 0.0037487382), (155, 0.0037117873), (191, 0.0036895522), (274, 0.0036074722), (290, 0.0036020214), (178, 0.0035734756), (375, 0.0035413215), (252, 0.0034732572), (175, 0.0034560354), (182, 0.0034342995), (169, 0.0034314706), (94, 0.0034274135), (261, 0.0034162477), (249, 0.003373628), (197, 0.0033672259), (224, 0.0033647215), (67, 0.003326052), (299, 0.0033203156), (237, 0.0033117603), (363, 0.003281217), (69, 0.0032383194), (204, 0.003237666), (104, 0.003227911), (58, 0.0032216285), (17, 0.0032067248), (223, 0.0031910394), (66, 0.0031595305), (229, 0.0031286294), (326, 0.0031253067), (147, 0.0030774602), (334, 0.0030723757), (271, 0.0030460851), (354, 0.0030367742), (202, 0.0029732564), (184, 0.0029723276), (41, 0.002957664), (325, 0.0029427246), (106, 0.0029402948), (78, 0.00291765), (181, 0.0029093786), (18, 0.0029080363), (272, 0.0029014924), (323, 0.0028890655), (276, 0.0028620202), (269, 0.0028360528), (200, 0.002831809), (218, 0.0028210997), (283, 0.0028119176), (139, 0.0027964606), (102, 0.002794228), (179, 0.002787798), (150, 0.0027844987), (366, 0.00277736), (228, 0.0027712444), (51, 0.0027653642), (16, 0.0027476887), (56, 0.0027424193), (211, 0.0027398276), (365, 0.0027388772), (360, 0.0027254156), (132, 0.0027105685), (266, 0.0026880559), (203, 0.0026808959), (316, 0.002673805), (254, 0.0026400662), (287, 0.0026268885), (89, 0.00262155), (128, 0.0025976426), (343, 0.0025944195), (105, 0.0025913191), (264, 0.0025908868), (142, 0.0025779284), (135, 0.0025764192), (380, 0.0025508378), (305, 0.0025233636), (4, 0.0025143954), (134, 0.002509866), (103, 0.0025020519), (45, 0.0024984605), (346, 0.0024966374), (125, 0.002492619), (46, 0.0024898078), (344, 0.0024758067), (55, 0.0024670744), (5, 0.0024600308), (297, 0.002431442), (245, 0.002424657), (356, 0.0024056993), (340, 0.0023966432), (28, 0.0023931467), (158, 0.002392395), (314, 0.0023897635), (86, 0.0023739974), (192, 0.002372704), (33, 0.0023716), (190, 0.0023616396), (160, 0.0023595763), (30, 0.002356001), (227, 0.0023441745), (341, 0.0023359375), (27, 0.0023324739), (47, 0.002329663), (338, 0.0023294832), (289, 0.0023285665), (122, 0.002328532), (275, 0.0023145678), (288, 0.0023131534), (358, 0.0023097969), (188, 0.0023056713), (140, 0.0022818444), (345, 0.0022690261), (174, 0.00226143), (87, 0.0022493673), (145, 0.002249013), (96, 0.0022467), (163, 0.002246394), (151, 0.0022330252), (304, 0.0022307322), (301, 0.002221859), (9, 0.0022215487), (63, 0.0022209596), (97, 0.002212657), (153, 0.0021904733), (210, 0.002189119), (282, 0.0021856376), (373, 0.0021855503), (313, 0.0021704817), (241, 0.0021465907), (43, 0.0021447658), (3, 0.0021441386), (247, 0.0021375543), (98, 0.0021290819), (263, 0.0021225254), (113, 0.0021131025), (22, 0.0021095674), (225, 0.0021082787), (309, 0.00210655), (219, 0.0021033578), (72, 0.0020994474), (84, 0.002087605), (48, 0.002085154), (312, 0.0020845023), (101, 0.0020727103), (357, 0.0020640695), (143, 0.002048939), (329, 0.0020390248), (149, 0.0020321195), (165, 0.002030094), (31, 0.0020278995), (206, 0.0020253325), (138, 0.002020703), (198, 0.0020162521), (298, 0.0020152084), (267, 0.0020140985), (156, 0.0020073415), (77, 0.0020045019), (146, 0.0019981374), (88, 0.0019860382), (21, 0.0019639556), (217, 0.0019604971), (25, 0.0019555588), (111, 0.0019541201), (337, 0.0019506473), (32, 0.0019381698), (35, 0.0019375621), (112, 0.0019367151), (110, 0.0019320999), (193, 0.0019301744), (34, 0.0019274361), (172, 0.0019254504), (342, 0.0019227004), (71, 0.0019221364), (348, 0.0019210861), (377, 0.0019200497), (362, 0.0019154596), (300, 0.0019144281), (118, 0.001907957), (23, 0.001907923), (207, 0.0018907734), (295, 0.0018768191), (349, 0.0018639843), (336, 0.0018585246), (61, 0.0018531224), (352, 0.0018395309), (306, 0.0018386154), (68, 0.0018317098), (208, 0.0018305775), (292, 0.0018294689), (361, 0.001825894), (381, 0.0018249541), (124, 0.0018243089), (141, 0.0018144494), (26, 0.0018097338), (107, 0.0018089797), (250, 0.001806449), (37, 0.0018014613), (44, 0.0018005575), (291, 0.0017932379), (92, 0.0017874928), (201, 0.0017813273), (170, 0.0017783424), (221, 0.0017645403), (173, 0.0017549825), (15, 0.0017524762), (2, 0.0017401599), (115, 0.0017384693), (8, 0.0017348314), (65, 0.0017296044), (307, 0.0017256758), (38, 0.0017138161), (121, 0.0017052467), (176, 0.0016990945), (240, 0.0016860765), (280, 0.0016845951), (129, 0.0016773611), (130, 0.001666447), (374, 0.0016578551), (243, 0.0016486235), (242, 0.0016362224), (319, 0.0016346992), (76, 0.0016339344), (186, 0.001631436), (116, 0.0016284726), (355, 0.0016262123), (302, 0.0016214558), (248, 0.0016196517), (317, 0.0016191208), (40, 0.0016071037), (246, 0.0015910664), (93, 0.0015904409), (154, 0.0015889584), (60, 0.0015859072), (364, 0.0015720273), (119, 0.0015678103), (180, 0.0015624693), (372, 0.0015529746), (123, 0.0015472729), (42, 0.0015347714), (371, 0.001531971), (330, 0.0015279202), (114, 0.0015195715), (95, 0.0015149971), (24, 0.0015100973), (183, 0.0015045423), (234, 0.0015029337), (83, 0.0015007125), (327, 0.0014972157), (347, 0.0014932582), (311, 0.0014718743), (13, 0.0014700897), (120, 0.0014664251), (260, 0.0014637503), (277, 0.0014539212), (382, 0.0014396777), (168, 0.001416178), (281, 0.001414343), (379, 0.0014096175), (131, 0.0013971495), (293, 0.0013949901), (117, 0.0013839238), (82, 0.0013750774), (232, 0.0013475221), (166, 0.0013387024), (64, 0.0013336869), (20, 0.0013307948), (251, 0.0013180705), (209, 0.0013051687), (222, 0.001298099), (159, 0.0012710127), (213, 0.0012432955), (162, 0.0012388865), (171, 0.0012367751), (335, 0.0012252515), (214, 0.001215726), (286, 0.0012111798), (14, 0.0012082256), (12, 0.0011924573), (320, 0.0011860197), (199, 0.0011472652), (177, 0.0011394979), (70, 0.0011392958), (253, 0.0010803943), (333, 0.0010749433), (52, 0.0010677993), (258, 0.0010055478), (321, 0.0009931257), (196, 0.0009877785), (370, 0.00096516043), (215, 0.00095668365), (332, 0.000951916), (152, 0.0009037656), (80, 0.00087259535), (310, 0.0008625801), (235, 0.000858848), (226, 0.00083678815), (376, 0.00083037495), (53, 0.00081855), (328, 0.0007346176), (195, 0.0006882079), (90, 0.00053566246), (257, 0.00046358368), (100, 0.00045248755), (39, 0.00035712836), (133, 0.00031241903), (285, 0.00010010073), (74, 7.878474e-05), (7, 0.0), (29, 0.0), (57, 0.0), (73, 0.0), (75, 0.0), (81, 0.0), (109, 0.0), (148, 0.0), (161, 0.0), (167, 0.0), (205, 0.0), (216, 0.0), (231, 0.0), (236, 0.0), (238, 0.0), (239, 0.0), (255, 0.0), (262, 0.0), (268, 0.0), (270, 0.0), (284, 0.0), (303, 0.0), (318, 0.0), (322, 0.0), (324, 0.0), (350, 0.0), (353, 0.0), (359, 0.0), (378, 0.0)], 384, 0.8012699564586357, 0.8062948948637834, 0.8012699564586357, 0.8004515305030631, 0.883454089349323, 0.6811764705882353, 0.6811822483584805, 0.6811764705882353, 0.6811739288053009, 0.7553208027681662, 0.7182795698924731, 0.7205004087882996, 0.7182795698924731, 0.7175684203747827, 0.7918464562377153]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/cryptocurrency/models_combine/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------UkrainianConflict-----------------------\n",
      "model_name: DecisionTreeClassifier\n",
      "class_name exist\n",
      "((19108, 385), (19108,))\n",
      "((5459, 385), (5459,))\n",
      "((2731, 385), (2731,))\n",
      "is_splited None\n",
      "here\n",
      "dec_tree_params {'criterion': 'gini', 'max_depth': 5}\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 65.48%\n",
      "precision: 66.56% \n",
      "recall: 65.48% \n",
      "fscore: 64.90% \n",
      "AUC 69.8345%\n",
      "---VALID---\n",
      "Accuracy: 61.97%\n",
      "precision: 62.77% \n",
      "recall: 61.97% \n",
      "fscore: 61.37% \n",
      "AUC 65.0346%\n",
      "---TEST---\n",
      "Accuracy: 59.40%\n",
      "precision: 60.86% \n",
      "recall: 59.40% \n",
      "fscore: 57.99% \n",
      "AUC 62.9419%\n",
      "TN:  177 FP:  254  FN:  96  TP:  335\n",
      "classifier_data:  [1, '06/09/2022', '08:29:04', 5, 'DecisionTreeClassifier', 'binary', 'exist', [('exist', 5353), ('not_exist', 5353)], [('not_exist', 431), ('exist', 431)], [('exist', 1182), ('not_exist', 1182)], 20332, [(99, 0.2521901322671957), (134, 0.10838015768151506), (135, 0.08440302795402282), (125, 0.06517921069110032), (177, 0.06060190463430212), (377, 0.04266018452368486), (63, 0.0418695956352858), (48, 0.030432537707804012), (172, 0.029744080260865505), (291, 0.029410520000526696), (98, 0.02322177393395652), (275, 0.022073517401904604), (78, 0.018565969754939186), (11, 0.018461223166667505), (145, 0.01790238035664445), (292, 0.016937807474528603), (335, 0.01670858275622136), (268, 0.015162038959799755), (329, 0.014476094910484223), (129, 0.013310572865046848), (309, 0.012538896235245383), (294, 0.011436079520176043), (216, 0.010758551220809397), (251, 0.009766673787308952), (164, 0.009526348332313639), (158, 0.00679154434315932), (272, 0.006466539769076242), (167, 0.005669154263160985), (283, 0.005354899592254054), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (60, 0.0), (61, 0.0), (62, 0.0), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (73, 0.0), (74, 0.0), (75, 0.0), (76, 0.0), (77, 0.0), (79, 0.0), (80, 0.0), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.0), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (100, 0.0), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.0), (122, 0.0), (123, 0.0), (124, 0.0), (126, 0.0), (127, 0.0), (128, 0.0), (130, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (136, 0.0), (137, 0.0), (138, 0.0), (139, 0.0), (140, 0.0), (141, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (156, 0.0), (157, 0.0), (159, 0.0), (160, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (165, 0.0), (166, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (173, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (178, 0.0), (179, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (183, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (191, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0), (200, 0.0), (201, 0.0), (202, 0.0), (203, 0.0), (204, 0.0), (205, 0.0), (206, 0.0), (207, 0.0), (208, 0.0), (209, 0.0), (210, 0.0), (211, 0.0), (212, 0.0), (213, 0.0), (214, 0.0), (215, 0.0), (217, 0.0), (218, 0.0), (219, 0.0), (220, 0.0), (221, 0.0), (222, 0.0), (223, 0.0), (224, 0.0), (225, 0.0), (226, 0.0), (227, 0.0), (228, 0.0), (229, 0.0), (230, 0.0), (231, 0.0), (232, 0.0), (233, 0.0), (234, 0.0), (235, 0.0), (236, 0.0), (237, 0.0), (238, 0.0), (239, 0.0), (240, 0.0), (241, 0.0), (242, 0.0), (243, 0.0), (244, 0.0), (245, 0.0), (246, 0.0), (247, 0.0), (248, 0.0), (249, 0.0), (250, 0.0), (252, 0.0), (253, 0.0), (254, 0.0), (255, 0.0), (256, 0.0), (257, 0.0), (258, 0.0), (259, 0.0), (260, 0.0), (261, 0.0), (262, 0.0), (263, 0.0), (264, 0.0), (265, 0.0), (266, 0.0), (267, 0.0), (269, 0.0), (270, 0.0), (271, 0.0), (273, 0.0), (274, 0.0), (276, 0.0), (277, 0.0), (278, 0.0), (279, 0.0), (280, 0.0), (281, 0.0), (282, 0.0), (284, 0.0), (285, 0.0), (286, 0.0), (287, 0.0), (288, 0.0), (289, 0.0), (290, 0.0), (293, 0.0), (295, 0.0), (296, 0.0), (297, 0.0), (298, 0.0), (299, 0.0), (300, 0.0), (301, 0.0), (302, 0.0), (303, 0.0), (304, 0.0), (305, 0.0), (306, 0.0), (307, 0.0), (308, 0.0), (310, 0.0), (311, 0.0), (312, 0.0), (313, 0.0), (314, 0.0), (315, 0.0), (316, 0.0), (317, 0.0), (318, 0.0), (319, 0.0), (320, 0.0), (321, 0.0), (322, 0.0), (323, 0.0), (324, 0.0), (325, 0.0), (326, 0.0), (327, 0.0), (328, 0.0), (330, 0.0), (331, 0.0), (332, 0.0), (333, 0.0), (334, 0.0), (336, 0.0), (337, 0.0), (338, 0.0), (339, 0.0), (340, 0.0), (341, 0.0), (342, 0.0), (343, 0.0), (344, 0.0), (345, 0.0), (346, 0.0), (347, 0.0), (348, 0.0), (349, 0.0), (350, 0.0), (351, 0.0), (352, 0.0), (353, 0.0), (354, 0.0), (355, 0.0), (356, 0.0), (357, 0.0), (358, 0.0), (359, 0.0), (360, 0.0), (361, 0.0), (362, 0.0), (363, 0.0), (364, 0.0), (365, 0.0), (366, 0.0), (367, 0.0), (368, 0.0), (369, 0.0), (370, 0.0), (371, 0.0), (372, 0.0), (373, 0.0), (374, 0.0), (375, 0.0), (376, 0.0), (378, 0.0), (379, 0.0), (380, 0.0), (381, 0.0), (382, 0.0), (383, 0.0)], 384, 0.6547730244722585, 0.6655874945418179, 0.6547730244722585, 0.6490427968060033, 0.698345264456409, 0.6197123519458545, 0.6276643978813815, 0.6197123519458545, 0.613696773099413, 0.6503456386118913, 0.5939675174013921, 0.6085561297785407, 0.5939675174013921, 0.5798518270944741, 0.6294189846092559]\n",
      "model_name: RandomForestClassifier\n",
      "class_name exist\n",
      "((19108, 385), (19108,))\n",
      "((5459, 385), (5459,))\n",
      "((2731, 385), (2731,))\n",
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 70.01%\n",
      "precision: 70.38% \n",
      "recall: 70.01% \n",
      "fscore: 69.87% \n",
      "AUC 77.1938%\n",
      "---VALID---\n",
      "Accuracy: 67.68%\n",
      "precision: 67.79% \n",
      "recall: 67.68% \n",
      "fscore: 67.63% \n",
      "AUC 73.2589%\n",
      "---TEST---\n",
      "Accuracy: 61.02%\n",
      "precision: 61.76% \n",
      "recall: 61.02% \n",
      "fscore: 60.40% \n",
      "AUC 66.2405%\n",
      "TN:  209 FP:  222  FN:  114  TP:  317\n",
      "classifier_data:  [2, '06/09/2022', '08:29:12', '--', 'RandomForestClassifier', 'binary', 'exist', [('exist', 5353), ('not_exist', 5353)], [('not_exist', 431), ('exist', 431)], [('exist', 1182), ('not_exist', 1182)], 20332, [(99, 0.060271628371854374), (125, 0.03487988620133782), (134, 0.028095794451012367), (249, 0.027301874160449543), (165, 0.025522498817318127), (48, 0.024834969086112567), (363, 0.02144974515924209), (377, 0.02144231371188873), (157, 0.019933633605834448), (11, 0.019686030277800068), (203, 0.018837403541793595), (177, 0.01875971896961792), (63, 0.01807202929860344), (135, 0.016042700962441772), (132, 0.01602962164479608), (78, 0.015693034481702787), (292, 0.015135604992016943), (192, 0.014461991844334283), (380, 0.013149301866047241), (375, 0.01240012240614277), (10, 0.012249534155406763), (37, 0.011053069091770711), (129, 0.01099743115962016), (127, 0.010981837293534098), (291, 0.010777362018042696), (144, 0.010641421881431306), (351, 0.01019400225874561), (256, 0.009892085036709759), (62, 0.0097370974594552), (126, 0.008629867088467262), (223, 0.0075736029091713186), (229, 0.006976050830251651), (251, 0.006751840042117225), (379, 0.0065140370201964585), (95, 0.006376365072216537), (98, 0.006357115242551131), (206, 0.0054268748630694), (267, 0.0051056296994590935), (2, 0.004823221293812035), (313, 0.004794803354465151), (24, 0.0047094289229455775), (228, 0.004680417260704326), (106, 0.00458708962934835), (274, 0.0045525409757102555), (239, 0.004478061929798982), (338, 0.004444718137834671), (91, 0.004362655697532776), (329, 0.0036577969534496674), (221, 0.003526857756784184), (266, 0.003461944359920606), (219, 0.0034544179096734095), (113, 0.0033635085361070917), (29, 0.0032214967547006435), (172, 0.003144682954279572), (100, 0.0029745358832118803), (262, 0.002974149048590752), (65, 0.0029373979937261903), (340, 0.0029046668464681414), (74, 0.002865172934471833), (250, 0.00278534058513448), (140, 0.0027294034799754154), (68, 0.0026703951406096605), (69, 0.002601478834612564), (294, 0.002600514962369526), (320, 0.0025167069375235836), (308, 0.002509784331645952), (336, 0.002448212997770231), (120, 0.00244692409173298), (358, 0.0024447456942092672), (27, 0.0024082587198005507), (246, 0.002337808774672357), (248, 0.0022940038017916927), (231, 0.0022834321464831457), (55, 0.0022788653792182325), (297, 0.002269322307608586), (6, 0.0022385271779434583), (368, 0.0022187840193908202), (19, 0.00220381449145088), (247, 0.0021948962647855568), (334, 0.002177597402115214), (241, 0.002173169794820323), (77, 0.002123586518242482), (179, 0.002121744219645411), (162, 0.0020492147895762804), (339, 0.00200627419806952), (161, 0.002000936722331667), (66, 0.0019794601870194856), (245, 0.0019755393801928417), (4, 0.0019707104031022756), (112, 0.0019681429887018578), (296, 0.0019184920962529567), (335, 0.0018945589722550594), (81, 0.0018375714448264182), (218, 0.0018303458999002584), (181, 0.0018241771242199839), (214, 0.001813622805182002), (369, 0.0018047602130710852), (230, 0.0017968359031278408), (237, 0.0017887325342618523), (31, 0.001782458298550018), (32, 0.0017601486428806563), (5, 0.0017184521763653304), (21, 0.0017146254074690997), (149, 0.0017002592884946819), (210, 0.0016797507296028847), (121, 0.0016728797021974629), (118, 0.0016719164240263297), (72, 0.0016548203374344955), (311, 0.0016423285166781722), (378, 0.0016364947196840381), (366, 0.0016208862008215967), (154, 0.0016135590567513353), (93, 0.001603762699438534), (258, 0.001598000327391608), (235, 0.001588005453321018), (355, 0.001584349977088309), (137, 0.0015798706061257467), (166, 0.0015677452653916478), (268, 0.0015557745638272598), (282, 0.001541028815188628), (88, 0.001538323135896882), (85, 0.0015176158508773926), (345, 0.0014993070743853437), (347, 0.0014990907968187256), (305, 0.001477525712053676), (1, 0.0014677000497622995), (138, 0.0014607511007225578), (105, 0.0014571819527417899), (381, 0.0014328443512559592), (56, 0.0014240681365148588), (12, 0.001418848120486359), (53, 0.001403130818560023), (330, 0.0014006176380126766), (178, 0.001380583458325555), (39, 0.0013786221302599852), (92, 0.0013765944574598299), (117, 0.0013642858047765308), (156, 0.001354879276970323), (67, 0.0013391339274708225), (155, 0.0013310510991300472), (265, 0.0013159760956367571), (111, 0.001313925895704377), (342, 0.0013097398355837774), (57, 0.0013086694123179887), (312, 0.0012904996197183826), (365, 0.0012887989153063505), (145, 0.00128079292479434), (61, 0.0012744262482925), (200, 0.0012744130741970752), (22, 0.001267617557453919), (350, 0.0012628737140150152), (201, 0.0012543963121563878), (382, 0.0012534094951261899), (360, 0.0012511348789786784), (186, 0.0012509830062439815), (139, 0.0012270903421785794), (75, 0.001218894236640137), (275, 0.0012188843490672089), (331, 0.0012143746894880624), (257, 0.0012137542745963453), (101, 0.0012095181346111808), (315, 0.0011956928313489247), (344, 0.0011812177019800945), (89, 0.0011775263600826114), (169, 0.0011645732514377954), (52, 0.0011605425528367996), (263, 0.0011604493651712698), (319, 0.001155479068104985), (353, 0.0011525512032974788), (273, 0.001144036507329978), (261, 0.0011344097998771032), (208, 0.0011303193856803282), (182, 0.0011296031177758542), (41, 0.0011237668985797044), (376, 0.0011206547022334155), (96, 0.0011186874111675806), (370, 0.0011144785869932694), (310, 0.0011128482822790437), (318, 0.0011075080094593926), (209, 0.0010967679503534244), (79, 0.0010937837932936646), (16, 0.001093202159935254), (35, 0.0010917950239962308), (152, 0.0010769165771675427), (281, 0.0010694863369311708), (90, 0.0010675417952903748), (51, 0.0010647978654441362), (83, 0.0010457784671907182), (114, 0.0010384329735725186), (173, 0.0010360403670997918), (128, 0.0010303133997001952), (195, 0.0010253546373631918), (302, 0.0010250597354578333), (64, 0.0010221870348487714), (43, 0.0010184226949448841), (343, 0.0010173350813962587), (383, 0.0010164183306088758), (123, 0.0010102313779826082), (349, 0.0009882925288801194), (303, 0.000986926670602325), (254, 0.0009832282376753265), (40, 0.0009795812075930027), (328, 0.000978901211178015), (94, 0.0009679044053548952), (3, 0.000956629147375873), (168, 0.0009495095686259411), (283, 0.0009390098295524574), (153, 0.0009327951016227892), (133, 0.0009276425465381092), (18, 0.0009250657988853283), (323, 0.0009184512322282676), (280, 0.0009032237152934011), (103, 0.0009030080153262491), (102, 0.0008946493668944401), (337, 0.0008749508369255697), (325, 0.0008690020654517981), (84, 0.0008611403705775655), (333, 0.0008607073872388964), (222, 0.0008551710775169396), (49, 0.000848359015803855), (373, 0.0008423109194075974), (59, 0.0008403628228941805), (50, 0.0008380416449718351), (142, 0.0008377677851782399), (151, 0.0008354381300928343), (104, 0.0008326522667846673), (227, 0.0008219463868240595), (45, 0.0008209835187401692), (298, 0.0008201417091723505), (70, 0.0008196215936464773), (191, 0.000818820903520336), (314, 0.0008159727375558866), (14, 0.0008116736028642836), (324, 0.0008113895192118042), (28, 0.0008088007076671548), (174, 0.0008080329632088188), (354, 0.0008050741723421092), (327, 0.0008050089361115448), (97, 0.0008041890485371855), (87, 0.0008014191094114381), (215, 0.0007971330715657358), (269, 0.0007908060459482695), (164, 0.0007878925036456598), (244, 0.0007731263640413269), (194, 0.0007701530082965693), (36, 0.0007677704217645463), (306, 0.0007654970773607467), (146, 0.0007553140797809691), (362, 0.0007546518243781059), (220, 0.0007513653476660739), (253, 0.0007409398629116584), (80, 0.0007387604617608242), (352, 0.0007305359296508301), (259, 0.0007275833084625287), (357, 0.0007265441673012244), (226, 0.00072245828608768), (211, 0.0007223550868596512), (159, 0.0007188922067420606), (148, 0.0007135998145029093), (301, 0.0007122108196255196), (9, 0.0007065598774880377), (326, 0.0006983785761552337), (307, 0.0006931880454178061), (136, 0.0006930326845060176), (204, 0.0006857289314344881), (361, 0.0006780478083383759), (364, 0.0006733111581173795), (372, 0.0006732181209381809), (290, 0.0006714127052562227), (176, 0.000669521277487068), (321, 0.0006624560535013578), (278, 0.0006594381613283796), (322, 0.0006521944557375391), (119, 0.0006512404997364459), (238, 0.0006379637026468508), (180, 0.0006354448756232282), (284, 0.0006332003412082436), (73, 0.0006266476226288583), (356, 0.0006236386538848698), (359, 0.0006222210136704667), (183, 0.0006193424552199345), (299, 0.0006176583467905417), (25, 0.0005982921165479535), (216, 0.0005949641086718582), (130, 0.0005925569754358075), (198, 0.0005898851846177591), (30, 0.0005836917184520768), (175, 0.0005817463702212705), (13, 0.0005787920613753141), (295, 0.0005503203170101443), (141, 0.0005484539190821279), (42, 0.0005438910212743688), (207, 0.0005391915317735642), (190, 0.0005384635780034326), (300, 0.0005375815502269068), (213, 0.0005358480323166869), (240, 0.0005258742912277381), (234, 0.0005241518068431621), (38, 0.0005184128660811041), (287, 0.0005164642050901253), (108, 0.0005131063873625407), (332, 0.0005059060854569223), (264, 0.0005035360536978837), (199, 0.0005023792204941253), (271, 0.000501449142140801), (309, 0.0005001840865806178), (82, 0.0004987494080974851), (17, 0.0004948225968738065), (193, 0.0004939239469956435), (115, 0.000493793358582125), (76, 0.0004903336130044954), (255, 0.0004869313086625232), (0, 0.00048418297263906824), (197, 0.0004825663039311642), (224, 0.00047971901774635927), (26, 0.00046677576736967535), (33, 0.0004658549280160315), (196, 0.00046515412638024607), (272, 0.0004614573303470501), (185, 0.00045230401265812883), (122, 0.00044957748493192366), (276, 0.0004482355408414379), (202, 0.0004410244539670237), (279, 0.0004373190088699635), (46, 0.00043251014091848277), (116, 0.000427461184783217), (286, 0.0004245228742485248), (260, 0.0004244583736305102), (171, 0.000421568673143068), (124, 0.0004197471733311806), (34, 0.0004191288487581006), (205, 0.0004177124383550349), (143, 0.00040481757078393907), (23, 0.00040248031866969516), (109, 0.00038551798975168826), (371, 0.0003850780749034097), (60, 0.0003817328038975004), (187, 0.00037499100796350873), (288, 0.0003727078867156593), (184, 0.0003552093484664113), (110, 0.00035223393545368714), (217, 0.0003357274821103981), (20, 0.00033125818484166516), (304, 0.00033019335839923363), (8, 0.00032486137743538055), (232, 0.00032481677368057573), (367, 0.000324123964492206), (54, 0.00032283537578558526), (242, 0.00031971944739120193), (374, 0.0003154692017835213), (316, 0.00030944962182955155), (236, 0.0003045453351997388), (189, 0.0002994797780167573), (58, 0.0002929128362909632), (285, 0.0002924740991884844), (293, 0.00029075575768257913), (212, 0.00028275734806265635), (348, 0.00027402442776334026), (233, 0.00027122079268388915), (346, 0.00026530613829433923), (160, 0.0002533610777347244), (163, 0.0002500228770562535), (7, 0.00024573296535476615), (270, 0.00023908259489544746), (341, 0.0002335936846734228), (225, 0.00022981548675466389), (147, 0.0002293944342223251), (15, 0.0002187738926661061), (277, 0.0002088792413709326), (167, 0.00019790571936432092), (131, 0.00019587549710001223), (289, 0.00019442881031873788), (44, 0.00018805750323059044), (188, 0.00017285040296179916), (107, 0.00014212882976254897), (252, 0.00013485911052795165), (47, 0.0001164700290004564), (170, 0.00011105248490133457), (71, 9.689554243884258e-05), (86, 9.44152611075235e-05), (150, 8.724710391874146e-05), (158, 6.144895855067755e-05), (317, 5.7157475370230784e-05), (243, 2.8598798086913674e-05)], 384, 0.7000747244535774, 0.7037714688104058, 0.7000747244535774, 0.6987082443916158, 0.7719378058866551, 0.676818950930626, 0.6778500587462852, 0.676818950930626, 0.6763498504407215, 0.7325892333107155, 0.6102088167053364, 0.6175924915420713, 0.6102088167053364, 0.603992452623807, 0.662404918147512]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/UkrainianConflict/models_combine/'\n",
      "model_name: XGBClassifier\n",
      "class_name exist\n",
      "((19108, 385), (19108,))\n",
      "((5459, 385), (5459,))\n",
      "((2731, 385), (2731,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "[08:29:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.65957\tvalidation_1-logloss:0.67541\n",
      "[1]\tvalidation_0-logloss:0.63580\tvalidation_1-logloss:0.66295\n",
      "[2]\tvalidation_0-logloss:0.61793\tvalidation_1-logloss:0.65377\n",
      "[3]\tvalidation_0-logloss:0.60109\tvalidation_1-logloss:0.65095\n",
      "[4]\tvalidation_0-logloss:0.58546\tvalidation_1-logloss:0.65045\n",
      "[5]\tvalidation_0-logloss:0.57476\tvalidation_1-logloss:0.64839\n",
      "[6]\tvalidation_0-logloss:0.56589\tvalidation_1-logloss:0.64782\n",
      "[7]\tvalidation_0-logloss:0.55396\tvalidation_1-logloss:0.64923\n",
      "[8]\tvalidation_0-logloss:0.54855\tvalidation_1-logloss:0.65087\n",
      "[9]\tvalidation_0-logloss:0.53717\tvalidation_1-logloss:0.64594\n",
      "[10]\tvalidation_0-logloss:0.52752\tvalidation_1-logloss:0.64585\n",
      "[11]\tvalidation_0-logloss:0.51830\tvalidation_1-logloss:0.64594\n",
      "[12]\tvalidation_0-logloss:0.50972\tvalidation_1-logloss:0.64228\n",
      "[13]\tvalidation_0-logloss:0.50274\tvalidation_1-logloss:0.64241\n",
      "[14]\tvalidation_0-logloss:0.49557\tvalidation_1-logloss:0.64502\n",
      "[15]\tvalidation_0-logloss:0.48692\tvalidation_1-logloss:0.64399\n",
      "[16]\tvalidation_0-logloss:0.47911\tvalidation_1-logloss:0.64725\n",
      "[17]\tvalidation_0-logloss:0.47553\tvalidation_1-logloss:0.64466\n",
      "[18]\tvalidation_0-logloss:0.46932\tvalidation_1-logloss:0.64667\n",
      "[19]\tvalidation_0-logloss:0.46462\tvalidation_1-logloss:0.64658\n",
      "[20]\tvalidation_0-logloss:0.46044\tvalidation_1-logloss:0.64578\n",
      "[21]\tvalidation_0-logloss:0.45308\tvalidation_1-logloss:0.64782\n",
      "[22]\tvalidation_0-logloss:0.44983\tvalidation_1-logloss:0.64791\n",
      "---TRAIN---\n",
      "Accuracy: 77.78%\n",
      "precision: 77.97% \n",
      "recall: 77.78% \n",
      "fscore: 77.74% \n",
      "AUC 85.9033%\n",
      "---VALID---\n",
      "Accuracy: 65.74%\n",
      "precision: 65.75% \n",
      "recall: 65.74% \n",
      "fscore: 65.73% \n",
      "AUC 71.9016%\n",
      "---TEST---\n",
      "Accuracy: 61.02%\n",
      "precision: 61.65% \n",
      "recall: 61.02% \n",
      "fscore: 60.49% \n",
      "AUC 67.0657%\n",
      "TN:  213 FP:  218  FN:  118  TP:  313\n",
      "classifier_data:  [3, '06/09/2022', '08:29:19', '--', 'XGBClassifier', 'binary', 'exist', [('exist', 5353), ('not_exist', 5353)], [('not_exist', 431), ('exist', 431)], [('exist', 1182), ('not_exist', 1182)], 20332, [(165, 0.022706367), (99, 0.020438101), (377, 0.0135577135), (125, 0.011906614), (134, 0.011849304), (249, 0.011415207), (228, 0.009333497), (157, 0.008539669), (11, 0.008510941), (291, 0.008164939), (209, 0.008080066), (275, 0.007980129), (241, 0.007929323), (135, 0.007254786), (233, 0.007042727), (72, 0.0069906283), (48, 0.0069496166), (375, 0.0069419392), (177, 0.0068643424), (227, 0.0066770245), (292, 0.006504915), (63, 0.006384812), (78, 0.006035107), (247, 0.005951305), (172, 0.005735615), (308, 0.0056239655), (293, 0.0054925983), (380, 0.005463534), (129, 0.0054503614), (268, 0.0054296968), (231, 0.0053394926), (335, 0.0051780986), (133, 0.0051597273), (68, 0.005078175), (313, 0.005009208), (103, 0.004954104), (144, 0.0048606666), (127, 0.004813679), (239, 0.004789115), (154, 0.004776736), (381, 0.004751082), (102, 0.0047410624), (120, 0.0047116894), (340, 0.0047000186), (98, 0.004677132), (218, 0.0046767937), (139, 0.0046698162), (147, 0.004624462), (363, 0.004614313), (299, 0.0046051377), (263, 0.0045624236), (13, 0.0045550363), (41, 0.004553661), (112, 0.0045410474), (245, 0.004540984), (320, 0.004439717), (60, 0.0043938025), (251, 0.004316804), (79, 0.0042572347), (55, 0.004233883), (243, 0.0042103804), (93, 0.004165362), (357, 0.0041300943), (306, 0.004129034), (178, 0.004119145), (382, 0.004112053), (45, 0.004105425), (43, 0.0040805917), (319, 0.004068604), (364, 0.0040483945), (383, 0.004027048), (192, 0.004008384), (145, 0.004004293), (132, 0.0039661527), (168, 0.0039578164), (4, 0.003947999), (250, 0.0039420915), (305, 0.0039293645), (31, 0.0039253384), (161, 0.0039216764), (196, 0.003875288), (372, 0.003871191), (96, 0.0038706483), (149, 0.003827835), (174, 0.0037613024), (294, 0.0037575942), (2, 0.0037452942), (347, 0.0037415705), (28, 0.003740053), (27, 0.0037399642), (368, 0.0037356243), (84, 0.0037342792), (369, 0.0037023814), (137, 0.0036921944), (329, 0.0036622565), (113, 0.0036500182), (234, 0.0036458808), (37, 0.0036242288), (15, 0.003612564), (130, 0.0036029704), (101, 0.0035716672), (56, 0.0035533619), (91, 0.0035412784), (324, 0.003518208), (256, 0.0035181572), (267, 0.003505181), (169, 0.0034933928), (65, 0.0034807124), (376, 0.003480184), (266, 0.0034579667), (183, 0.003456272), (156, 0.003447675), (225, 0.0034271874), (265, 0.0033944547), (24, 0.0033796546), (242, 0.0033650643), (281, 0.0033465945), (44, 0.0033426885), (141, 0.003324955), (19, 0.0033185196), (353, 0.0033162679), (206, 0.0033135558), (217, 0.0032983897), (219, 0.0032982859), (295, 0.0032962847), (246, 0.0032836087), (42, 0.003282803), (289, 0.0032761137), (12, 0.0032648672), (373, 0.0032533903), (20, 0.0032483032), (176, 0.003238352), (253, 0.0032212911), (171, 0.0032132082), (258, 0.0032096116), (213, 0.003208361), (80, 0.0032057236), (325, 0.0031790694), (124, 0.003155529), (216, 0.0031168621), (64, 0.0030716662), (166, 0.003068184), (323, 0.003057796), (316, 0.003055103), (187, 0.003026315), (69, 0.0030140344), (107, 0.0030077035), (66, 0.0030021633), (366, 0.0029851834), (164, 0.0029818462), (5, 0.0029752902), (189, 0.0029604798), (282, 0.0029510618), (379, 0.002917506), (198, 0.0029015383), (82, 0.0028926972), (314, 0.0028681427), (92, 0.0028633496), (160, 0.0028607605), (9, 0.002857952), (334, 0.0028434272), (310, 0.0028421893), (288, 0.0028364712), (238, 0.0028127062), (32, 0.0028011824), (153, 0.002791652), (87, 0.0027824573), (114, 0.0027641475), (104, 0.0027442775), (277, 0.002726814), (111, 0.0026890514), (81, 0.0026881509), (274, 0.0026667248), (74, 0.002661027), (237, 0.002642895), (110, 0.002635607), (49, 0.002624994), (328, 0.002606023), (360, 0.0026042818), (105, 0.002589655), (94, 0.0025857186), (36, 0.0025717744), (270, 0.0025666174), (378, 0.0025373881), (374, 0.0025357772), (229, 0.002525115), (67, 0.0025156445), (33, 0.002504923), (188, 0.002503568), (54, 0.0024911975), (365, 0.0024836727), (179, 0.0024762992), (336, 0.002471081), (356, 0.0024604404), (286, 0.002452541), (358, 0.002422159), (22, 0.0024190194), (326, 0.002409921), (73, 0.0024041699), (260, 0.0024039452), (136, 0.0023957558), (371, 0.0023891442), (257, 0.0023672266), (230, 0.002363926), (108, 0.002353101), (122, 0.0023505634), (205, 0.0023469937), (121, 0.0023444465), (51, 0.002337231), (355, 0.0023347735), (83, 0.0023330387), (50, 0.002318598), (152, 0.0023152623), (254, 0.002314067), (315, 0.0023054853), (57, 0.0023042827), (62, 0.00229099), (61, 0.0022816693), (35, 0.0022790174), (163, 0.0022717111), (59, 0.0022712876), (330, 0.0022699735), (6, 0.0022654769), (88, 0.0022643986), (46, 0.0022630014), (58, 0.002257498), (151, 0.0022488076), (123, 0.002244032), (345, 0.0022324673), (38, 0.0022290137), (220, 0.0022239678), (327, 0.0022088953), (342, 0.0022004459), (29, 0.0021823575), (221, 0.002180795), (39, 0.002180043), (255, 0.0021760669), (193, 0.0021711364), (100, 0.0021608158), (296, 0.0021605573), (143, 0.0021472625), (116, 0.0021225582), (53, 0.0021128478), (322, 0.0021004619), (106, 0.002057929), (211, 0.0020481932), (138, 0.0020398502), (77, 0.0020000471), (167, 0.001984024), (148, 0.0019594373), (343, 0.0019228061), (362, 0.0017575513), (333, 0.0017540255), (349, 0.0016611734), (351, 0.0016601454), (318, 0.0016538163), (283, 0.0015986614), (117, 0.0015656272), (309, 0.0015005708), (30, 0.0014557174), (212, 0.001363039), (184, 0.0013624026), (40, 0.0013409358), (158, 0.0013408078), (207, 0.0013128564), (21, 0.0012933334), (304, 0.0012859452), (332, 0.0012468506), (200, 0.0012135437), (70, 0.0011959557), (89, 0.0011952957), (226, 0.0011784419), (331, 0.0011638706), (3, 0.0011525457), (190, 0.000931714), (195, 0.0008641401), (303, 0.0006746348), (203, 0.00046510922), (214, 0.00038867572), (0, 0.0), (1, 0.0), (7, 0.0), (8, 0.0), (10, 0.0), (14, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (23, 0.0), (25, 0.0), (26, 0.0), (34, 0.0), (47, 0.0), (52, 0.0), (71, 0.0), (75, 0.0), (76, 0.0), (85, 0.0), (86, 0.0), (90, 0.0), (95, 0.0), (97, 0.0), (109, 0.0), (115, 0.0), (118, 0.0), (119, 0.0), (126, 0.0), (128, 0.0), (131, 0.0), (140, 0.0), (142, 0.0), (146, 0.0), (150, 0.0), (155, 0.0), (159, 0.0), (162, 0.0), (170, 0.0), (173, 0.0), (175, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (185, 0.0), (186, 0.0), (191, 0.0), (194, 0.0), (197, 0.0), (199, 0.0), (201, 0.0), (202, 0.0), (204, 0.0), (208, 0.0), (210, 0.0), (215, 0.0), (222, 0.0), (223, 0.0), (224, 0.0), (232, 0.0), (235, 0.0), (236, 0.0), (240, 0.0), (244, 0.0), (248, 0.0), (252, 0.0), (259, 0.0), (261, 0.0), (262, 0.0), (264, 0.0), (269, 0.0), (271, 0.0), (272, 0.0), (273, 0.0), (276, 0.0), (278, 0.0), (279, 0.0), (280, 0.0), (284, 0.0), (285, 0.0), (287, 0.0), (290, 0.0), (297, 0.0), (298, 0.0), (300, 0.0), (301, 0.0), (302, 0.0), (307, 0.0), (311, 0.0), (312, 0.0), (317, 0.0), (321, 0.0), (337, 0.0), (338, 0.0), (339, 0.0), (341, 0.0), (344, 0.0), (346, 0.0), (348, 0.0), (350, 0.0), (352, 0.0), (354, 0.0), (359, 0.0), (361, 0.0), (367, 0.0), (370, 0.0)], 384, 0.777788156174108, 0.779721241312731, 0.777788156174108, 0.7774035788072812, 0.8590334629936844, 0.6573604060913706, 0.6575065122636887, 0.6573604060913706, 0.6572809278350515, 0.7190163507319322, 0.6102088167053364, 0.6164791961811779, 0.6102088167053364, 0.6048913844189434, 0.6706574577010245]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/UkrainianConflict/models_combine/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------robinhood-----------------------\n",
      "model_name: DecisionTreeClassifier\n",
      "class_name exist\n",
      "((1696, 385), (1696,))\n",
      "((484, 385), (484,))\n",
      "((243, 385), (243,))\n",
      "is_splited None\n",
      "here\n",
      "dec_tree_params {'criterion': 'gini', 'max_depth': 5}\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 93.37%\n",
      "precision: 93.89% \n",
      "recall: 93.37% \n",
      "fscore: 93.35% \n",
      "AUC 95.6089%\n",
      "---VALID---\n",
      "Accuracy: 61.61%\n",
      "precision: 61.70% \n",
      "recall: 61.61% \n",
      "fscore: 61.53% \n",
      "AUC 64.2060%\n",
      "---TEST---\n",
      "Accuracy: 51.56%\n",
      "precision: 51.70% \n",
      "recall: 51.56% \n",
      "fscore: 50.59% \n",
      "AUC 56.6406%\n",
      "TN:  21 FP:  11  FN:  20  TP:  12\n",
      "classifier_data:  [1, '06/09/2022', '08:29:38', 5, 'DecisionTreeClassifier', 'binary', 'exist', [('exist', 166), ('not_exist', 166)], [('exist', 32), ('not_exist', 32)], [('not_exist', 56), ('exist', 56)], 254, [(250, 0.30467145739434587), (73, 0.0754071113580535), (239, 0.06971618951875501), (204, 0.06266896980082814), (124, 0.05564644359445078), (78, 0.053268107365376204), (326, 0.05208156423721328), (381, 0.047415261529032166), (309, 0.04703130056278103), (364, 0.04694772157934768), (379, 0.0460229280486988), (100, 0.03734773419016855), (290, 0.030421609971820387), (80, 0.029130996215440116), (183, 0.014305399927225055), (179, 0.014045301746730058), (14, 0.013871902959733387), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.0), (55, 0.0), (56, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (60, 0.0), (61, 0.0), (62, 0.0), (63, 0.0), (64, 0.0), (65, 0.0), (66, 0.0), (67, 0.0), (68, 0.0), (69, 0.0), (70, 0.0), (71, 0.0), (72, 0.0), (74, 0.0), (75, 0.0), (76, 0.0), (77, 0.0), (79, 0.0), (81, 0.0), (82, 0.0), (83, 0.0), (84, 0.0), (85, 0.0), (86, 0.0), (87, 0.0), (88, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.0), (94, 0.0), (95, 0.0), (96, 0.0), (97, 0.0), (98, 0.0), (99, 0.0), (101, 0.0), (102, 0.0), (103, 0.0), (104, 0.0), (105, 0.0), (106, 0.0), (107, 0.0), (108, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (112, 0.0), (113, 0.0), (114, 0.0), (115, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.0), (122, 0.0), (123, 0.0), (125, 0.0), (126, 0.0), (127, 0.0), (128, 0.0), (129, 0.0), (130, 0.0), (131, 0.0), (132, 0.0), (133, 0.0), (134, 0.0), (135, 0.0), (136, 0.0), (137, 0.0), (138, 0.0), (139, 0.0), (140, 0.0), (141, 0.0), (142, 0.0), (143, 0.0), (144, 0.0), (145, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (149, 0.0), (150, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (155, 0.0), (156, 0.0), (157, 0.0), (158, 0.0), (159, 0.0), (160, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (166, 0.0), (167, 0.0), (168, 0.0), (169, 0.0), (170, 0.0), (171, 0.0), (172, 0.0), (173, 0.0), (174, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (178, 0.0), (180, 0.0), (181, 0.0), (182, 0.0), (184, 0.0), (185, 0.0), (186, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (191, 0.0), (192, 0.0), (193, 0.0), (194, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (198, 0.0), (199, 0.0), (200, 0.0), (201, 0.0), (202, 0.0), (203, 0.0), (205, 0.0), (206, 0.0), (207, 0.0), (208, 0.0), (209, 0.0), (210, 0.0), (211, 0.0), (212, 0.0), (213, 0.0), (214, 0.0), (215, 0.0), (216, 0.0), (217, 0.0), (218, 0.0), (219, 0.0), (220, 0.0), (221, 0.0), (222, 0.0), (223, 0.0), (224, 0.0), (225, 0.0), (226, 0.0), (227, 0.0), (228, 0.0), (229, 0.0), (230, 0.0), (231, 0.0), (232, 0.0), (233, 0.0), (234, 0.0), (235, 0.0), (236, 0.0), (237, 0.0), (238, 0.0), (240, 0.0), (241, 0.0), (242, 0.0), (243, 0.0), (244, 0.0), (245, 0.0), (246, 0.0), (247, 0.0), (248, 0.0), (249, 0.0), (251, 0.0), (252, 0.0), (253, 0.0), (254, 0.0), (255, 0.0), (256, 0.0), (257, 0.0), (258, 0.0), (259, 0.0), (260, 0.0), (261, 0.0), (262, 0.0), (263, 0.0), (264, 0.0), (265, 0.0), (266, 0.0), (267, 0.0), (268, 0.0), (269, 0.0), (270, 0.0), (271, 0.0), (272, 0.0), (273, 0.0), (274, 0.0), (275, 0.0), (276, 0.0), (277, 0.0), (278, 0.0), (279, 0.0), (280, 0.0), (281, 0.0), (282, 0.0), (283, 0.0), (284, 0.0), (285, 0.0), (286, 0.0), (287, 0.0), (288, 0.0), (289, 0.0), (291, 0.0), (292, 0.0), (293, 0.0), (294, 0.0), (295, 0.0), (296, 0.0), (297, 0.0), (298, 0.0), (299, 0.0), (300, 0.0), (301, 0.0), (302, 0.0), (303, 0.0), (304, 0.0), (305, 0.0), (306, 0.0), (307, 0.0), (308, 0.0), (310, 0.0), (311, 0.0), (312, 0.0), (313, 0.0), (314, 0.0), (315, 0.0), (316, 0.0), (317, 0.0), (318, 0.0), (319, 0.0), (320, 0.0), (321, 0.0), (322, 0.0), (323, 0.0), (324, 0.0), (325, 0.0), (327, 0.0), (328, 0.0), (329, 0.0), (330, 0.0), (331, 0.0), (332, 0.0), (333, 0.0), (334, 0.0), (335, 0.0), (336, 0.0), (337, 0.0), (338, 0.0), (339, 0.0), (340, 0.0), (341, 0.0), (342, 0.0), (343, 0.0), (344, 0.0), (345, 0.0), (346, 0.0), (347, 0.0), (348, 0.0), (349, 0.0), (350, 0.0), (351, 0.0), (352, 0.0), (353, 0.0), (354, 0.0), (355, 0.0), (356, 0.0), (357, 0.0), (358, 0.0), (359, 0.0), (360, 0.0), (361, 0.0), (362, 0.0), (363, 0.0), (365, 0.0), (366, 0.0), (367, 0.0), (368, 0.0), (369, 0.0), (370, 0.0), (371, 0.0), (372, 0.0), (373, 0.0), (374, 0.0), (375, 0.0), (376, 0.0), (377, 0.0), (378, 0.0), (380, 0.0), (382, 0.0), (383, 0.0)], 384, 0.9337349397590361, 0.9388954171562868, 0.9337349397590361, 0.9335395814376706, 0.9560894179126143, 0.6160714285714286, 0.6170041787206687, 0.6160714285714286, 0.6153047368000639, 0.6420599489795918, 0.515625, 0.5169671261930011, 0.515625, 0.5058530510585305, 0.56640625]\n",
      "model_name: RandomForestClassifier\n",
      "class_name exist\n",
      "((1696, 385), (1696,))\n",
      "((484, 385), (484,))\n",
      "((243, 385), (243,))\n",
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "---TRAIN---\n",
      "Accuracy: 98.19%\n",
      "precision: 98.26% \n",
      "recall: 98.19% \n",
      "fscore: 98.19% \n",
      "AUC 100.0000%\n",
      "---VALID---\n",
      "Accuracy: 70.54%\n",
      "precision: 75.65% \n",
      "recall: 70.54% \n",
      "fscore: 68.99% \n",
      "AUC 73.5332%\n",
      "---TEST---\n",
      "Accuracy: 73.44%\n",
      "precision: 76.58% \n",
      "recall: 73.44% \n",
      "fscore: 72.63% \n",
      "AUC 82.7148%\n",
      "TN:  29 FP:  3  FN:  14  TP:  18\n",
      "classifier_data:  [2, '06/09/2022', '08:29:38', '--', 'RandomForestClassifier', 'binary', 'exist', [('exist', 166), ('not_exist', 166)], [('exist', 32), ('not_exist', 32)], [('not_exist', 56), ('exist', 56)], 254, [(37, 0.026143012134438565), (319, 0.016499715045438533), (100, 0.015083641562203797), (250, 0.013999357824708663), (102, 0.013105603145880893), (67, 0.012877919459372158), (133, 0.01283886546797623), (49, 0.010902836425343478), (255, 0.010863089715048492), (352, 0.010656617007117125), (181, 0.010375996286210968), (22, 0.009932592372866951), (150, 0.009591701552003153), (301, 0.009388832880117262), (305, 0.008786256843646718), (30, 0.00852396677188723), (295, 0.008434468134708599), (239, 0.007268524217089403), (116, 0.007075102770660187), (326, 0.006655538339000055), (50, 0.00665523051117741), (221, 0.006537281059082094), (173, 0.006380889317565093), (174, 0.006378656367513569), (364, 0.006332660964148012), (287, 0.006297045462927268), (332, 0.0062890620502478855), (297, 0.006233323803828575), (315, 0.006179063746935054), (54, 0.006028850716542776), (327, 0.006027630157128819), (210, 0.005729726538428813), (156, 0.0056244535144665386), (93, 0.005466294645976731), (57, 0.00544849502673132), (178, 0.005306908874121089), (112, 0.005213189604291884), (32, 0.005163823657795021), (176, 0.005079490333761598), (268, 0.0049924029677845), (95, 0.00496880015790674), (351, 0.004967417830377864), (152, 0.00486580481769985), (96, 0.004862782882556245), (370, 0.004688871656541718), (276, 0.004636963118162669), (343, 0.004610927649505869), (311, 0.004508251079491461), (230, 0.004394062218412887), (109, 0.004392729448152671), (66, 0.004383717053611138), (7, 0.004275839611903231), (342, 0.004274389716115812), (366, 0.00426673413798663), (124, 0.00418183335961629), (304, 0.004171912449997378), (303, 0.004122221191619433), (197, 0.004041469665966867), (118, 0.00401525589080601), (273, 0.003957718322492378), (320, 0.003805718505592351), (73, 0.003789187923897436), (78, 0.0037075965516600866), (29, 0.0036826596580198114), (15, 0.0036630688481651017), (165, 0.003560322494752077), (203, 0.003540140129194731), (271, 0.003483457512171495), (356, 0.003475297199301362), (20, 0.0034662543811710134), (146, 0.003459730698319322), (183, 0.0033947056943530746), (302, 0.0033826695685641294), (316, 0.0033508149387381607), (263, 0.003350530753440768), (285, 0.0033281742757702345), (260, 0.0033165542380206537), (289, 0.0032672823917600656), (333, 0.003241199894691462), (74, 0.003225068597938752), (44, 0.003219271118617792), (85, 0.0031866369539684425), (379, 0.0031818700929329806), (63, 0.00317849203752928), (99, 0.0031777832627621484), (243, 0.003176604531915874), (10, 0.003165251745738995), (90, 0.003161572062896896), (368, 0.003143432822685679), (382, 0.0031359762264950475), (202, 0.0031088173598535053), (114, 0.003106643429438385), (8, 0.00309567177183335), (283, 0.003093157939310591), (117, 0.0030887022178799173), (128, 0.0030763147403277203), (216, 0.0030700385697414695), (31, 0.003066474462058249), (292, 0.0030565120885577674), (104, 0.003032435370599977), (207, 0.003027746960945924), (38, 0.0030273797150921667), (229, 0.003022867616270099), (272, 0.003019887313438175), (214, 0.0029731139019614373), (35, 0.0029558264173069814), (300, 0.0029512613871034428), (0, 0.00293965760997683), (270, 0.0029284909114632757), (86, 0.002922464822619151), (171, 0.0029141401026454078), (264, 0.002903242258902749), (374, 0.002873172136697298), (211, 0.002864580825738192), (242, 0.00286118265734564), (13, 0.0028594412385606837), (275, 0.002817893386827795), (235, 0.0027984438398065002), (314, 0.0027965536606210564), (313, 0.0027721053384450484), (193, 0.002745999327462185), (41, 0.002741683629174702), (45, 0.002724387578662528), (16, 0.002720734108425324), (158, 0.002683820834231726), (195, 0.0026729627056487374), (310, 0.002660689578898722), (253, 0.00264111273803143), (52, 0.0026184118169017907), (157, 0.0025717493749626424), (324, 0.0025624167460530723), (43, 0.0025493552921472375), (280, 0.0025366661786043014), (334, 0.002529028041007011), (371, 0.0024914595352714757), (75, 0.0024903649912467953), (279, 0.0024873481700261528), (84, 0.0024870300943348445), (358, 0.0024827559407683826), (258, 0.0024817675330071603), (262, 0.002474858872293865), (162, 0.00247293040519434), (338, 0.0024728758550394974), (108, 0.002465943463510576), (24, 0.0024423816017962886), (328, 0.0024421852080108173), (318, 0.0024301388283339605), (201, 0.0024148646542940977), (92, 0.0024038200272222645), (281, 0.0023915790655564973), (172, 0.0023887204440054114), (291, 0.0023884974586388923), (329, 0.0023581208971622347), (306, 0.0023549883805231773), (88, 0.0023533197032980136), (131, 0.002348914613582115), (309, 0.002338946223923798), (87, 0.002324803396219084), (383, 0.0023105517996222912), (347, 0.0023067983263883912), (42, 0.0023039678252067338), (187, 0.0023017853229093293), (56, 0.002301458285737394), (308, 0.0022919431004527866), (345, 0.0022815681776626705), (357, 0.002277966410693688), (53, 0.002251812410822118), (335, 0.002245295636660871), (180, 0.0022349161417987754), (381, 0.0022326878248475302), (106, 0.0022298609984503647), (89, 0.00222182741268033), (129, 0.0021986731840769823), (164, 0.002197540182116384), (220, 0.0021797316674737645), (55, 0.00216631007117234), (144, 0.002154301171997767), (307, 0.0021373826654797175), (248, 0.0021343196800538964), (65, 0.0021248436310043733), (373, 0.002118327846475101), (3, 0.002111485264157604), (363, 0.0021021642933894405), (184, 0.002100951729996386), (80, 0.0020580777058393086), (169, 0.002054792499918533), (236, 0.002048958814776299), (288, 0.002025554148339315), (130, 0.002007161841037037), (122, 0.001984400289067475), (48, 0.0019775685424798716), (17, 0.0019752123804384807), (212, 0.0019601928617331856), (336, 0.001948668645818708), (346, 0.0019435640163440369), (225, 0.0019285185647878705), (140, 0.001894118286191315), (240, 0.0018866855320591148), (167, 0.0018807275467400617), (380, 0.0018743023969890553), (182, 0.0018735336981246737), (139, 0.0018628884214310372), (269, 0.001860953431014969), (350, 0.0018546607887697324), (376, 0.001844444296811193), (127, 0.001843460360535325), (282, 0.0018344241761991647), (244, 0.0018329941642990777), (126, 0.0018193386584749054), (147, 0.0018175513574109083), (274, 0.0018174318147287682), (123, 0.0018117754962034508), (362, 0.0018063295429570736), (98, 0.001796175503941933), (19, 0.0017925263409667741), (47, 0.0017920150757774706), (103, 0.0017826625865202137), (266, 0.0017795335644600935), (340, 0.0017634350899082068), (290, 0.0017628286829533427), (18, 0.0017614825256712457), (355, 0.0017611033830976707), (21, 0.0017513616520451014), (138, 0.0017505509877984735), (331, 0.0017488037939912195), (218, 0.0017477371751771565), (134, 0.001746527468954019), (217, 0.001738566586579628), (245, 0.0017136403067483552), (196, 0.0017003484048835973), (186, 0.001693810707206079), (61, 0.0016839744287536398), (153, 0.0016820568369180695), (72, 0.0016625397032225652), (317, 0.001652660318918078), (367, 0.0016335626535072026), (224, 0.0016285144768657684), (194, 0.0016247781804188588), (359, 0.0016168607554992273), (348, 0.0016039895458198805), (233, 0.0015967164200942543), (208, 0.0015920303436538989), (215, 0.0015910314971601968), (14, 0.0015843809986426933), (143, 0.0015754199159075972), (34, 0.0015628357388853737), (69, 0.001548616800441855), (159, 0.0015447746868423602), (60, 0.0015255750523171224), (110, 0.001504140368641916), (136, 0.0014782567252703022), (76, 0.0014580994496278664), (234, 0.0014578502402978772), (148, 0.0014533974740563948), (228, 0.0014466332332555099), (145, 0.0014436107172664222), (6, 0.0014362606696738363), (259, 0.0014259974059823946), (26, 0.0014182079897134975), (33, 0.0014124950270444627), (77, 0.001406419284469324), (278, 0.001401608797572338), (141, 0.0013903353816310709), (105, 0.0013625991296805707), (36, 0.0013322005380719884), (5, 0.0013274845353649875), (97, 0.0013163404745156787), (377, 0.0012998100020643988), (137, 0.00128528585144248), (51, 0.001282260790929609), (294, 0.0012743140327290076), (125, 0.0012545988345891502), (237, 0.0012499551938930566), (46, 0.0012379710819745956), (25, 0.001220327495565956), (312, 0.0012146427382904478), (231, 0.001214051909001034), (204, 0.0011956648589707382), (361, 0.0011646682271500243), (142, 0.0011643870473533019), (81, 0.0011630640370571166), (191, 0.001135531709984457), (369, 0.0011353922083700214), (119, 0.0011300343606381177), (232, 0.001129087792441635), (59, 0.001123681215656256), (323, 0.0011104453085613863), (39, 0.0011093593816040182), (267, 0.0010936346437133157), (251, 0.001082772282809005), (2, 0.001073817387370596), (82, 0.0010491858421595602), (252, 0.0010480132946417244), (344, 0.0010464457506271366), (223, 0.0010212612452621176), (206, 0.0010120827435174385), (170, 0.0010058981709494192), (154, 0.0009947045572652987), (277, 0.0009774050643226383), (286, 0.0009678774885907391), (91, 0.0009656064117662221), (325, 0.0009602893714375989), (222, 0.0009491701123311784), (322, 0.0009434650661552354), (175, 0.0009225194905946048), (70, 0.0009096699741241234), (337, 0.00089945535642461), (94, 0.000892134007082617), (199, 0.000884720384671287), (83, 0.0008807975629568949), (205, 0.0008497750108802254), (299, 0.0008346685754862163), (135, 0.0008322019872467805), (284, 0.0008291069576518212), (190, 0.0008285466062929172), (163, 0.0008155356940171221), (188, 0.0008092947555946495), (9, 0.0007960701283708778), (185, 0.0007681087378942299), (189, 0.0007620797953499961), (149, 0.0007539397978547655), (155, 0.0007409998358451074), (349, 0.0007406903299400015), (219, 0.0007240370653725246), (79, 0.0007238608735967038), (256, 0.0006961124317062943), (101, 0.0006844832540184081), (4, 0.0006810044432476529), (209, 0.0006744941845711126), (341, 0.0006615557511605941), (257, 0.0006611924684559323), (40, 0.0006601884534222125), (296, 0.0006549233406986618), (330, 0.0006464877846957387), (115, 0.0006449279329057766), (241, 0.000607163439363633), (254, 0.0005971322794859856), (293, 0.0005956077740174523), (121, 0.000573025580944611), (192, 0.000567961101354688), (64, 0.000562312162929696), (68, 0.0005557395651269274), (23, 0.0005552893258388211), (339, 0.000554111418871928), (168, 0.0005434365502324496), (177, 0.0004932432812744226), (71, 0.0004789200882857595), (12, 0.00046767227845930984), (166, 0.0004659562768281053), (246, 0.0004522558972477554), (27, 0.00043404919573942406), (179, 0.0004223729973851199), (226, 0.0004170968456042651), (111, 0.00041365678240328613), (1, 0.00040420624429581616), (249, 0.0004024312855425162), (132, 0.0003867421933785022), (161, 0.00037980809122095774), (213, 0.0003642032890278158), (265, 0.00035115234740490245), (247, 0.0003278631787789607), (160, 0.0003108198923366142), (120, 0.00030099507856483137), (198, 0.0002996545895883743), (227, 0.00026710831750181575), (200, 0.00025927970614907786), (360, 0.00023886522600444584), (62, 0.00022910947068401582), (372, 0.0002075618441910226), (151, 0.00015176537073538675), (365, 0.0001501221905655151), (353, 0.00014686394086722244), (321, 0.00014172374633807377), (58, 0.0001320371504699068), (261, 0.00013076974910032377), (28, 0.00013059618310628024), (107, 0.0001275908831938416), (11, 9.485572171020204e-05), (113, 0.0), (238, 0.0), (298, 0.0), (354, 0.0), (375, 0.0), (378, 0.0)], 384, 0.9819277108433735, 0.9825581395348838, 0.9819277108433735, 0.9819218063672996, 1.0, 0.7053571428571429, 0.7564715252887295, 0.7053571428571429, 0.6899068713818273, 0.7353316326530612, 0.734375, 0.7657807308970099, 0.734375, 0.7262893081761006, 0.8271484375]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/robinhood/models_combine/'\n",
      "model_name: XGBClassifier\n",
      "class_name exist\n",
      "((1696, 385), (1696,))\n",
      "((484, 385), (484,))\n",
      "((243, 385), (243,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_splited None\n",
      "---------------------- exist -------------------------\n",
      "[08:29:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.52863\tvalidation_1-logloss:0.68895\n",
      "[1]\tvalidation_0-logloss:0.42136\tvalidation_1-logloss:0.70165\n",
      "[2]\tvalidation_0-logloss:0.35024\tvalidation_1-logloss:0.66015\n",
      "[3]\tvalidation_0-logloss:0.28661\tvalidation_1-logloss:0.63689\n",
      "[4]\tvalidation_0-logloss:0.23309\tvalidation_1-logloss:0.59840\n",
      "[5]\tvalidation_0-logloss:0.19310\tvalidation_1-logloss:0.59820\n",
      "[6]\tvalidation_0-logloss:0.16049\tvalidation_1-logloss:0.57195\n",
      "[7]\tvalidation_0-logloss:0.14018\tvalidation_1-logloss:0.55800\n",
      "[8]\tvalidation_0-logloss:0.12127\tvalidation_1-logloss:0.55722\n",
      "[9]\tvalidation_0-logloss:0.10731\tvalidation_1-logloss:0.56663\n",
      "[10]\tvalidation_0-logloss:0.09692\tvalidation_1-logloss:0.58176\n",
      "[11]\tvalidation_0-logloss:0.08541\tvalidation_1-logloss:0.56831\n",
      "[12]\tvalidation_0-logloss:0.07661\tvalidation_1-logloss:0.57576\n",
      "[13]\tvalidation_0-logloss:0.06897\tvalidation_1-logloss:0.56767\n",
      "[14]\tvalidation_0-logloss:0.06224\tvalidation_1-logloss:0.56817\n",
      "[15]\tvalidation_0-logloss:0.05669\tvalidation_1-logloss:0.56363\n",
      "[16]\tvalidation_0-logloss:0.05163\tvalidation_1-logloss:0.57336\n",
      "[17]\tvalidation_0-logloss:0.04786\tvalidation_1-logloss:0.57491\n",
      "---TRAIN---\n",
      "Accuracy: 100.00%\n",
      "precision: 100.00% \n",
      "recall: 100.00% \n",
      "fscore: 100.00% \n",
      "AUC 100.0000%\n",
      "---VALID---\n",
      "Accuracy: 70.54%\n",
      "precision: 70.70% \n",
      "recall: 70.54% \n",
      "fscore: 70.48% \n",
      "AUC 77.8699%\n",
      "---TEST---\n",
      "Accuracy: 70.31%\n",
      "precision: 70.49% \n",
      "recall: 70.31% \n",
      "fscore: 70.25% \n",
      "AUC 77.2461%\n",
      "TN:  24 FP:  8  FN:  11  TP:  21\n",
      "classifier_data:  [3, '06/09/2022', '08:29:39', '--', 'XGBClassifier', 'binary', 'exist', [('exist', 166), ('not_exist', 166)], [('exist', 32), ('not_exist', 32)], [('not_exist', 56), ('exist', 56)], 254, [(250, 0.09059527), (133, 0.030143594), (84, 0.022696817), (78, 0.022667212), (86, 0.021043245), (73, 0.019991467), (290, 0.0195821), (379, 0.018469552), (381, 0.018311338), (156, 0.017717188), (318, 0.017181832), (100, 0.01636683), (204, 0.015241245), (288, 0.014726369), (219, 0.013572477), (170, 0.013529629), (150, 0.01218077), (124, 0.012154814), (337, 0.012116624), (239, 0.0118468115), (309, 0.01182807), (37, 0.011809522), (326, 0.01116678), (67, 0.0111538395), (374, 0.011072033), (276, 0.010928363), (80, 0.010431753), (160, 0.010427738), (346, 0.010170281), (364, 0.00993493), (247, 0.009475073), (184, 0.00943646), (206, 0.009343912), (205, 0.0092665125), (332, 0.009252887), (76, 0.008765036), (211, 0.0087041), (283, 0.008550734), (267, 0.008204669), (18, 0.008194534), (112, 0.008191968), (285, 0.008021358), (182, 0.0077619143), (96, 0.007505324), (193, 0.0073155477), (47, 0.007283374), (101, 0.0070276973), (248, 0.007020191), (222, 0.0066005085), (314, 0.006597424), (297, 0.0065504713), (64, 0.006446413), (21, 0.006331534), (115, 0.006272156), (181, 0.006145526), (340, 0.0060455), (2, 0.0057854187), (99, 0.0056914734), (208, 0.005551911), (234, 0.005531332), (70, 0.0051864353), (278, 0.0051556067), (62, 0.005103516), (329, 0.0049195266), (39, 0.004841535), (214, 0.0046850964), (169, 0.004383682), (335, 0.004321536), (263, 0.0043128454), (141, 0.004216104), (291, 0.0041711214), (370, 0.0041254805), (348, 0.0041027097), (210, 0.004068055), (149, 0.0040291133), (373, 0.003950908), (252, 0.0038982085), (108, 0.0038431), (23, 0.0038382644), (104, 0.0038046937), (14, 0.0036832476), (274, 0.0035925703), (218, 0.00358692), (236, 0.0035073555), (10, 0.003426925), (85, 0.0033540986), (355, 0.003267786), (191, 0.0032075972), (35, 0.0032025168), (198, 0.0031629012), (341, 0.0031261602), (319, 0.0031091834), (223, 0.0029356682), (272, 0.0029131612), (243, 0.0028894802), (94, 0.00286865), (200, 0.0028671883), (255, 0.0028628844), (327, 0.0028367625), (343, 0.002790929), (212, 0.0027888787), (144, 0.002774182), (42, 0.0026293779), (258, 0.00261387), (323, 0.0025474357), (142, 0.0025055814), (106, 0.0024853388), (155, 0.0024513374), (281, 0.002395123), (293, 0.0023649517), (95, 0.0023539884), (324, 0.0023513266), (306, 0.0023354767), (1, 0.0022919443), (194, 0.0022535222), (81, 0.0022355255), (63, 0.0022062624), (31, 0.0021481446), (135, 0.0021029853), (304, 0.0018956842), (158, 0.0018530884), (295, 0.0018227228), (178, 0.0017690387), (107, 0.0017657111), (138, 0.0017450196), (368, 0.001660754), (221, 0.0016426722), (33, 0.001573075), (102, 0.0015459773), (88, 0.0014931168), (9, 0.0014707709), (12, 0.0014538338), (71, 0.0014363877), (60, 0.0014181677), (131, 0.0013741716), (367, 0.0013639817), (237, 0.0013557039), (13, 0.0013231238), (287, 0.0012974099), (7, 0.0012462602), (313, 0.0012398902), (168, 0.0012214606), (294, 0.0012049556), (72, 0.001199696), (185, 0.0011551034), (351, 0.001124781), (199, 0.001075065), (174, 0.0010585815), (16, 0.0010395077), (257, 0.0010016375), (44, 0.0009116272), (231, 0.0008200883), (128, 0.00072968326), (253, 0.0006984057), (139, 0.00068368256), (41, 0.00062734995), (17, 0.0005083872), (97, 0.0005069454), (173, 0.00046572945), (227, 0.00044362067), (328, 0.0004415633), (38, 0.00036440883), (186, 0.00034733556), (56, 0.00030438427), (167, 0.00027835896), (166, 0.0001267974), (0, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (8, 0.0), (11, 0.0), (15, 0.0), (19, 0.0), (20, 0.0), (22, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (32, 0.0), (34, 0.0), (36, 0.0), (40, 0.0), (43, 0.0), (45, 0.0), (46, 0.0), (48, 0.0), (49, 0.0), (50, 0.0), (51, 0.0), (52, 0.0), (53, 0.0), (54, 0.0), (55, 0.0), (57, 0.0), (58, 0.0), (59, 0.0), (61, 0.0), (65, 0.0), (66, 0.0), (68, 0.0), (69, 0.0), (74, 0.0), (75, 0.0), (77, 0.0), (79, 0.0), (82, 0.0), (83, 0.0), (87, 0.0), (89, 0.0), (90, 0.0), (91, 0.0), (92, 0.0), (93, 0.0), (98, 0.0), (103, 0.0), (105, 0.0), (109, 0.0), (110, 0.0), (111, 0.0), (113, 0.0), (114, 0.0), (116, 0.0), (117, 0.0), (118, 0.0), (119, 0.0), (120, 0.0), (121, 0.0), (122, 0.0), (123, 0.0), (125, 0.0), (126, 0.0), (127, 0.0), (129, 0.0), (130, 0.0), (132, 0.0), (134, 0.0), (136, 0.0), (137, 0.0), (140, 0.0), (143, 0.0), (145, 0.0), (146, 0.0), (147, 0.0), (148, 0.0), (151, 0.0), (152, 0.0), (153, 0.0), (154, 0.0), (157, 0.0), (159, 0.0), (161, 0.0), (162, 0.0), (163, 0.0), (164, 0.0), (165, 0.0), (171, 0.0), (172, 0.0), (175, 0.0), (176, 0.0), (177, 0.0), (179, 0.0), (180, 0.0), (183, 0.0), (187, 0.0), (188, 0.0), (189, 0.0), (190, 0.0), (192, 0.0), (195, 0.0), (196, 0.0), (197, 0.0), (201, 0.0), (202, 0.0), (203, 0.0), (207, 0.0), (209, 0.0), (213, 0.0), (215, 0.0), (216, 0.0), (217, 0.0), (220, 0.0), (224, 0.0), (225, 0.0), (226, 0.0), (228, 0.0), (229, 0.0), (230, 0.0), (232, 0.0), (233, 0.0), (235, 0.0), (238, 0.0), (240, 0.0), (241, 0.0), (242, 0.0), (244, 0.0), (245, 0.0), (246, 0.0), (249, 0.0), (251, 0.0), (254, 0.0), (256, 0.0), (259, 0.0), (260, 0.0), (261, 0.0), (262, 0.0), (264, 0.0), (265, 0.0), (266, 0.0), (268, 0.0), (269, 0.0), (270, 0.0), (271, 0.0), (273, 0.0), (275, 0.0), (277, 0.0), (279, 0.0), (280, 0.0), (282, 0.0), (284, 0.0), (286, 0.0), (289, 0.0), (292, 0.0), (296, 0.0), (298, 0.0), (299, 0.0), (300, 0.0), (301, 0.0), (302, 0.0), (303, 0.0), (305, 0.0), (307, 0.0), (308, 0.0), (310, 0.0), (311, 0.0), (312, 0.0), (315, 0.0), (316, 0.0), (317, 0.0), (320, 0.0), (321, 0.0), (322, 0.0), (325, 0.0), (330, 0.0), (331, 0.0), (333, 0.0), (334, 0.0), (336, 0.0), (338, 0.0), (339, 0.0), (342, 0.0), (344, 0.0), (345, 0.0), (347, 0.0), (349, 0.0), (350, 0.0), (352, 0.0), (353, 0.0), (354, 0.0), (356, 0.0), (357, 0.0), (358, 0.0), (359, 0.0), (360, 0.0), (361, 0.0), (362, 0.0), (363, 0.0), (365, 0.0), (366, 0.0), (369, 0.0), (371, 0.0), (372, 0.0), (375, 0.0), (376, 0.0), (377, 0.0), (378, 0.0), (380, 0.0), (382, 0.0), (383, 0.0)], 384, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7053571428571429, 0.7070073931211829, 0.7053571428571429, 0.7047687514977234, 0.7786989795918368, 0.703125, 0.7049261083743843, 0.703125, 0.7024712503058479, 0.7724609375]\n",
      "[Errno 17] File exists: '/dt/puzis/dt-reddit/feature_embedding/2022/robinhood/models_combine/'\n"
     ]
    }
   ],
   "source": [
    "models_name = ['DecisionTreeClassifier', \"RandomForestClassifier\", \"XGBClassifier\"]\n",
    "\n",
    "# without deleted class\n",
    "\n",
    "classes = ['exist'] \n",
    "\n",
    "# Insert the directory path in here\n",
    "path_to_classifier_data_folder = r'/dt/puzis/dt-reddit/feature_embedding/' \n",
    "\n",
    "\n",
    "l_year_folders = os.listdir(path_to_classifier_data_folder)\n",
    "file_set = set()\n",
    "\n",
    "already_done=[\"antiwork\"]\n",
    "\n",
    "for year in l_year_folders:\n",
    "    print(\"-------------------------------{}-----------------------\".format(year))\n",
    "    for file_name in os.listdir(path_to_classifier_data_folder + year):\n",
    "        subreddit = file_name.split(\"_\")[0]\n",
    "        file_set.add(file_name)\n",
    "        if year != \"2022\":\n",
    "            print(\"-----------------------------{}    DONE-----------------------\".format(file_name))\n",
    "            continue\n",
    "#         if year ==\"2022\" and subreddit in already_done: # Cryptocurrency LateStageCapitalism\n",
    "#             print(\"-----------------------------{}    DONE-----------------------\".format(file_name))\n",
    "#             continue            \n",
    "        print(\"-----------------------------{}-----------------------\".format(file_name))\n",
    "        subreddiit_path_to_write_analysis = path_to_classifier_data_folder + '{}/{}/embedding_classifier_{}_post_{}.csv'.format(year, subreddit, subreddit, year) # folder, file\n",
    "        model = Model(year=year, subreddit=subreddit, sub_kind='post')\n",
    "#         model_2022 = Model(year='2022', subreddit=subreddit, sub_kind='post')\n",
    "#         data = [model.data ,model_2022.data]\n",
    "#         model.data = pd.concat(data)\n",
    "        feature_list = model.data.columns.tolist()\n",
    "        feature_list = data_preprocissing(model, feature_list)\n",
    "        k_features = 6\n",
    "        k_feature_names = model.data.columns.tolist()\n",
    "        \n",
    "        for _class in classes:\n",
    "            for model_name in models_name:\n",
    "                is_splited = split_data_for_class(model, model_name, _class, k_feature_names, \"binary\")\n",
    "                print(\"is_splited\", is_splited)\n",
    "                if is_splited == False: \n",
    "                    print(\"subreddit\", file_name, \"  status\", model.data.status.value_counts())\n",
    "                    break\n",
    "                paths={'write_result_path': subreddiit_path_to_write_analysis, 'save_model_path': path_to_classifier_data_folder + '{}/{}/models_combine/'.format(year, subreddit, file_name)}\n",
    "                if model_name == 'DecisionTreeClassifier':\n",
    "                    print(\"here\")\n",
    "#                     criterion ,max_depth = DecisionTree_optimization(model)\n",
    "                    dec_tree_params = {\"criterion\": 'gini', \"max_depth\":5}\n",
    "                    print(\"dec_tree_params\", dec_tree_params)\n",
    "                    run_model(model, _class, \"binary\", model_name, k_feature_names, paths, subreddit, dec_tree_params)\n",
    "                else:\n",
    "                    run_model(model, _class, \"binary\", model_name, k_feature_names, paths, subreddit)\n",
    "            model.data = model.read_dataset(year=year, subreddit=subreddit, sub_kind='post')\n",
    "            feature_list = data_preprocissing(model, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce4094ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dt/puzis/dt-reddit/feature_embedding/2020/antiwork/embedding_classifier_antiwork_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/conservative/embedding_classifier_conservative_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/cryptocurrency/embedding_classifier_cryptocurrency_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/israel/embedding_classifier_israel_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/mensrights/embedding_classifier_mensrights_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/palestine/embedding_classifier_palestine_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/politics/embedding_classifier_politics_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/stocks/embedding_classifier_stocks_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/technology/embedding_classifier_technology_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/wallstreetbets/embedding_classifier_wallstreetbets_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2020/robinhood/embedding_classifier_robinhood_post_2020.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2021/antiwork/embedding_classifier_antiwork_post_2021.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2021/cryptocurrency/embedding_classifier_cryptocurrency_post_2021.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2021/trueoffmychest/embedding_classifier_trueoffmychest_post_2021.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2021/robinhood/embedding_classifier_robinhood_post_2021.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2022/antiwork/embedding_classifier_antiwork_post_2022.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2022/AskARussian/embedding_classifier_AskARussian_post_2022.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2022/cryptocurrency/embedding_classifier_cryptocurrency_post_2022.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2022/UkrainianConflict/embedding_classifier_UkrainianConflict_post_2022.csv\n",
      "/dt/puzis/dt-reddit/feature_embedding/2022/robinhood/embedding_classifier_robinhood_post_2022.csv\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "# # path_to_classifier_data_features_folder = r'/sise/home/shai1/reddit_code_shai/NLP_features/'\n",
    "# path_to_classifier_data_folder = r'/sise/home/shai1/reddit_code_shai/simple_classifier_by_subreddit/'\n",
    "# # Insert the directory path in here\n",
    " \n",
    "# # subreddiit_path_to_write_analysis = path_to_classifier_data_folder + '{}/{}/classifier_{}'.format(year, subreddit, file_name) # folder, file\n",
    "# # subreddiit_path_to_read_analysis = path_to_classifier_data_features_folder + '{}/{}'.format(year,file_name) # folder, file\n",
    "\n",
    "# # Extracting all the contents in the directory corresponding to path\n",
    "# l_year_folders = os.listdir(path_to_classifier_data_folder)\n",
    "# file_set = set()\n",
    "# for year in l_year_folders:\n",
    "#     if year in [\"2020\", \"2021\", \"2022\"]:\n",
    "#         for folder_name in os.listdir(path_to_classifier_data_folder + year):\n",
    "# #             if year == \"2021\" and folder_name == \"antiwork\":\n",
    "# #                 continue\n",
    "# #             print(path_to_classifier_data_folder + year+\"/\"+folder_name)\n",
    "#             original = r'/sise/home/shai1/reddit_code_shai/simple_classifier_by_subreddit/classifier_test.csv'\n",
    "#             target = r'/dt/puzis/dt-reddit/feature_embedding/{}/{}/embedding_classifier_{}_post_{}.csv'.format(year, folder_name,folder_name,year) \n",
    "# #             original  = r'/sise/home/shai1/reddit_code_shai/simple_classifier_by_subreddit/{}/{}/simple_classifier_{}_post_{}_r2.csv'.format(year, folder_name,folder_name,year)\n",
    "# #             target = r'/sise/home/shai1/reddit_code_shai/simple_classifier_by_subreddit/simple_featuers_xl_models_r2/simple_classifier_{}_post_{}.csv'.format(folder_name,year)\n",
    "#             print(target)\n",
    "#             shutil.copyfile(original, target)\n",
    "# #             print(folder_name.split(\"_\")[0])\n",
    "# # #             name = folder_name\n",
    "# #             print('/dt/puzis/dt-reddit/feature_embedding/' + year+\"/\"+folder_name)\n",
    "#             #Create the directory \n",
    "# #             try: \n",
    "# #                 os.mkdir('/dt/puzis/dt-reddit/feature_embedding/' + year+\"/\"+folder_name) #'/{}'.format(\"models\")\n",
    "# #             except OSError as error: \n",
    "# #                 print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b7ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
